{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drizzle/tiffanyw/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np \n",
    "import scipy.misc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length train x: 50000\n",
      "length train y: 50000\n",
      "length test x: 10000\n"
     ]
    }
   ],
   "source": [
    "URL_ENDPOINT = \"dataset/\"\n",
    "\n",
    "train_x = np.loadtxt(URL_ENDPOINT+\"train_x.csv\", delimiter=\",\")\n",
    "train_y = np.loadtxt(URL_ENDPOINT+\"train_y.csv\", delimiter=\",\")\n",
    "test_x = np.loadtxt(URL_ENDPOINT+\"test_x.csv\", delimiter=\",\")\n",
    "\n",
    "train_x = train_x.reshape(-1, 64, 64) # reshape \n",
    "test_x = test_x.reshape(-1, 64, 64)\n",
    "\n",
    "print(\"length train x:\", len(train_x))\n",
    "print(\"length train y:\", len(train_y))\n",
    "print(\"length test x:\", len(test_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train_x.reshape(-1, 4096) \n",
    "y = np.zeros((50000, 10))\n",
    "for i in range(len(y)):\n",
    "    y[i][int(train_y[i])] = 1\n",
    "\n",
    "test = test_x.reshape(-1, 4096) \n",
    "\n",
    "x[x < 235] = 0 \n",
    "test[test < 235] = 0 \n",
    "\n",
    "x /= 255.0\n",
    "test /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "# Make a matrix \n",
    "def matrix(m, n, fill=0.0):\n",
    "    return np.zeros(shape=(m,n)) + fill\n",
    "\n",
    "# Make a random matrix\n",
    "def rand_matrix(m, n, a=0, b=1):\n",
    "\treturn np.random.rand(m, n) * (b - a) + a\n",
    "\n",
    "# use logistic regression loss function \n",
    "def loss_fn(predict, truth):\n",
    "    n = len(truth)\n",
    "    loss = (- 1 / n) * np.sum(truth * np.log(predict) + (1 - truth) * (np.log(1 - predict)))\n",
    "    loss = np.squeeze(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # number of input, hidden, and output nodes\n",
    "        self.ni = ni\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "        \n",
    "        # bias vectors \n",
    "#         self.bh = np.zeros((1, self.nh))\n",
    "#         self.bo = np.zeros((1, self.no))\n",
    "        self.bh = np.ones(self.nh)\n",
    "        self.bo = np.ones(self.no)\n",
    "    \n",
    "\n",
    "        # create weights\n",
    "        # default to range (-0.5, 0.5)\n",
    "        self.wh = rand_matrix(self.ni, self.nh, -0.5, 0.5)\n",
    "        self.wo = rand_matrix(self.nh, self.no, -0.5, 0.5)\n",
    "\n",
    "    def propagate(self, inputs):\n",
    "        self.ai = inputs\n",
    "\n",
    "        # hidden layers activations\n",
    "        #bh is bias of hidden layers\n",
    "        self.ah = np.dot(self.ai, self.wh) + self.bh\n",
    "\n",
    "        # hidden output \n",
    "        self.oh = np.tanh(self.ah)\n",
    "\n",
    "        # output layers activations\n",
    "        self.ao = np.dot(self.ah, self.wo) + self.bo\n",
    "        \n",
    "        #h output layers output \n",
    "        self.oo = sigmoid(self.ao)\n",
    "\n",
    "    def backPropagate(self, x, y, eta):\n",
    "        n = x.shape[0]\n",
    "                 \n",
    "        self.dao = self.oo - y\n",
    "        self.dwo = np.dot(self.oh.T, self.dao) / n\n",
    "        self.dbo = np.sum(self.dao) / n\n",
    "        \n",
    "        self.dah = np.dot(self.dao, self.wo.T)*(1-np.tanh(self.ah))\n",
    "        self.dwh = np.dot(x.T, self.dah) / n\n",
    "        self.dbh = np.sum(self.dah) / n\n",
    "        \n",
    "        #update weights using gradient descent method. learning rate = eta\n",
    "        self.wo = self.wo - eta * self.dwo\n",
    "        self.wh = self.wh - eta * self.dwh\n",
    "        self.bo = self.bo - eta * self.dbo\n",
    "        self.bh = self.bh - eta * self.dbh\n",
    "        \n",
    "    def train(self, x, y, iterations = 1000, eta=0.5):\n",
    "        for i in range(iterations):\n",
    "            output = self.propagate(x)\n",
    "            loss = loss_fn(self.oo, y)\n",
    "            \n",
    "            pred = np.argmax(self.oo, axis=1)\n",
    "            diff = train_y - pred\n",
    "            acc = (diff == 0).sum() / len(y)\n",
    "            \n",
    "            self.backPropagate(x, y, eta)\n",
    "            if i % 100 == 0:\n",
    "                print('iteration ', i, \":     loss: \", loss, \",    acc: \", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = NN(ni=4096, nh=6, no=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  [[1.01282876 0.8258037  2.10472678 ... 1.85213159 2.08352894 1.53097282]\n",
      " [1.01282876 0.8258037  2.10472678 ... 1.85213159 2.08352894 1.53097282]\n",
      " [1.01282876 0.8258037  2.10472678 ... 1.85213159 2.08352894 1.53097282]\n",
      " ...\n",
      " [1.01282876 0.8258037  2.10472678 ... 1.85213159 2.08352894 1.53097282]\n",
      " [1.01282876 0.8258037  2.10472678 ... 1.85213159 2.08352894 1.53097282]\n",
      " [1.01282876 0.8258037  2.10472678 ... 1.85213159 2.08352894 1.53097282]]\n",
      "output shape:  (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "nn.propagate(x)\n",
    "print(\"output: \", nn.ao)\n",
    "print(\"output shape: \", nn.ao.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  0 :     loss:  14.2004261213919 ,    acc:  0.09818\n",
      "iteration  100 :     loss:  3.2496876433363244 ,    acc:  0.1114\n",
      "iteration  200 :     loss:  3.2496187490525235 ,    acc:  0.1114\n",
      "iteration  300 :     loss:  3.249617556227717 ,    acc:  0.1114\n",
      "iteration  400 :     loss:  3.2496175348931975 ,    acc:  0.1114\n",
      "iteration  500 :     loss:  3.2496175344976774 ,    acc:  0.1114\n",
      "iteration  600 :     loss:  3.2496175344902456 ,    acc:  0.1114\n",
      "iteration  700 :     loss:  3.2496175344901053 ,    acc:  0.1114\n",
      "iteration  800 :     loss:  3.249617534490102 ,    acc:  0.1114\n",
      "iteration  900 :     loss:  3.2496175344901013 ,    acc:  0.1114\n",
      "iteration  1000 :     loss:  3.249617534490102 ,    acc:  0.1114\n",
      "iteration  1100 :     loss:  3.2496175344901017 ,    acc:  0.1114\n",
      "iteration  1200 :     loss:  3.2496175344901026 ,    acc:  0.1114\n",
      "iteration  1300 :     loss:  3.249617534490102 ,    acc:  0.1114\n"
     ]
    }
   ],
   "source": [
    "nn.train(x, y, 2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
