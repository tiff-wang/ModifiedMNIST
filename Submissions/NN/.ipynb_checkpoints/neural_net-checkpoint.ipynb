{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1091,
     "status": "ok",
     "timestamp": 1521731580793,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "pehzYQkFyC6d",
    "outputId": "fad13a25-71b7-4618-8ab4-2ac36b0a57d6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "S9XGehxjyC6i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import numpy as np \n",
    "import scipy.misc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68999,
     "status": "ok",
     "timestamp": 1521731650949,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "6lpHce1pyC6k",
    "outputId": "e322bc8b-a228-45bf-e478-02d8bb2e0f6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data retrieved\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://s3.us-east-2.amazonaws.com/kaggle551/'\n",
    "\n",
    "train_x = pd.read_csv(URL + 'train_x_preproc.csv', header=None)\n",
    "train_y = pd.read_csv(URL + 'train_y.csv', header=None)\n",
    "\n",
    "train_x = np.array(train_x.as_matrix())\n",
    "train_y = np.array(train_y[0])\n",
    "\n",
    "test_x = pd.read_csv(URL + 'test_x_preproc.csv', header=None)\n",
    "test_x = np.array(test_x.as_matrix())\n",
    "\n",
    "print('data retrieved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "8s16eJsqyC6o"
   },
   "outputs": [],
   "source": [
    "train_x[train_x < 235] = 0 \n",
    "test_x[test_x < 235] = 0 \n",
    "\n",
    "train_x /= 255.0\n",
    "test_x /= 255.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1521691849320,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "CK-Eypx8zgHY",
    "outputId": "45898ba5-a9db-4e2c-b476-96034c37bb4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 4096)\n",
      "(50000,)\n",
      "(10000, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "bAmGpwo7yC6r"
   },
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-1 * x))\n",
    "\n",
    "# derivative of our sigmoid function, in terms of the output (i.e. y)\n",
    "def dsigmoid(x):\n",
    "    return 1.0 - x**2\n",
    "\n",
    "# Make a matrix \n",
    "def matrix(m, n, fill=0.0):\n",
    "    return np.zeros(shape=(m,n)) + fill\n",
    "\n",
    "# Make a random matrix\n",
    "def rand_matrix(m, n, a=0, b=1):\n",
    "\treturn np.random.rand(m, n) * (b - a) + a\n",
    "\n",
    "# use logistic regression loss function \n",
    "def loss_fn(predict, truth):\n",
    "    n = len(truth)\n",
    "    loss = (- 1 / n) * np.sum(truth * np.log(predict) + (1 - truth) * (np.log(1 - predict)))\n",
    "    loss = np.squeeze(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "nnc3BCibyC6v"
   },
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, ni, nh, no):\n",
    "        # number of input, hidden, and output nodes\n",
    "        self.ni = ni\n",
    "        self.nh = nh\n",
    "        self.no = no\n",
    "        \n",
    "        # bias vectors \n",
    "#         self.bh = np.zeros((1, self.nh))\n",
    "#         self.bo = np.zeros((1, self.no))\n",
    "        self.bh = np.ones(self.nh)\n",
    "        self.bo = np.ones(self.no)\n",
    "    \n",
    "\n",
    "        # create weights\n",
    "        # default to range (-0.5, 0.5)\n",
    "        self.wh = rand_matrix(self.ni, self.nh, -0.5, 0.5)\n",
    "        self.wo = rand_matrix(self.nh, self.no, -0.5, 0.5)\n",
    "        \n",
    "    \n",
    "    # training feed forward, obtain output from weight matrices and bias vectors\n",
    "    def propagate(self, inputs):\n",
    "        self.ai = inputs\n",
    "\n",
    "        # hidden layers activations\n",
    "        #bh is bias of hidden layers\n",
    "        self.ah = np.dot(self.ai, self.wh) + self.bh\n",
    "\n",
    "        # hidden output \n",
    "        self.oh = np.tanh(self.ah)\n",
    "\n",
    "        # output layers activations\n",
    "        self.ao = np.dot(self.ah, self.wo) + self.bo\n",
    "        \n",
    "        #h output layers output \n",
    "        self.oo = sigmoid(self.ao)\n",
    "\n",
    "    # training back propagation, updates neural network's weight matrices and bias vectors\n",
    "    def backPropagate(self, x, y, eta):\n",
    "        n = x.shape[0]\n",
    "        self.dao = self.oo - y\n",
    "        self.dwo = np.dot(self.oh.T, self.dao) / n\n",
    "        self.dbo = np.sum(self.dao) / n\n",
    "        \n",
    "        self.dah = np.dot(self.dao, self.wo.T)*(1-np.tanh(self.ah))\n",
    "        self.dwh = np.dot(x.T, self.dah) / n\n",
    "        self.dbh = np.sum(self.dah) / n\n",
    "        \n",
    "        #update weights using gradient descent method. learning rate = eta\n",
    "        self.wo = self.wo - eta * self.dwo\n",
    "        self.wh = self.wh - eta * self.dwh\n",
    "        self.bo = self.bo - eta * self.dbo\n",
    "        self.bh = self.bh - eta * self.dbh\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self, x):\n",
    "        ah = np.dot(x, self.wh) + self.bh\n",
    "\n",
    "        # hidden output \n",
    "        oh = np.tanh(ah)\n",
    "\n",
    "        # output layers activations\n",
    "        ao = np.dot(ah, self.wo) + self.bo\n",
    "        \n",
    "        #h output layers output \n",
    "        oo = sigmoid(ao)\n",
    "        return oo\n",
    "      \n",
    "    \n",
    "    # takes in Y     \n",
    "    def train(self, X, Y, iterations = 1000, eta=0.5):\n",
    "        trend = []\n",
    "        \n",
    "        # create output matrix\n",
    "        Y_m = np.zeros((X.shape[0], 10))\n",
    "        for i in range(len(Y)):\n",
    "          Y_m[i][int(Y[i])] = 1\n",
    "\n",
    "        for i in range(iterations):\n",
    "            output = self.propagate(X)\n",
    "            self.backPropagate(X, Y_m, eta)\n",
    "\n",
    "            pred = np.argmax(self.oo, axis=1)\n",
    "            loss = loss_fn(self.oo, Y_m)\n",
    "            diff = Y - pred\n",
    "            acc = (diff == 0).sum() / len(Y)\n",
    "            if( i % (iterations / 100) == 0): \n",
    "              trend.append([acc, loss])\n",
    "              print(\"iteration \", i, \" :    training acc: \", acc, \"   training loss: \", loss)\n",
    "\n",
    "        return trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "_QrdKwX4yC6x"
   },
   "outputs": [],
   "source": [
    "nn = NN(ni=4096, nh=6, no=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 255,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1745,
     "status": "ok",
     "timestamp": 1521696503488,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "gMXhM3tXyC60",
    "outputId": "6bff17a6-74b1-4b57-f014-e977177df9cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:  [[-2.38073109 -4.67663621 -3.2488232  ... -2.3355193  -3.37784973\n",
      "  -6.0460432 ]\n",
      " [-2.07071252 -3.92317283 -2.30168386 ... -1.7732754  -4.98692747\n",
      "  -5.49520772]\n",
      " [-3.21069716  0.56758109 -0.91953393 ... -2.64763564 -4.10285608\n",
      "  -6.14584302]\n",
      " ...\n",
      " [-1.45052863 -6.28624402 -3.14098832 ... -0.78301937 -3.36092328\n",
      "  -3.78289706]\n",
      " [-1.43457474 -5.2075142  -4.43390331 ... -3.2506142  -3.99810985\n",
      "  -6.77302833]\n",
      " [-2.1950731  -3.47372842  0.0717446  ... -0.47794964 -6.17378913\n",
      "  -4.16986025]]\n",
      "output shape:  (50000, 10)\n"
     ]
    }
   ],
   "source": [
    "nn.propagate(train_x)\n",
    "print(\"output: \", nn.ao)\n",
    "print(\"output shape: \", nn.ao.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8755,
     "output_extras": [
      {
       "item_id": 43
      },
      {
       "item_id": 68
      },
      {
       "item_id": 69
      },
      {
       "item_id": 70
      },
      {
       "item_id": 71
      },
      {
       "item_id": 89
      },
      {
       "item_id": 90
      },
      {
       "item_id": 136
      },
      {
       "item_id": 179
      },
      {
       "item_id": 223
      },
      {
       "item_id": 265
      },
      {
       "item_id": 306
      },
      {
       "item_id": 348
      },
      {
       "item_id": 392
      },
      {
       "item_id": 436
      },
      {
       "item_id": 479
      },
      {
       "item_id": 485
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 751494,
     "status": "ok",
     "timestamp": 1521697257393,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "yT12gcOoyC65",
    "outputId": "9491125a-e890-43a0-d922-b285d49aeab3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.11403853716083366    training loss:  19.624486824491612\n",
      "iteration  20  :    training acc:  0.11718442784113252    training loss:  4.147369099907744\n",
      "iteration  40  :    training acc:  0.11049941014549744    training loss:  4.506449693848254\n",
      "iteration  60  :    training acc:  0.10774675580023595    training loss:  4.519086611630984\n",
      "iteration  80  :    training acc:  0.10617381046008652    training loss:  4.429468353575487\n",
      "iteration  100  :    training acc:  0.11443177349587103    training loss:  4.374235212353267\n",
      "iteration  120  :    training acc:  0.10656704679512387    training loss:  4.618412310935821\n",
      "iteration  140  :    training acc:  0.10735351946519858    training loss:  4.783145045948446\n",
      "iteration  160  :    training acc:  0.10696028313016123    training loss:  4.631687219094227\n",
      "iteration  180  :    training acc:  0.1081399921352733    training loss:  4.934498492464705\n",
      "iteration  200  :    training acc:  0.1238694455367676    training loss:  4.281457737546193\n",
      "iteration  220  :    training acc:  0.11875737318128195    training loss:  4.185012636043689\n",
      "iteration  240  :    training acc:  0.11285882815572158    training loss:  4.535972267328772\n",
      "iteration  260  :    training acc:  0.11797090051120723    training loss:  4.635313116132459\n",
      "iteration  280  :    training acc:  0.12111679119150609    training loss:  4.479749423552733\n",
      "iteration  300  :    training acc:  0.12780180888714118    training loss:  4.105894570885351\n",
      "iteration  320  :    training acc:  0.10696028313016123    training loss:  4.498950146762526\n",
      "iteration  340  :    training acc:  0.12544239087691703    training loss:  4.136113788273947\n",
      "iteration  360  :    training acc:  0.11167911915060952    training loss:  4.195533762747827\n",
      "iteration  380  :    training acc:  0.13448682658277625    training loss:  3.6395287454819174\n",
      "iteration  400  :    training acc:  0.13370035391270155    training loss:  3.6406111001070918\n",
      "iteration  420  :    training acc:  0.11325206449075895    training loss:  4.200701557518083\n",
      "iteration  440  :    training acc:  0.13959889893826188    training loss:  3.691866531414042\n",
      "iteration  460  :    training acc:  0.13802595359811246    training loss:  3.6003173114541265\n",
      "iteration  480  :    training acc:  0.10342115611482501    training loss:  4.434858005188674\n",
      "iteration  500  :    training acc:  0.14313802595359812    training loss:  3.73186269882807\n",
      "iteration  520  :    training acc:  0.13841918993314983    training loss:  3.5060859142628935\n",
      "iteration  540  :    training acc:  0.14195831694848604    training loss:  3.654902538115673\n",
      "iteration  560  :    training acc:  0.14825009830908376    training loss:  3.609030715903666\n",
      "iteration  580  :    training acc:  0.13448682658277625    training loss:  3.583282810494826\n",
      "iteration  600  :    training acc:  0.1451042076287849    training loss:  3.5174800966545625\n",
      "iteration  620  :    training acc:  0.13841918993314983    training loss:  3.712770509579298\n",
      "iteration  640  :    training acc:  0.14353126228863547    training loss:  3.492977117683133\n",
      "iteration  660  :    training acc:  0.1451042076287849    training loss:  3.551898350851443\n",
      "iteration  680  :    training acc:  0.1403853716083366    training loss:  3.625939816140508\n",
      "iteration  700  :    training acc:  0.13566653558788833    training loss:  3.5913193547931117\n",
      "iteration  720  :    training acc:  0.14628391663389698    training loss:  3.6471843995943587\n",
      "iteration  740  :    training acc:  0.15454187966968147    training loss:  3.4606331219705866\n",
      "iteration  760  :    training acc:  0.10617381046008652    training loss:  4.769147715269669\n",
      "iteration  780  :    training acc:  0.1376327172630751    training loss:  3.736850899774887\n",
      "iteration  800  :    training acc:  0.13723948092803775    training loss:  3.6200370535063047\n",
      "iteration  820  :    training acc:  0.13566653558788833    training loss:  3.646860282732037\n",
      "iteration  840  :    training acc:  0.15296893432953204    training loss:  3.501527159335186\n",
      "iteration  860  :    training acc:  0.11600471883602045    training loss:  4.3952100103005165\n",
      "iteration  880  :    training acc:  0.15572158867479355    training loss:  3.545472578262479\n",
      "iteration  900  :    training acc:  0.17105780574125048    training loss:  3.377341789666958\n",
      "iteration  920  :    training acc:  0.17970900511207236    training loss:  3.4262441908701735\n",
      "iteration  940  :    training acc:  0.1655524970507275    training loss:  3.4247127118770835\n",
      "iteration  960  :    training acc:  0.16830515139598898    training loss:  3.4028159157626963\n",
      "iteration  980  :    training acc:  0.17105780574125048    training loss:  3.3731396928015753\n",
      "iteration  1000  :    training acc:  0.17341722375147464    training loss:  3.3717897816122915\n",
      "iteration  1020  :    training acc:  0.17577664176169877    training loss:  3.3475387737363906\n",
      "iteration  1040  :    training acc:  0.18364136846244594    training loss:  3.341170209165058\n",
      "iteration  1060  :    training acc:  0.18324813212740856    training loss:  3.328887143777641\n",
      "iteration  1080  :    training acc:  0.18246165945733386    training loss:  3.3202841855853538\n",
      "iteration  1100  :    training acc:  0.18796696814785685    training loss:  3.3169780107219777\n",
      "iteration  1120  :    training acc:  0.19307904050334251    training loss:  3.3395295585771696\n",
      "iteration  1140  :    training acc:  0.19268580416830516    training loss:  3.4023547648953896\n",
      "iteration  1160  :    training acc:  0.19976405819897758    training loss:  3.40400851607092\n",
      "iteration  1180  :    training acc:  0.2052693668895006    training loss:  3.413773943964124\n",
      "iteration  1200  :    training acc:  0.20487613055446324    training loss:  3.4418974057315945\n",
      "iteration  1220  :    training acc:  0.20448289421942586    training loss:  3.575598267433604\n",
      "iteration  1240  :    training acc:  0.19819111285882815    training loss:  3.650478029039455\n",
      "iteration  1260  :    training acc:  0.18796696814785685    training loss:  3.7184559524681036\n",
      "iteration  1280  :    training acc:  0.20094376720408966    training loss:  3.7904953609052874\n",
      "iteration  1300  :    training acc:  0.15100275265434526    training loss:  5.410174710087593\n",
      "iteration  1320  :    training acc:  0.15847424302005506    training loss:  4.720788119150087\n",
      "iteration  1340  :    training acc:  0.1411718442784113    training loss:  6.013893632937067\n",
      "iteration  1360  :    training acc:  0.1726307510813999    training loss:  4.285929765637044\n",
      "iteration  1380  :    training acc:  0.20920173023987415    training loss:  4.024953276529628\n",
      "iteration  1400  :    training acc:  0.1451042076287849    training loss:  6.574171928931192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1420  :    training acc:  0.16948486040110106    training loss:  6.4585837099167405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1440  :    training acc:  0.13291388124262682    training loss:  inf\n",
      "iteration  1460  :    training acc:  0.13999213527329926    training loss:  nan\n",
      "iteration  1480  :    training acc:  0.18128195045222179    training loss:  inf\n",
      "iteration  1500  :    training acc:  0.12072355485646874    training loss:  nan\n",
      "iteration  1520  :    training acc:  0.1183641368462446    training loss:  inf\n",
      "iteration  1540  :    training acc:  0.11915060951631931    training loss:  inf\n",
      "iteration  1560  :    training acc:  0.15572158867479355    training loss:  nan\n",
      "iteration  1580  :    training acc:  0.12622886354699175    training loss:  nan\n",
      "iteration  1600  :    training acc:  0.1403853716083366    training loss:  nan\n",
      "iteration  1620  :    training acc:  0.11600471883602045    training loss:  nan\n",
      "iteration  1640  :    training acc:  0.15690129767990563    training loss:  nan\n",
      "iteration  1660  :    training acc:  0.17577664176169877    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.11993708218639403    training loss:  nan\n",
      "iteration  1700  :    training acc:  0.11482500983090838    training loss:  nan\n",
      "iteration  1720  :    training acc:  0.14628391663389698    training loss:  nan\n",
      "iteration  1740  :    training acc:  0.12426268187180496    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.12504915454187968    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.1301612268973653    training loss:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1800  :    training acc:  0.15021627998427053    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.12780180888714118    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.11797090051120723    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.1238694455367676    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.1423515532835234    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.12033031852143138    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.12622886354699175    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.12898151789225323    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.11954384585135666    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.14746362563900905    training loss:  nan\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.10160847391133777    training loss:  23.925115842874888\n",
      "iteration  20  :    training acc:  0.09964692036092586    training loss:  5.0977322534388\n",
      "iteration  40  :    training acc:  0.10513927030207924    training loss:  6.207901379283603\n",
      "iteration  60  :    training acc:  0.10082385249117301    training loss:  5.073381173581832\n",
      "iteration  80  :    training acc:  0.10160847391133777    training loss:  4.941439717637704\n",
      "iteration  100  :    training acc:  0.09886229894076108    training loss:  4.96844547605754\n",
      "iteration  120  :    training acc:  0.10043154178109062    training loss:  5.159247497583085\n",
      "iteration  140  :    training acc:  0.09650843468026678    training loss:  5.083758375770666\n",
      "iteration  160  :    training acc:  0.10278540604158494    training loss:  5.566812326410826\n",
      "iteration  180  :    training acc:  0.09415457041977246    training loss:  4.791557656339316\n",
      "iteration  200  :    training acc:  0.09807767752059632    training loss:  5.330828775911693\n",
      "iteration  220  :    training acc:  0.09650843468026678    training loss:  5.67086423115174\n",
      "iteration  240  :    training acc:  0.1035700274617497    training loss:  4.55994496213225\n",
      "iteration  260  :    training acc:  0.10278540604158494    training loss:  4.9443603849178155\n",
      "iteration  280  :    training acc:  0.10474695959199685    training loss:  4.552973860560749\n",
      "iteration  300  :    training acc:  0.09846998823067869    training loss:  4.5244849898781965\n",
      "iteration  320  :    training acc:  0.09768536681051393    training loss:  4.528996887926476\n",
      "iteration  340  :    training acc:  0.09846998823067869    training loss:  4.717302193507482\n",
      "iteration  360  :    training acc:  0.10592389172224402    training loss:  4.385354349468126\n",
      "iteration  380  :    training acc:  0.09807767752059632    training loss:  5.0908885404502415\n",
      "iteration  400  :    training acc:  0.10317771675166731    training loss:  4.907469607894751\n",
      "iteration  420  :    training acc:  0.09690074539034915    training loss:  4.750314132379896\n",
      "iteration  440  :    training acc:  0.09690074539034915    training loss:  4.331629846143882\n",
      "iteration  460  :    training acc:  0.10160847391133777    training loss:  4.5240938357716916\n",
      "iteration  480  :    training acc:  0.09729305610043154    training loss:  4.7571602896731475\n",
      "iteration  500  :    training acc:  0.1012161632012554    training loss:  4.604389933377209\n",
      "iteration  520  :    training acc:  0.09807767752059632    training loss:  4.467102173823184\n",
      "iteration  540  :    training acc:  0.10749313456257356    training loss:  4.908839922656391\n",
      "iteration  560  :    training acc:  0.10553158101216163    training loss:  4.376786879590678\n",
      "iteration  580  :    training acc:  0.10513927030207924    training loss:  4.479973269239974\n",
      "iteration  600  :    training acc:  0.10317771675166731    training loss:  4.3871990255411415\n",
      "iteration  620  :    training acc:  0.10435464888191448    training loss:  4.758002825081335\n",
      "iteration  640  :    training acc:  0.09690074539034915    training loss:  6.1362709810512746\n",
      "iteration  660  :    training acc:  0.09964692036092586    training loss:  4.693552196076898\n",
      "iteration  680  :    training acc:  0.10788544527265595    training loss:  4.965705138888504\n",
      "iteration  700  :    training acc:  0.11533934876422126    training loss:  5.345853515064562\n",
      "iteration  720  :    training acc:  0.12200863083562181    training loss:  4.382684211948004\n",
      "iteration  740  :    training acc:  0.11612397018438603    training loss:  4.7495360269140035\n",
      "iteration  760  :    training acc:  0.11533934876422126    training loss:  5.570422535553432\n",
      "iteration  780  :    training acc:  0.1118085523734798    training loss:  4.96688438329263\n",
      "iteration  800  :    training acc:  0.1090623774029031    training loss:  4.3246667240881935\n",
      "iteration  820  :    training acc:  0.11808552373479796    training loss:  4.953275309203581\n",
      "iteration  840  :    training acc:  0.1224009415457042    training loss:  4.344785076479855\n",
      "iteration  860  :    training acc:  0.11023930953315025    training loss:  4.593023740865834\n",
      "iteration  880  :    training acc:  0.11063162024323264    training loss:  4.92650691059651\n",
      "iteration  900  :    training acc:  0.10200078462142016    training loss:  4.102212684367902\n",
      "iteration  920  :    training acc:  0.12475480580619851    training loss:  4.520678885511569\n",
      "iteration  940  :    training acc:  0.1114162416633974    training loss:  5.091964052645988\n",
      "iteration  960  :    training acc:  0.11808552373479796    training loss:  4.852792489615397\n",
      "iteration  980  :    training acc:  0.09964692036092586    training loss:  6.058238856289463\n",
      "iteration  1000  :    training acc:  0.13613181639858768    training loss:  4.646696057696512\n",
      "iteration  1020  :    training acc:  0.12985484503726952    training loss:  4.937421527684963\n",
      "iteration  1040  :    training acc:  0.12867791290702235    training loss:  4.4740478070890815\n",
      "iteration  1060  :    training acc:  0.10317771675166731    training loss:  5.442897387320128\n",
      "iteration  1080  :    training acc:  0.14240878775990584    training loss:  4.763471269261906\n",
      "iteration  1100  :    training acc:  0.12161632012553943    training loss:  4.892572716137409\n",
      "iteration  1120  :    training acc:  0.13652412710867007    training loss:  4.73590054442004\n",
      "iteration  1140  :    training acc:  0.10160847391133777    training loss:  5.392771216904151\n",
      "iteration  1160  :    training acc:  0.11023930953315025    training loss:  5.283078524798018\n",
      "iteration  1180  :    training acc:  0.1224009415457042    training loss:  4.216733597097146\n",
      "iteration  1200  :    training acc:  0.12279325225578658    training loss:  4.763810542399\n",
      "iteration  1220  :    training acc:  0.1278932914868576    training loss:  4.2201804150227336\n",
      "iteration  1240  :    training acc:  0.10670851314240878    training loss:  4.907514838040766\n",
      "iteration  1260  :    training acc:  0.14162416633974106    training loss:  4.867292606388539\n",
      "iteration  1280  :    training acc:  0.12828560219693996    training loss:  4.542820456043867\n",
      "iteration  1300  :    training acc:  0.12279325225578658    training loss:  4.762240362637043\n",
      "iteration  1320  :    training acc:  0.11808552373479796    training loss:  4.281273624964906\n",
      "iteration  1340  :    training acc:  0.11847783444488034    training loss:  4.625345918624997\n",
      "iteration  1360  :    training acc:  0.13927030207924676    training loss:  4.607667962234532\n",
      "iteration  1380  :    training acc:  0.11887014515496273    training loss:  4.774614890910242\n",
      "iteration  1400  :    training acc:  0.11847783444488034    training loss:  5.3903357944231844\n",
      "iteration  1420  :    training acc:  0.12867791290702235    training loss:  4.918232189935564\n",
      "iteration  1440  :    training acc:  0.13730874852883485    training loss:  4.890668249521928\n",
      "iteration  1460  :    training acc:  0.13966261278932915    training loss:  5.177600850678777\n",
      "iteration  1480  :    training acc:  0.12122400941545704    training loss:  4.624094136990855\n",
      "iteration  1500  :    training acc:  0.1349548842683405    training loss:  4.11047475126814\n",
      "iteration  1520  :    training acc:  0.15457041977245978    training loss:  4.5992044905936496\n",
      "iteration  1540  :    training acc:  0.14829344841114162    training loss:  4.079854739921359\n",
      "iteration  1560  :    training acc:  0.14437034131031778    training loss:  4.166253170389911\n",
      "iteration  1580  :    training acc:  0.12828560219693996    training loss:  4.0498682202670215\n",
      "iteration  1600  :    training acc:  0.13417026284817576    training loss:  4.059194058848183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1620  :    training acc:  0.14319340918007062    training loss:  3.8867554263463573\n",
      "iteration  1640  :    training acc:  0.10435464888191448    training loss:  4.608595249335547\n",
      "iteration  1660  :    training acc:  0.15339348764221264    training loss:  3.8959178141731305\n",
      "iteration  1680  :    training acc:  0.12357787367595136    training loss:  3.9147171157664045\n",
      "iteration  1700  :    training acc:  0.1357395056885053    training loss:  3.863927762710443\n",
      "iteration  1720  :    training acc:  0.12907022361710474    training loss:  4.348772502277225\n",
      "iteration  1740  :    training acc:  0.15064731267163595    training loss:  4.1569671589197\n",
      "iteration  1760  :    training acc:  0.13770105923891723    training loss:  4.395011863638751\n",
      "iteration  1780  :    training acc:  0.1380933699489996    training loss:  3.971796429495516\n",
      "iteration  1800  :    training acc:  0.15457041977245978    training loss:  3.989838161051992\n",
      "iteration  1820  :    training acc:  0.10435464888191448    training loss:  4.598026027836387\n",
      "iteration  1840  :    training acc:  0.143585719890153    training loss:  4.3695700461154\n",
      "iteration  1860  :    training acc:  0.12867791290702235    training loss:  4.255867127527733\n",
      "iteration  1880  :    training acc:  0.13377795213809338    training loss:  3.657811590634999\n",
      "iteration  1900  :    training acc:  0.1490780698313064    training loss:  4.122123443700316\n",
      "iteration  1920  :    training acc:  0.1353471949784229    training loss:  3.9068983616757014\n",
      "iteration  1940  :    training acc:  0.14476265202040015    training loss:  4.013874994341988\n",
      "iteration  1960  :    training acc:  0.14986269125147117    training loss:  3.978334638868039\n",
      "iteration  1980  :    training acc:  0.143585719890153    training loss:  4.27017636431525\n",
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.1051792828685259    training loss:  20.07015795484492\n",
      "iteration  20  :    training acc:  0.10119521912350597    training loss:  6.460916004591288\n",
      "iteration  40  :    training acc:  0.10398406374501992    training loss:  6.1383647111811435\n",
      "iteration  60  :    training acc:  0.0952191235059761    training loss:  6.443727996081442\n",
      "iteration  80  :    training acc:  0.09800796812749003    training loss:  6.122820956248125\n",
      "iteration  100  :    training acc:  0.09760956175298804    training loss:  5.469539356004708\n",
      "iteration  120  :    training acc:  0.10796812749003984    training loss:  6.394457724114411\n",
      "iteration  140  :    training acc:  0.10199203187250996    training loss:  5.443257597681723\n",
      "iteration  160  :    training acc:  0.09681274900398407    training loss:  6.02871614363137\n",
      "iteration  180  :    training acc:  0.10079681274900398    training loss:  7.047276656082643\n",
      "iteration  200  :    training acc:  0.09601593625498007    training loss:  5.642528706602759\n",
      "iteration  220  :    training acc:  0.09402390438247012    training loss:  5.314833494845158\n",
      "iteration  240  :    training acc:  0.10239043824701195    training loss:  5.203863359182923\n",
      "iteration  260  :    training acc:  0.09123505976095618    training loss:  6.076142202437584\n",
      "iteration  280  :    training acc:  0.10119521912350597    training loss:  5.272751129991107\n",
      "iteration  300  :    training acc:  0.09681274900398407    training loss:  4.897664242115268\n",
      "iteration  320  :    training acc:  0.10199203187250996    training loss:  4.974347765387591\n",
      "iteration  340  :    training acc:  0.1099601593625498    training loss:  6.937297059497431\n",
      "iteration  360  :    training acc:  0.10717131474103586    training loss:  4.955271889639399\n",
      "iteration  380  :    training acc:  0.09322709163346614    training loss:  5.450449523015492\n",
      "iteration  400  :    training acc:  0.09282868525896415    training loss:  5.666713882437185\n",
      "iteration  420  :    training acc:  0.1051792828685259    training loss:  4.8819325065717765\n",
      "iteration  440  :    training acc:  0.09322709163346614    training loss:  5.389006474788661\n",
      "iteration  460  :    training acc:  0.09840637450199204    training loss:  5.151137172965657\n",
      "iteration  480  :    training acc:  0.10956175298804781    training loss:  5.137360276902299\n",
      "iteration  500  :    training acc:  0.11195219123505976    training loss:  5.78199743693486\n",
      "iteration  520  :    training acc:  0.11792828685258964    training loss:  5.740858875842455\n",
      "iteration  540  :    training acc:  0.100398406374502    training loss:  5.382154285913806\n",
      "iteration  560  :    training acc:  0.1051792828685259    training loss:  6.988108866248284\n",
      "iteration  580  :    training acc:  0.1151394422310757    training loss:  5.733459821040052\n",
      "iteration  600  :    training acc:  0.11075697211155379    training loss:  5.32122214682869\n",
      "iteration  620  :    training acc:  0.11434262948207172    training loss:  5.111297285096214\n",
      "iteration  640  :    training acc:  0.11314741035856574    training loss:  4.783081586699013\n",
      "iteration  660  :    training acc:  0.10119521912350597    training loss:  4.933714729229184\n",
      "iteration  680  :    training acc:  0.11354581673306773    training loss:  4.987733851113695\n",
      "iteration  700  :    training acc:  0.10239043824701195    training loss:  4.646743480035041\n",
      "iteration  720  :    training acc:  0.10637450199203187    training loss:  6.815949630473014\n",
      "iteration  740  :    training acc:  0.1147410358565737    training loss:  5.047864273448901\n",
      "iteration  760  :    training acc:  0.11075697211155379    training loss:  5.601836504550258\n",
      "iteration  780  :    training acc:  0.10438247011952191    training loss:  5.075940071145734\n",
      "iteration  800  :    training acc:  0.09880478087649402    training loss:  5.823104273599148\n",
      "iteration  820  :    training acc:  0.10796812749003984    training loss:  4.477092217892986\n",
      "iteration  840  :    training acc:  0.10916334661354582    training loss:  4.715265988245678\n",
      "iteration  860  :    training acc:  0.11155378486055777    training loss:  4.765333544676098\n",
      "iteration  880  :    training acc:  0.11434262948207172    training loss:  4.6847902618644515\n",
      "iteration  900  :    training acc:  0.11434262948207172    training loss:  5.014008761399728\n",
      "iteration  920  :    training acc:  0.11035856573705179    training loss:  6.184525570709174\n",
      "iteration  940  :    training acc:  0.11235059760956176    training loss:  5.025102516750923\n",
      "iteration  960  :    training acc:  0.11593625498007969    training loss:  4.981509850936292\n",
      "iteration  980  :    training acc:  0.11075697211155379    training loss:  5.066090651735013\n",
      "iteration  1000  :    training acc:  0.12111553784860558    training loss:  5.3234955910757416\n",
      "iteration  1020  :    training acc:  0.11713147410358565    training loss:  4.485913864948003\n",
      "iteration  1040  :    training acc:  0.11713147410358565    training loss:  4.840822027555101\n",
      "iteration  1060  :    training acc:  0.1254980079681275    training loss:  5.217039365764043\n",
      "iteration  1080  :    training acc:  0.11394422310756971    training loss:  4.9909689691547054\n",
      "iteration  1100  :    training acc:  0.1350597609561753    training loss:  4.465019111769822\n",
      "iteration  1120  :    training acc:  0.13824701195219125    training loss:  4.594714948285536\n",
      "iteration  1140  :    training acc:  0.13545816733067728    training loss:  4.378929427302864\n",
      "iteration  1160  :    training acc:  0.1310756972111554    training loss:  4.862747078577947\n",
      "iteration  1180  :    training acc:  0.14103585657370518    training loss:  5.619771979248622\n",
      "iteration  1200  :    training acc:  0.11235059760956176    training loss:  5.6154886359316265\n",
      "iteration  1220  :    training acc:  0.11075697211155379    training loss:  6.1215665271174\n",
      "iteration  1240  :    training acc:  0.11195219123505976    training loss:  6.937005012774915\n",
      "iteration  1260  :    training acc:  0.1362549800796813    training loss:  4.482221782667787\n",
      "iteration  1280  :    training acc:  0.1398406374501992    training loss:  4.617473434116783\n",
      "iteration  1300  :    training acc:  0.11792828685258964    training loss:  5.393680213359726\n",
      "iteration  1320  :    training acc:  0.11115537848605578    training loss:  5.345688071886725\n",
      "iteration  1340  :    training acc:  0.12908366533864543    training loss:  5.333273382852654\n",
      "iteration  1360  :    training acc:  0.13904382470119522    training loss:  5.0605493141955655\n",
      "iteration  1380  :    training acc:  0.12669322709163347    training loss:  6.96289595965743\n",
      "iteration  1400  :    training acc:  0.11314741035856574    training loss:  6.063040247328255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1420  :    training acc:  0.1641434262948207    training loss:  4.372860592677519\n",
      "iteration  1440  :    training acc:  0.1541832669322709    training loss:  4.952890392119028\n",
      "iteration  1460  :    training acc:  0.1254980079681275    training loss:  4.918279453805513\n",
      "iteration  1480  :    training acc:  0.12470119521912351    training loss:  5.334501131607965\n",
      "iteration  1500  :    training acc:  0.14860557768924304    training loss:  4.45099353321917\n",
      "iteration  1520  :    training acc:  0.11075697211155379    training loss:  5.122574305815827\n",
      "iteration  1540  :    training acc:  0.16374501992031873    training loss:  3.711269274188175\n",
      "iteration  1560  :    training acc:  0.11832669322709163    training loss:  5.23604479879193\n",
      "iteration  1580  :    training acc:  0.1549800796812749    training loss:  4.71519241221914\n",
      "iteration  1600  :    training acc:  0.14063745019920318    training loss:  3.9840643120724324\n",
      "iteration  1620  :    training acc:  0.15617529880478087    training loss:  4.11433939210601\n",
      "iteration  1640  :    training acc:  0.13705179282868526    training loss:  4.166715094098623\n",
      "iteration  1660  :    training acc:  0.1557768924302789    training loss:  3.7065308343637495\n",
      "iteration  1680  :    training acc:  0.14302788844621514    training loss:  4.028285925313608\n",
      "iteration  1700  :    training acc:  0.15378486055776894    training loss:  4.035464844384551\n",
      "iteration  1720  :    training acc:  0.15338645418326693    training loss:  3.8494535663892284\n",
      "iteration  1740  :    training acc:  0.16135458167330677    training loss:  3.591821978547792\n",
      "iteration  1760  :    training acc:  0.13187250996015937    training loss:  3.990684333176752\n",
      "iteration  1780  :    training acc:  0.14621513944223108    training loss:  3.8462690006999605\n",
      "iteration  1800  :    training acc:  0.15298804780876493    training loss:  3.8062672380794687\n",
      "iteration  1820  :    training acc:  0.15776892430278885    training loss:  3.899022656087684\n",
      "iteration  1840  :    training acc:  0.16334661354581673    training loss:  3.702493293127897\n",
      "iteration  1860  :    training acc:  0.16294820717131475    training loss:  3.849767933826602\n",
      "iteration  1880  :    training acc:  0.16852589641434262    training loss:  3.6755122107762057\n",
      "iteration  1900  :    training acc:  0.16175298804780877    training loss:  3.786619703303834\n",
      "iteration  1920  :    training acc:  0.16294820717131475    training loss:  3.7325747641356757\n",
      "iteration  1940  :    training acc:  0.16294820717131475    training loss:  3.8212355997524443\n",
      "iteration  1960  :    training acc:  0.17051792828685258    training loss:  3.6043414979393322\n",
      "iteration  1980  :    training acc:  0.16653386454183267    training loss:  3.7139001305342827\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.0976916735366859    training loss:  20.59561813039913\n",
      "iteration  20  :    training acc:  0.09934047815333882    training loss:  5.243662099040801\n",
      "iteration  40  :    training acc:  0.10469909315746084    training loss:  5.287499194356354\n",
      "iteration  60  :    training acc:  0.10057708161582853    training loss:  5.547984859289906\n",
      "iteration  80  :    training acc:  0.09686727122835943    training loss:  4.93060025108234\n",
      "iteration  100  :    training acc:  0.09934047815333882    training loss:  5.315678403639558\n",
      "iteration  120  :    training acc:  0.09480626545754328    training loss:  5.702936147605717\n",
      "iteration  140  :    training acc:  0.10016488046166529    training loss:  5.141846052356107\n",
      "iteration  160  :    training acc:  0.09851607584501236    training loss:  5.702326992022757\n",
      "iteration  180  :    training acc:  0.11294311624072548    training loss:  4.938493116423022\n",
      "iteration  200  :    training acc:  0.10387469084913438    training loss:  4.6583515550855665\n",
      "iteration  220  :    training acc:  0.09727947238252267    training loss:  5.000676816075011\n",
      "iteration  240  :    training acc:  0.10387469084913438    training loss:  4.820674669280993\n",
      "iteration  260  :    training acc:  0.10222588623248145    training loss:  5.194973278359259\n",
      "iteration  280  :    training acc:  0.10758450123660346    training loss:  4.744015198652114\n",
      "iteration  300  :    training acc:  0.10469909315746084    training loss:  5.1767402716837\n",
      "iteration  320  :    training acc:  0.11211871393239901    training loss:  4.935105356341095\n",
      "iteration  340  :    training acc:  0.10758450123660346    training loss:  4.6487582716783535\n",
      "iteration  360  :    training acc:  0.1079967023907667    training loss:  4.631172980669115\n",
      "iteration  380  :    training acc:  0.11046990931574609    training loss:  5.3171160792133385\n",
      "iteration  400  :    training acc:  0.11211871393239901    training loss:  5.031471874087468\n",
      "iteration  420  :    training acc:  0.11129431162407255    training loss:  5.080402319644851\n",
      "iteration  440  :    training acc:  0.10634789777411377    training loss:  6.097186521354928\n",
      "iteration  460  :    training acc:  0.11747732893652102    training loss:  5.327222071720951\n",
      "iteration  480  :    training acc:  0.11788953009068426    training loss:  5.624415933992829\n",
      "iteration  500  :    training acc:  0.11788953009068426    training loss:  5.125902313327708\n",
      "iteration  520  :    training acc:  0.1236603462489695    training loss:  4.926810206572971\n",
      "iteration  540  :    training acc:  0.12530915086562242    training loss:  4.930238160426793\n",
      "iteration  560  :    training acc:  0.1302555647155812    training loss:  5.854372500585573\n",
      "iteration  580  :    training acc:  0.13231657048639736    training loss:  5.487161308129372\n",
      "iteration  600  :    training acc:  0.14303380049464137    training loss:  6.056563500769332\n",
      "iteration  620  :    training acc:  0.10923330585325638    training loss:  5.276549622702376\n",
      "iteration  640  :    training acc:  0.14179719703215168    training loss:  4.306228825886627\n",
      "iteration  660  :    training acc:  0.13602638087386645    training loss:  6.854840076410732\n",
      "iteration  680  :    training acc:  0.14633140972794723    training loss:  7.125270516299621\n",
      "iteration  700  :    training acc:  0.1508656224237428    training loss:  4.800419651472812\n",
      "iteration  720  :    training acc:  0.10263808738664468    training loss:  5.855246020893685\n",
      "iteration  740  :    training acc:  0.14550700741962078    training loss:  5.334894237930841\n",
      "iteration  760  :    training acc:  0.17930750206100576    training loss:  4.107948877648653\n",
      "iteration  780  :    training acc:  0.1640560593569662    training loss:  4.782123169471535\n",
      "iteration  800  :    training acc:  0.1537510305028854    training loss:  5.5071508605984025\n",
      "iteration  820  :    training acc:  0.1586974443528442    training loss:  6.367422074808546\n",
      "iteration  840  :    training acc:  0.16859027205276175    training loss:  3.734891765342481\n",
      "iteration  860  :    training acc:  0.1875515251442704    training loss:  3.68296311697777\n",
      "iteration  880  :    training acc:  0.18219291014014838    training loss:  3.820106932969269\n",
      "iteration  900  :    training acc:  0.1314921681780709    training loss:  7.543779235097018\n",
      "iteration  920  :    training acc:  0.14014839241549876    training loss:  4.744460787562928\n",
      "iteration  940  :    training acc:  0.18796372629843364    training loss:  3.813842113531802\n",
      "iteration  960  :    training acc:  0.14509480626545754    training loss:  4.509314590017191\n",
      "iteration  980  :    training acc:  0.19249793899422918    training loss:  3.789488812438538\n",
      "iteration  1000  :    training acc:  0.20362737015663643    training loss:  3.6016773389081624\n",
      "iteration  1020  :    training acc:  0.19620774938169827    training loss:  3.7751777734404395\n",
      "iteration  1040  :    training acc:  0.11871393239901072    training loss:  5.5562980583944\n",
      "iteration  1060  :    training acc:  0.20321516900247322    training loss:  3.4744522933727704\n",
      "iteration  1080  :    training acc:  0.1314921681780709    training loss:  5.54304961747804\n",
      "iteration  1100  :    training acc:  0.20939818631492169    training loss:  3.5735683430855123\n",
      "iteration  1120  :    training acc:  0.19332234130255566    training loss:  3.536944997806959\n",
      "iteration  1140  :    training acc:  0.17889530090684255    training loss:  3.917538208609856\n",
      "iteration  1160  :    training acc:  0.1953833470733718    training loss:  3.627425901421454\n",
      "iteration  1180  :    training acc:  0.1953833470733718    training loss:  3.581396229831155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1200  :    training acc:  0.19167353668590273    training loss:  3.871482317708837\n",
      "iteration  1220  :    training acc:  0.18013190436933224    training loss:  3.799428793556211\n",
      "iteration  1240  :    training acc:  0.20651277823577907    training loss:  3.5101617281256368\n",
      "iteration  1260  :    training acc:  0.16611706512778235    training loss:  4.770730457563197\n",
      "iteration  1280  :    training acc:  0.1471558120362737    training loss:  4.131740143496369\n",
      "iteration  1300  :    training acc:  0.16817807089859851    training loss:  4.316843544035342\n",
      "iteration  1320  :    training acc:  0.21063478977741137    training loss:  3.8874268804701835\n",
      "iteration  1340  :    training acc:  0.24196207749381698    training loss:  3.378041684309042\n",
      "iteration  1360  :    training acc:  0.17889530090684255    training loss:  3.9335104979157065\n",
      "iteration  1380  :    training acc:  0.17065127782357792    training loss:  3.92734646466384\n",
      "iteration  1400  :    training acc:  0.21764220939818632    training loss:  3.634996805382936\n",
      "iteration  1420  :    training acc:  0.1665292662819456    training loss:  4.355920850439487\n",
      "iteration  1440  :    training acc:  0.18054410552349548    training loss:  4.034061554481672\n",
      "iteration  1460  :    training acc:  0.1574608408903545    training loss:  4.016079036558478\n",
      "iteration  1480  :    training acc:  0.19373454245671887    training loss:  4.452312706986099\n",
      "iteration  1500  :    training acc:  0.20403957131079967    training loss:  3.719225554585777\n",
      "iteration  1520  :    training acc:  0.18590272052761747    training loss:  5.009857201801163\n",
      "iteration  1540  :    training acc:  0.2056883759274526    training loss:  3.7891597035568925\n",
      "iteration  1560  :    training acc:  0.18507831821929102    training loss:  3.943776411941328\n",
      "iteration  1580  :    training acc:  0.16240725474031328    training loss:  5.428613681503153\n",
      "iteration  1600  :    training acc:  0.18961253091508656    training loss:  4.216050852326127\n",
      "iteration  1620  :    training acc:  0.14756801319043694    training loss:  4.817249242126929\n",
      "iteration  1640  :    training acc:  0.18507831821929102    training loss:  4.131740007843997\n",
      "iteration  1660  :    training acc:  0.17353668590272053    training loss:  5.023379793800958\n",
      "iteration  1680  :    training acc:  0.1875515251442704    training loss:  4.347713784388626\n",
      "iteration  1700  :    training acc:  0.1170651277823578    training loss:  12.151649378900629\n",
      "iteration  1720  :    training acc:  0.1834295136026381    training loss:  6.410964104215228\n",
      "iteration  1740  :    training acc:  0.18301731244847486    training loss:  7.048040369578527\n",
      "iteration  1760  :    training acc:  0.1731244847485573    training loss:  7.8781191166634\n",
      "iteration  1780  :    training acc:  0.1537510305028854    training loss:  6.265020155072943\n",
      "iteration  1800  :    training acc:  0.15210222588623248    training loss:  9.369370785886753\n",
      "iteration  1820  :    training acc:  0.13973619126133552    training loss:  8.13131681528953\n",
      "iteration  1840  :    training acc:  0.11624072547403133    training loss:  10.61970162392205\n",
      "iteration  1860  :    training acc:  0.2019785655399835    training loss:  4.7374457565808585\n",
      "iteration  1880  :    training acc:  0.10593569661995053    training loss:  11.704393885493987\n",
      "iteration  1900  :    training acc:  0.12737015663643858    training loss:  8.88122823964036\n",
      "iteration  1920  :    training acc:  0.11253091508656224    training loss:  16.43588959868823\n",
      "iteration  1940  :    training acc:  0.12943116240725475    training loss:  9.379243301715874\n",
      "iteration  1960  :    training acc:  0.19620774938169827    training loss:  5.538920404479449\n",
      "iteration  1980  :    training acc:  0.13355317394888705    training loss:  13.12423420061982\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.09382239382239382    training loss:  20.466473355764\n",
      "iteration  20  :    training acc:  0.11196911196911197    training loss:  6.759781962799054\n",
      "iteration  40  :    training acc:  0.10463320463320464    training loss:  5.678649420430241\n",
      "iteration  60  :    training acc:  0.10038610038610038    training loss:  8.176442968242396\n",
      "iteration  80  :    training acc:  0.10501930501930502    training loss:  6.279957484991975\n",
      "iteration  100  :    training acc:  0.09536679536679536    training loss:  5.561450700008427\n",
      "iteration  120  :    training acc:  0.09922779922779923    training loss:  5.527955034384529\n",
      "iteration  140  :    training acc:  0.10424710424710425    training loss:  5.127166277594907\n",
      "iteration  160  :    training acc:  0.10308880308880308    training loss:  7.066639550085655\n",
      "iteration  180  :    training acc:  0.11274131274131274    training loss:  5.660021736870971\n",
      "iteration  200  :    training acc:  0.10888030888030888    training loss:  6.009979089000833\n",
      "iteration  220  :    training acc:  0.09498069498069497    training loss:  5.656245198784955\n",
      "iteration  240  :    training acc:  0.09961389961389962    training loss:  5.378725778353\n",
      "iteration  260  :    training acc:  0.1138996138996139    training loss:  6.86215154534016\n",
      "iteration  280  :    training acc:  0.11235521235521236    training loss:  6.199946053604181\n",
      "iteration  300  :    training acc:  0.11003861003861004    training loss:  5.213164497036494\n",
      "iteration  320  :    training acc:  0.12046332046332046    training loss:  7.155084229587202\n",
      "iteration  340  :    training acc:  0.10772200772200773    training loss:  5.318734447012659\n",
      "iteration  360  :    training acc:  0.11312741312741313    training loss:  5.85426457752702\n",
      "iteration  380  :    training acc:  0.11042471042471043    training loss:  5.573195774177872\n",
      "iteration  400  :    training acc:  0.11081081081081082    training loss:  7.158196269466963\n",
      "iteration  420  :    training acc:  0.12046332046332046    training loss:  5.47209411177826\n",
      "iteration  440  :    training acc:  0.10077220077220077    training loss:  5.622431758237177\n",
      "iteration  460  :    training acc:  0.10888030888030888    training loss:  5.277336652701293\n",
      "iteration  480  :    training acc:  0.11698841698841698    training loss:  5.465010547924495\n",
      "iteration  500  :    training acc:  0.11274131274131274    training loss:  4.96320997753018\n",
      "iteration  520  :    training acc:  0.11621621621621622    training loss:  5.309915922879524\n",
      "iteration  540  :    training acc:  0.10193050193050193    training loss:  5.257012050761976\n",
      "iteration  560  :    training acc:  0.1138996138996139    training loss:  8.037898758607183\n",
      "iteration  580  :    training acc:  0.10849420849420849    training loss:  6.294249302705148\n",
      "iteration  600  :    training acc:  0.11428571428571428    training loss:  4.617187221805613\n",
      "iteration  620  :    training acc:  0.12432432432432433    training loss:  5.24103720985745\n",
      "iteration  640  :    training acc:  0.12625482625482626    training loss:  4.63055966621777\n",
      "iteration  660  :    training acc:  0.13590733590733592    training loss:  4.447656768662802\n",
      "iteration  680  :    training acc:  0.13127413127413126    training loss:  5.496988493640974\n",
      "iteration  700  :    training acc:  0.13590733590733592    training loss:  6.011311455597513\n",
      "iteration  720  :    training acc:  0.14671814671814673    training loss:  4.5684129034327245\n",
      "iteration  740  :    training acc:  0.11312741312741313    training loss:  8.355004909699316\n",
      "iteration  760  :    training acc:  0.09922779922779923    training loss:  6.654200349481669\n",
      "iteration  780  :    training acc:  0.12007722007722008    training loss:  6.7795173322103555\n",
      "iteration  800  :    training acc:  0.11312741312741313    training loss:  8.417569775848728\n",
      "iteration  820  :    training acc:  0.11544401544401545    training loss:  5.901486804958414\n",
      "iteration  840  :    training acc:  0.1305019305019305    training loss:  5.137928267842674\n",
      "iteration  860  :    training acc:  0.14903474903474903    training loss:  4.6375891732277355\n",
      "iteration  880  :    training acc:  0.13436293436293437    training loss:  7.365818125078106\n",
      "iteration  900  :    training acc:  0.1664092664092664    training loss:  3.685432062242595\n",
      "iteration  920  :    training acc:  0.177992277992278    training loss:  3.754143950681228\n",
      "iteration  940  :    training acc:  0.16525096525096525    training loss:  3.7268766368592523\n",
      "iteration  960  :    training acc:  0.18687258687258687    training loss:  4.0121625692230305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  980  :    training acc:  0.15019305019305018    training loss:  4.571249315795841\n",
      "iteration  1000  :    training acc:  0.14787644787644788    training loss:  5.872185159095236\n",
      "iteration  1020  :    training acc:  0.13938223938223937    training loss:  4.593687825441248\n",
      "iteration  1040  :    training acc:  0.17297297297297298    training loss:  3.671806010076379\n",
      "iteration  1060  :    training acc:  0.177992277992278    training loss:  3.793997256633013\n",
      "iteration  1080  :    training acc:  0.1335907335907336    training loss:  4.107082690840424\n",
      "iteration  1100  :    training acc:  0.13474903474903474    training loss:  4.9879171213386675\n",
      "iteration  1120  :    training acc:  0.18030888030888031    training loss:  3.781648095811297\n",
      "iteration  1140  :    training acc:  0.12084942084942085    training loss:  4.73547108270313\n",
      "iteration  1160  :    training acc:  0.17683397683397684    training loss:  3.7264508178438773\n",
      "iteration  1180  :    training acc:  0.14594594594594595    training loss:  3.9097467990399792\n",
      "iteration  1200  :    training acc:  0.13706563706563707    training loss:  4.263004445131674\n",
      "iteration  1220  :    training acc:  0.11351351351351352    training loss:  4.982881039149673\n",
      "iteration  1240  :    training acc:  0.12084942084942085    training loss:  4.578140092588711\n",
      "iteration  1260  :    training acc:  0.1498069498069498    training loss:  4.474900563329098\n",
      "iteration  1280  :    training acc:  0.17876447876447876    training loss:  4.083038502258263\n",
      "iteration  1300  :    training acc:  0.1664092664092664    training loss:  4.947574413737414\n",
      "iteration  1320  :    training acc:  0.11621621621621622    training loss:  6.079566959796791\n",
      "iteration  1340  :    training acc:  0.15482625482625484    training loss:  4.720562403235884\n",
      "iteration  1360  :    training acc:  0.12664092664092663    training loss:  6.6182920731271455\n",
      "iteration  1380  :    training acc:  0.12393822393822394    training loss:  5.143149375737272\n",
      "iteration  1400  :    training acc:  0.1718146718146718    training loss:  5.0191071620305125\n",
      "iteration  1420  :    training acc:  0.1362934362934363    training loss:  5.405053756685944\n",
      "iteration  1440  :    training acc:  0.16872586872586873    training loss:  5.261997297895892\n",
      "iteration  1460  :    training acc:  0.1640926640926641    training loss:  6.759816848705369\n",
      "iteration  1480  :    training acc:  0.10193050193050193    training loss:  13.778170227603356\n",
      "iteration  1500  :    training acc:  0.1888030888030888    training loss:  6.1628372014459085\n",
      "iteration  1520  :    training acc:  0.17567567567567569    training loss:  6.969653942163951\n",
      "iteration  1540  :    training acc:  0.08146718146718147    training loss:  12.48933825917538\n",
      "iteration  1560  :    training acc:  0.09961389961389962    training loss:  10.921403234432718\n",
      "iteration  1580  :    training acc:  0.16525096525096525    training loss:  9.660319799437463\n",
      "iteration  1600  :    training acc:  0.12123552123552124    training loss:  8.260867277168161\n",
      "iteration  1620  :    training acc:  0.1583011583011583    training loss:  6.541563552216964\n",
      "iteration  1640  :    training acc:  0.10617760617760617    training loss:  nan\n",
      "iteration  1660  :    training acc:  0.16872586872586873    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.16602316602316602    training loss:  7.199552702700482\n",
      "iteration  1700  :    training acc:  0.11698841698841698    training loss:  7.2195022270948295\n",
      "iteration  1720  :    training acc:  0.10540540540540541    training loss:  15.879878925762627\n",
      "iteration  1740  :    training acc:  0.16023166023166024    training loss:  inf\n",
      "iteration  1760  :    training acc:  0.1308880308880309    training loss:  inf\n",
      "iteration  1780  :    training acc:  0.1193050193050193    training loss:  10.907175418396802\n",
      "iteration  1800  :    training acc:  0.18301158301158302    training loss:  9.821157120925902\n",
      "iteration  1820  :    training acc:  0.12972972972972974    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.15444015444015444    training loss:  10.4393883119787\n",
      "iteration  1860  :    training acc:  0.12393822393822394    training loss:  inf\n",
      "iteration  1880  :    training acc:  0.0945945945945946    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.10115830115830116    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.13552123552123552    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.13552123552123552    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.10270270270270271    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.09922779922779923    training loss:  nan\n"
     ]
    }
   ],
   "source": [
    "cross_validation=5\n",
    "valid_split = 0.05\n",
    "\n",
    "# best_pred = [accuracy, nn, training trend]\n",
    "best_pred = (0, 0, [])\n",
    "\n",
    "for valid in range(cross_validation):\n",
    "    print(\"\\nCross Validation fold \", valid)\n",
    "  \n",
    "    # randomly split the dataset into validation and training sets \n",
    "    mask = np.random.rand(train_x.shape[0]) <= valid_split\n",
    "    t_x = train_x[mask]\n",
    "    t_y = train_y[mask]\n",
    "\n",
    "    v_x = train_x[~mask]\n",
    "    v_y = train_y[~mask]\n",
    "    \n",
    "    nn = NN(ni=4096, nh=6, no=10)\n",
    "    \n",
    "    res = nn.train(t_x, t_y, 2000)\n",
    "    \n",
    "    # validate with validation set after the training\n",
    "    v_o = nn.predict(v_x)\n",
    "    pred = np.argmax(v_o, axis=1)\n",
    "    diff = v_y - pred\n",
    "    acc = (diff == 0).sum() / len(v_y)\n",
    "   \n",
    "    \n",
    "    if(acc > best_pred[0]): best_pred = (acc, nn, res) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "oaicow30-1GQ"
   },
   "outputs": [],
   "source": [
    "best_nn = best_pred[1]\n",
    "\n",
    "pred = best_nn.predict(test_x)\n",
    "pred = np.argmax(v_o, axis=1)\n",
    "\n",
    "arr = np.arange(len(pred))\n",
    "\n",
    "np.savetxt('nn_prediction.csv', np.dstack((arr, pred))[0], \"%d,%d\", header = \"Id,Label\", comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 707,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 619,
     "status": "ok",
     "timestamp": 1521698485870,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "mrGqmUFkRdT_",
    "outputId": "e2a923af-2eb3-422e-e70e-32943b9aabd0"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFZCAYAAACizedRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEckMy5mIiEhm/g3C8jkurbmC2AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac7c8db5f8>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFZCAYAAACv05cWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAVIvjtHsZB/AAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac7c825470>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch = np.arange(len(res))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.title('Neural Network Training Loss Trend')\n",
    "plt.plot(epoch, res[:,1], 'r--')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.title('Neural Network Training Accuracy Trend')\n",
    "plt.plot(epoch, res[:,0], 'b')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 8755,
     "output_extras": [
      {
       "item_id": 2
      },
      {
       "item_id": 3
      },
      {
       "item_id": 31
      },
      {
       "item_id": 39
      },
      {
       "item_id": 40
      },
      {
       "item_id": 61
      },
      {
       "item_id": 77
      },
      {
       "item_id": 96
      },
      {
       "item_id": 98
      },
      {
       "item_id": 99
      },
      {
       "item_id": 120
      },
      {
       "item_id": 141
      },
      {
       "item_id": 163
      },
      {
       "item_id": 185
      },
      {
       "item_id": 205
      },
      {
       "item_id": 225
      },
      {
       "item_id": 236
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 269976,
     "status": "ok",
     "timestamp": 1521727492780,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "FXauAQhsEhZ4",
    "outputId": "2ed31438-8fb6-4b08-dd77-362ee931fd43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.08959835221421215    training loss:  19.212018377192777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  20  :    training acc:  0.10916580844490216    training loss:  6.016921067099159\n",
      "iteration  40  :    training acc:  0.10607621009268794    training loss:  6.8177959506742\n",
      "iteration  60  :    training acc:  0.10401647785787847    training loss:  5.850191781635482\n",
      "iteration  80  :    training acc:  0.10401647785787847    training loss:  5.412260999941086\n",
      "iteration  100  :    training acc:  0.10504634397528322    training loss:  5.205933421850757\n",
      "iteration  120  :    training acc:  0.121524201853759    training loss:  5.078892825000366\n",
      "iteration  140  :    training acc:  0.13079299691040164    training loss:  5.889614761416814\n",
      "iteration  160  :    training acc:  0.12770339855818744    training loss:  6.42710231830367\n",
      "iteration  180  :    training acc:  0.12358393408856849    training loss:  5.757856266025936\n",
      "iteration  200  :    training acc:  0.11122554067971163    training loss:  5.691809703876902\n",
      "iteration  220  :    training acc:  0.12873326467559218    training loss:  4.66190139892152\n",
      "iteration  240  :    training acc:  0.12049433573635428    training loss:  5.4425165706346705\n",
      "iteration  260  :    training acc:  0.12358393408856849    training loss:  5.2734364666199856\n",
      "iteration  280  :    training acc:  0.12770339855818744    training loss:  5.120060150950468\n",
      "iteration  300  :    training acc:  0.1431513903192585    training loss:  4.04814111857429\n",
      "iteration  320  :    training acc:  0.1266735324407827    training loss:  4.566801510965353\n",
      "iteration  340  :    training acc:  0.13079299691040164    training loss:  4.329306326628818\n",
      "iteration  360  :    training acc:  0.1400617919670443    training loss:  4.567962648532766\n",
      "iteration  380  :    training acc:  0.13285272914521112    training loss:  3.8925221620868813\n",
      "iteration  400  :    training acc:  0.12976313079299692    training loss:  3.9582435680788555\n",
      "iteration  420  :    training acc:  0.12770339855818744    training loss:  4.688683853217742\n",
      "iteration  440  :    training acc:  0.15756951596292482    training loss:  4.222409963002736\n",
      "iteration  460  :    training acc:  0.1513903192584964    training loss:  4.772508354361786\n",
      "iteration  480  :    training acc:  0.18331616889804325    training loss:  4.054002550262191\n",
      "iteration  500  :    training acc:  0.16683831101956745    training loss:  4.3247781800386935\n",
      "iteration  520  :    training acc:  0.19979402677651906    training loss:  4.601404684298795\n",
      "iteration  540  :    training acc:  0.13388259526261587    training loss:  6.104133340066098\n",
      "iteration  560  :    training acc:  0.14418125643666324    training loss:  4.478363856827923\n",
      "iteration  580  :    training acc:  0.18640576725025745    training loss:  4.052540503898722\n",
      "iteration  600  :    training acc:  0.14727085478887744    training loss:  4.299081193307534\n",
      "iteration  620  :    training acc:  0.15550978372811536    training loss:  4.210619569227791\n",
      "iteration  640  :    training acc:  0.17610710607621008    training loss:  4.150208490604192\n",
      "iteration  660  :    training acc:  0.1874356333676622    training loss:  3.7358019983593143\n",
      "iteration  680  :    training acc:  0.16580844490216273    training loss:  6.478986846603285\n",
      "iteration  700  :    training acc:  0.2059732234809475    training loss:  3.8317389372592383\n",
      "iteration  720  :    training acc:  0.203913491246138    training loss:  4.071896166713162\n",
      "iteration  740  :    training acc:  0.18537590113285274    training loss:  4.103467646655974\n",
      "iteration  760  :    training acc:  0.16992790937178168    training loss:  5.23943468285067\n",
      "iteration  780  :    training acc:  0.3027806385169928    training loss:  3.2581787112852747\n",
      "iteration  800  :    training acc:  0.21833161688980432    training loss:  4.161985566343082\n",
      "iteration  820  :    training acc:  0.2687950566426365    training loss:  4.2182175055948\n",
      "iteration  840  :    training acc:  0.262615859938208    training loss:  3.6807548030903345\n",
      "iteration  860  :    training acc:  0.27703398558187436    training loss:  3.6334115170231356\n",
      "iteration  880  :    training acc:  0.2729145211122554    training loss:  3.5939627760479675\n",
      "iteration  900  :    training acc:  0.21112255406797117    training loss:  4.437957147994441\n",
      "iteration  920  :    training acc:  0.2554067971163749    training loss:  4.0447298051027\n",
      "iteration  940  :    training acc:  0.2615859938208033    training loss:  4.031802005478453\n",
      "iteration  960  :    training acc:  0.27188465499485065    training loss:  4.377234426587898\n",
      "iteration  980  :    training acc:  0.2729145211122554    training loss:  3.9602204807011088\n",
      "iteration  1000  :    training acc:  0.2729145211122554    training loss:  3.4928163351220567\n",
      "iteration  1020  :    training acc:  0.282183316168898    training loss:  3.9026426612072984\n",
      "iteration  1040  :    training acc:  0.2584963954685891    training loss:  4.172802428895059\n",
      "iteration  1060  :    training acc:  0.2760041194644696    training loss:  3.4476249714701903\n",
      "iteration  1080  :    training acc:  0.28527291452111225    training loss:  4.683690326810815\n",
      "iteration  1100  :    training acc:  0.3120494335736354    training loss:  3.6617370777023592\n",
      "iteration  1120  :    training acc:  0.2090628218331617    training loss:  5.83750106690213\n",
      "iteration  1140  :    training acc:  0.27909371781668385    training loss:  3.980676012525966\n",
      "iteration  1160  :    training acc:  0.22657054582904224    training loss:  7.653260402021226\n",
      "iteration  1180  :    training acc:  0.2533470648815654    training loss:  3.8652788546715904\n",
      "iteration  1200  :    training acc:  0.26364572605561276    training loss:  5.047394265181781\n",
      "iteration  1220  :    training acc:  0.29660144181256437    training loss:  4.92724668071924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1240  :    training acc:  0.2780638516992791    training loss:  3.9523326079450554\n",
      "iteration  1260  :    training acc:  0.2646755921730175    training loss:  3.7386607783421706\n",
      "iteration  1280  :    training acc:  0.29042224510813597    training loss:  3.975631086613076\n",
      "iteration  1300  :    training acc:  0.2883625128733265    training loss:  4.248839216492595\n",
      "iteration  1320  :    training acc:  0.35015447991761073    training loss:  3.7369858707327803\n",
      "iteration  1340  :    training acc:  0.28321318228630277    training loss:  4.057163670841769\n",
      "iteration  1360  :    training acc:  0.3419155509783728    training loss:  3.237700595525978\n",
      "iteration  1380  :    training acc:  0.26364572605561276    training loss:  4.822991888425799\n",
      "iteration  1400  :    training acc:  0.243048403707518    training loss:  6.311665399268906\n",
      "iteration  1420  :    training acc:  0.33161688980432547    training loss:  4.250120349762747\n",
      "iteration  1440  :    training acc:  0.2646755921730175    training loss:  5.342903822090177\n",
      "iteration  1460  :    training acc:  0.2760041194644696    training loss:  3.8981254174507307\n",
      "iteration  1480  :    training acc:  0.28527291452111225    training loss:  4.2293245185655115\n",
      "iteration  1500  :    training acc:  0.305870236869207    training loss:  4.0774379633190385\n",
      "iteration  1520  :    training acc:  0.3707518022657055    training loss:  3.2106396116133817\n",
      "iteration  1540  :    training acc:  0.3408856848609681    training loss:  4.093895003742962\n",
      "iteration  1560  :    training acc:  0.3027806385169928    training loss:  3.478842390528398\n",
      "iteration  1580  :    training acc:  0.3192584963954686    training loss:  3.8672827963448837\n",
      "iteration  1600  :    training acc:  0.3419155509783728    training loss:  3.4502822792641057\n",
      "iteration  1620  :    training acc:  0.33470648815653964    training loss:  3.4149078249360265\n",
      "iteration  1640  :    training acc:  0.32440782698249226    training loss:  3.727661854610106\n",
      "iteration  1660  :    training acc:  0.27909371781668385    training loss:  4.409218779389404\n",
      "iteration  1680  :    training acc:  0.3233779608650875    training loss:  3.7103034241099633\n",
      "iteration  1700  :    training acc:  0.3388259526261586    training loss:  3.8455492811398853\n",
      "iteration  1720  :    training acc:  0.223480947476828    training loss:  nan\n",
      "iteration  1740  :    training acc:  0.2615859938208033    training loss:  5.925373656709664\n",
      "iteration  1760  :    training acc:  0.28321318228630277    training loss:  4.517472111833751\n",
      "iteration  1780  :    training acc:  0.29866117404737386    training loss:  4.326243481438086\n",
      "iteration  1800  :    training acc:  0.2780638516992791    training loss:  4.1968832781712475\n",
      "iteration  1820  :    training acc:  0.3079299691040165    training loss:  4.0947018874904355\n",
      "iteration  1840  :    training acc:  0.2708547888774459    training loss:  5.2081383644228865\n",
      "iteration  1860  :    training acc:  0.2481977342945417    training loss:  4.807853670915495\n",
      "iteration  1880  :    training acc:  0.35118434603501547    training loss:  4.078954215744312\n",
      "iteration  1900  :    training acc:  0.3120494335736354    training loss:  3.8567366042948223\n",
      "iteration  1920  :    training acc:  0.3305870236869207    training loss:  5.232978980099871\n",
      "iteration  1940  :    training acc:  0.3367662203913491    training loss:  3.95685139346197\n",
      "iteration  1960  :    training acc:  0.35221421215242016    training loss:  4.081101248810732\n",
      "iteration  1980  :    training acc:  0.2893923789907312    training loss:  4.178713594504978\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.09451219512195122    training loss:  25.72084269308167\n",
      "iteration  20  :    training acc:  0.1290650406504065    training loss:  3.657990678351832\n",
      "iteration  40  :    training acc:  0.13922764227642276    training loss:  3.573815944168758\n",
      "iteration  60  :    training acc:  0.14329268292682926    training loss:  3.5366931135170576\n",
      "iteration  80  :    training acc:  0.15040650406504066    training loss:  3.508308240061726\n",
      "iteration  100  :    training acc:  0.15752032520325204    training loss:  3.481223849686933\n",
      "iteration  120  :    training acc:  0.1524390243902439    training loss:  3.44510591064097\n",
      "iteration  140  :    training acc:  0.18597560975609756    training loss:  3.3908189805565128\n",
      "iteration  160  :    training acc:  0.19817073170731708    training loss:  3.3217408011254497\n",
      "iteration  180  :    training acc:  0.19613821138211382    training loss:  3.2571695112454213\n",
      "iteration  200  :    training acc:  0.2073170731707317    training loss:  3.211614069283638\n",
      "iteration  220  :    training acc:  0.21036585365853658    training loss:  3.1589277807072\n",
      "iteration  240  :    training acc:  0.21443089430894308    training loss:  3.1192102472533634\n",
      "iteration  260  :    training acc:  0.23983739837398374    training loss:  3.0802585698768254\n",
      "iteration  280  :    training acc:  0.22560975609756098    training loss:  3.0868980529783965\n",
      "iteration  300  :    training acc:  0.22357723577235772    training loss:  3.1905152853643073\n",
      "iteration  320  :    training acc:  0.21341463414634146    training loss:  3.1716434418416575\n",
      "iteration  340  :    training acc:  0.16056910569105692    training loss:  4.102578687281407\n",
      "iteration  360  :    training acc:  0.23882113821138212    training loss:  3.1259475599969075\n",
      "iteration  380  :    training acc:  0.24390243902439024    training loss:  3.040236540328899\n",
      "iteration  400  :    training acc:  0.21646341463414634    training loss:  3.349535667718932\n",
      "iteration  420  :    training acc:  0.2032520325203252    training loss:  3.4242529345983357\n",
      "iteration  440  :    training acc:  0.2459349593495935    training loss:  3.2899512334854046\n",
      "iteration  460  :    training acc:  0.2621951219512195    training loss:  3.064202698463661\n",
      "iteration  480  :    training acc:  0.25203252032520324    training loss:  3.547037065443117\n",
      "iteration  500  :    training acc:  0.23678861788617886    training loss:  3.1768515592412814\n",
      "iteration  520  :    training acc:  0.26727642276422764    training loss:  3.182521988200277\n",
      "iteration  540  :    training acc:  0.266260162601626    training loss:  3.1103921390703766\n",
      "iteration  560  :    training acc:  0.24085365853658536    training loss:  3.8884308939864733\n",
      "iteration  580  :    training acc:  0.2693089430894309    training loss:  3.2775605994519594\n",
      "iteration  600  :    training acc:  0.23577235772357724    training loss:  4.06527103785106\n",
      "iteration  620  :    training acc:  0.26727642276422764    training loss:  3.776833184278705\n",
      "iteration  640  :    training acc:  0.2225609756097561    training loss:  4.907172535854095\n",
      "iteration  660  :    training acc:  0.22560975609756098    training loss:  6.183434394377323\n",
      "iteration  680  :    training acc:  0.22459349593495934    training loss:  4.990411981422529\n",
      "iteration  700  :    training acc:  0.28760162601626016    training loss:  3.552673950837836\n",
      "iteration  720  :    training acc:  0.24491869918699186    training loss:  4.348688635825348\n",
      "iteration  740  :    training acc:  0.2459349593495935    training loss:  4.424021575332671\n",
      "iteration  760  :    training acc:  0.20630081300813008    training loss:  4.834922594496813\n",
      "iteration  780  :    training acc:  0.29065040650406504    training loss:  4.164736266855333\n",
      "iteration  800  :    training acc:  0.22764227642276422    training loss:  4.464320293944995\n",
      "iteration  820  :    training acc:  0.2784552845528455    training loss:  4.752228941058685\n",
      "iteration  840  :    training acc:  0.2967479674796748    training loss:  4.248895254074954\n",
      "iteration  860  :    training acc:  0.3079268292682927    training loss:  3.7981456040259314\n",
      "iteration  880  :    training acc:  0.19715447154471544    training loss:  6.8691443475918135\n",
      "iteration  900  :    training acc:  0.12804878048780488    training loss:  16.68886370224433\n",
      "iteration  920  :    training acc:  0.32926829268292684    training loss:  4.818495032150704\n",
      "iteration  940  :    training acc:  0.3475609756097561    training loss:  4.596273418721372\n",
      "iteration  960  :    training acc:  0.3709349593495935    training loss:  4.5311538814825285\n",
      "iteration  980  :    training acc:  0.29573170731707316    training loss:  6.118057635401131\n",
      "iteration  1000  :    training acc:  0.37296747967479676    training loss:  4.442570000292609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1020  :    training acc:  0.3231707317073171    training loss:  5.245444119868799\n",
      "iteration  1040  :    training acc:  0.2571138211382114    training loss:  4.9706303170347645\n",
      "iteration  1060  :    training acc:  0.3638211382113821    training loss:  3.57182399080702\n",
      "iteration  1080  :    training acc:  0.391260162601626    training loss:  4.297042599927051\n",
      "iteration  1100  :    training acc:  0.16463414634146342    training loss:  nan\n",
      "iteration  1120  :    training acc:  0.37601626016260165    training loss:  7.531771125051178\n",
      "iteration  1140  :    training acc:  0.30386178861788615    training loss:  5.729998770242995\n",
      "iteration  1160  :    training acc:  0.25203252032520324    training loss:  nan\n",
      "iteration  1180  :    training acc:  0.27134146341463417    training loss:  nan\n",
      "iteration  1200  :    training acc:  0.366869918699187    training loss:  5.384509745694575\n",
      "iteration  1220  :    training acc:  0.4136178861788618    training loss:  4.904074687393545\n",
      "iteration  1240  :    training acc:  0.3445121951219512    training loss:  nan\n",
      "iteration  1260  :    training acc:  0.32926829268292684    training loss:  nan\n",
      "iteration  1280  :    training acc:  0.39227642276422764    training loss:  nan\n",
      "iteration  1300  :    training acc:  0.12296747967479675    training loss:  46.343889592095486\n",
      "iteration  1320  :    training acc:  0.3597560975609756    training loss:  6.4243847489261565\n",
      "iteration  1340  :    training acc:  0.3851626016260163    training loss:  5.515481729093326\n",
      "iteration  1360  :    training acc:  0.27134146341463417    training loss:  nan\n",
      "iteration  1380  :    training acc:  0.3953252032520325    training loss:  5.368320033100048\n",
      "iteration  1400  :    training acc:  0.4105691056910569    training loss:  5.275822418407975\n",
      "iteration  1420  :    training acc:  0.40752032520325204    training loss:  7.275308295724575\n",
      "iteration  1440  :    training acc:  0.12601626016260162    training loss:  38.34312634811841\n",
      "iteration  1460  :    training acc:  0.40752032520325204    training loss:  6.064567736593719\n",
      "iteration  1480  :    training acc:  0.3932926829268293    training loss:  6.13397549435145\n",
      "iteration  1500  :    training acc:  0.29471544715447157    training loss:  nan\n",
      "iteration  1520  :    training acc:  0.20426829268292682    training loss:  nan\n",
      "iteration  1540  :    training acc:  0.40447154471544716    training loss:  nan\n",
      "iteration  1560  :    training acc:  0.3394308943089431    training loss:  nan\n",
      "iteration  1580  :    training acc:  0.4095528455284553    training loss:  nan\n",
      "iteration  1600  :    training acc:  0.30284552845528456    training loss:  15.197317654744861\n",
      "iteration  1620  :    training acc:  0.3790650406504065    training loss:  nan\n",
      "iteration  1640  :    training acc:  0.13821138211382114    training loss:  nan\n",
      "iteration  1660  :    training acc:  0.3282520325203252    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.39227642276422764    training loss:  nan\n",
      "iteration  1700  :    training acc:  0.3719512195121951    training loss:  nan\n",
      "iteration  1720  :    training acc:  0.3800813008130081    training loss:  6.554418290466317\n",
      "iteration  1740  :    training acc:  0.3323170731707317    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.2733739837398374    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.3790650406504065    training loss:  nan\n",
      "iteration  1800  :    training acc:  0.3434959349593496    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.42073170731707316    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.383130081300813    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.24288617886178862    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.38109756097560976    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.3150406504065041    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.34959349593495936    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.266260162601626    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.4969512195121951    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.3313008130081301    training loss:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.11736178467507274    training loss:  22.9465860933813\n",
      "iteration  20  :    training acc:  0.15033947623666344    training loss:  3.7646203656710675\n",
      "iteration  40  :    training acc:  0.15324927255092144    training loss:  3.6849586028579937\n",
      "iteration  60  :    training acc:  0.16100872938894278    training loss:  3.6381204622162455\n",
      "iteration  80  :    training acc:  0.16197866149369544    training loss:  3.6374297940168296\n",
      "iteration  100  :    training acc:  0.1658583899127061    training loss:  3.6455017737474114\n",
      "iteration  120  :    training acc:  0.1658583899127061    training loss:  3.7119740402456403\n",
      "iteration  140  :    training acc:  0.1483996120271581    training loss:  3.7185051684333694\n",
      "iteration  160  :    training acc:  0.1600387972841901    training loss:  3.6205188218729\n",
      "iteration  180  :    training acc:  0.1687681862269641    training loss:  3.5871603986435066\n",
      "iteration  200  :    training acc:  0.1784675072744908    training loss:  3.5665144307878585\n",
      "iteration  220  :    training acc:  0.18913676042677013    training loss:  3.548077654131677\n",
      "iteration  240  :    training acc:  0.19107662463627545    training loss:  3.5299769104812833\n",
      "iteration  260  :    training acc:  0.1959262851600388    training loss:  3.519644743368133\n",
      "iteration  280  :    training acc:  0.19980601357904948    training loss:  3.5041418396235393\n",
      "iteration  300  :    training acc:  0.20659553831231814    training loss:  3.4887435408423015\n",
      "iteration  320  :    training acc:  0.2133850630455868    training loss:  3.4606092825701134\n",
      "iteration  340  :    training acc:  0.20368574199806014    training loss:  3.59817392161086\n",
      "iteration  360  :    training acc:  0.21241513094083414    training loss:  3.550197252972283\n",
      "iteration  380  :    training acc:  0.27158098933074687    training loss:  3.1602995935715428\n",
      "iteration  400  :    training acc:  0.1930164888457808    training loss:  3.6242806436147657\n",
      "iteration  420  :    training acc:  0.2735208535402522    training loss:  3.104925470623092\n",
      "iteration  440  :    training acc:  0.1842870999030068    training loss:  3.8627715881050912\n",
      "iteration  460  :    training acc:  0.2696411251212415    training loss:  3.1446659119303426\n",
      "iteration  480  :    training acc:  0.20271580989330748    training loss:  3.9899225499520443\n",
      "iteration  500  :    training acc:  0.25897187196896215    training loss:  3.736885390805549\n",
      "iteration  520  :    training acc:  0.24151309408341415    training loss:  4.23485603872879\n",
      "iteration  540  :    training acc:  0.19495635305528614    training loss:  5.159960549148305\n",
      "iteration  560  :    training acc:  0.2725509214354995    training loss:  3.3155223520522834\n",
      "iteration  580  :    training acc:  0.19980601357904948    training loss:  5.8021647403705385\n",
      "iteration  600  :    training acc:  0.22114451988360814    training loss:  4.089714573228038\n",
      "iteration  620  :    training acc:  0.2696411251212415    training loss:  3.2245299258874254\n",
      "iteration  640  :    training acc:  0.25897187196896215    training loss:  4.160057884237118\n",
      "iteration  660  :    training acc:  0.33365664403491757    training loss:  3.0623345183198203\n",
      "iteration  680  :    training acc:  0.29776915615906885    training loss:  3.382528213754442\n",
      "iteration  700  :    training acc:  0.1813773035887488    training loss:  5.742200159199158\n",
      "iteration  720  :    training acc:  0.24539282250242483    training loss:  5.521762587138076\n",
      "iteration  740  :    training acc:  0.2502424830261882    training loss:  4.761142533398738\n",
      "iteration  760  :    training acc:  0.2677012609117362    training loss:  4.190496546856993\n",
      "iteration  780  :    training acc:  0.2890397672162949    training loss:  4.144366115969614\n",
      "iteration  800  :    training acc:  0.22502424830261883    training loss:  6.117944863288563\n",
      "iteration  820  :    training acc:  0.2162948593598448    training loss:  5.280794878931155\n",
      "iteration  840  :    training acc:  0.24830261881668284    training loss:  7.100188004364809\n",
      "iteration  860  :    training acc:  0.22987390882638215    training loss:  7.812749360626909\n",
      "iteration  880  :    training acc:  0.27740058195926287    training loss:  5.877460554948195\n",
      "iteration  900  :    training acc:  0.2618816682832202    training loss:  7.1893822485389265\n",
      "iteration  920  :    training acc:  0.26285160038797284    training loss:  6.5160918978898135\n",
      "iteration  940  :    training acc:  0.2347235693501455    training loss:  14.903315275754824\n",
      "iteration  960  :    training acc:  0.31910766246362754    training loss:  5.469146966951657\n",
      "iteration  980  :    training acc:  0.1755577109602328    training loss:  10.739151877364044\n",
      "iteration  1000  :    training acc:  0.3113482056256062    training loss:  5.2778183045684655\n",
      "iteration  1020  :    training acc:  0.2948593598448109    training loss:  7.910156249381022\n",
      "iteration  1040  :    training acc:  0.19495635305528614    training loss:  nan\n",
      "iteration  1060  :    training acc:  0.2638215324927255    training loss:  8.037777960981332\n",
      "iteration  1080  :    training acc:  0.20950533462657614    training loss:  nan\n",
      "iteration  1100  :    training acc:  0.1988360814742968    training loss:  15.329256626518529\n",
      "iteration  1120  :    training acc:  0.20853540252182348    training loss:  inf\n",
      "iteration  1140  :    training acc:  0.32783705140640157    training loss:  8.930133092450333\n",
      "iteration  1160  :    training acc:  0.30164888457807953    training loss:  10.151206254714841\n",
      "iteration  1180  :    training acc:  0.25121241513094084    training loss:  11.359890973390593\n",
      "iteration  1200  :    training acc:  0.2192046556741028    training loss:  inf\n",
      "iteration  1220  :    training acc:  0.2550921435499515    training loss:  nan\n",
      "iteration  1240  :    training acc:  0.2502424830261882    training loss:  nan\n",
      "iteration  1260  :    training acc:  0.2696411251212415    training loss:  nan\n",
      "iteration  1280  :    training acc:  0.22211445198836083    training loss:  nan\n",
      "iteration  1300  :    training acc:  0.2725509214354995    training loss:  nan\n",
      "iteration  1320  :    training acc:  0.24442289039767218    training loss:  nan\n",
      "iteration  1340  :    training acc:  0.2259941804073715    training loss:  nan\n",
      "iteration  1360  :    training acc:  0.20853540252182348    training loss:  nan\n",
      "iteration  1380  :    training acc:  0.22405431619786614    training loss:  nan\n",
      "iteration  1400  :    training acc:  0.2492725509214355    training loss:  nan\n",
      "iteration  1420  :    training acc:  0.2075654704170708    training loss:  nan\n",
      "iteration  1440  :    training acc:  0.2880698351115422    training loss:  nan\n",
      "iteration  1460  :    training acc:  0.2580019398642095    training loss:  nan\n",
      "iteration  1480  :    training acc:  0.2550921435499515    training loss:  nan\n",
      "iteration  1500  :    training acc:  0.24733268671193018    training loss:  nan\n",
      "iteration  1520  :    training acc:  0.12900096993210475    training loss:  nan\n",
      "iteration  1540  :    training acc:  0.17070805043646944    training loss:  nan\n",
      "iteration  1560  :    training acc:  0.21241513094083414    training loss:  nan\n",
      "iteration  1580  :    training acc:  0.2725509214354995    training loss:  nan\n",
      "iteration  1600  :    training acc:  0.23278370514064015    training loss:  nan\n",
      "iteration  1620  :    training acc:  0.22308438409311349    training loss:  nan\n",
      "iteration  1640  :    training acc:  0.2162948593598448    training loss:  nan\n",
      "iteration  1660  :    training acc:  0.2259941804073715    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.2550921435499515    training loss:  nan\n",
      "iteration  1700  :    training acc:  0.1842870999030068    training loss:  nan\n",
      "iteration  1720  :    training acc:  0.24151309408341415    training loss:  nan\n",
      "iteration  1740  :    training acc:  0.27740058195926287    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.23084384093113483    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.22502424830261883    training loss:  nan\n",
      "iteration  1800  :    training acc:  0.25994180407371487    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.20271580989330748    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.18622696411251213    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.2502424830261882    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.31328806983511154    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.25315227934044615    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.15227934044616878    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.2259941804073715    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.28419010669253153    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.2735208535402522    training loss:  nan\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.08863198458574181    training loss:  28.832280475237315\n",
      "iteration  20  :    training acc:  0.11946050096339114    training loss:  4.37253356187312\n",
      "iteration  40  :    training acc:  0.11849710982658959    training loss:  4.3561689143688875\n",
      "iteration  60  :    training acc:  0.11464354527938343    training loss:  4.32117505846228\n",
      "iteration  80  :    training acc:  0.11560693641618497    training loss:  4.288479308904125\n",
      "iteration  100  :    training acc:  0.11464354527938343    training loss:  4.267415097279544\n",
      "iteration  120  :    training acc:  0.11560693641618497    training loss:  4.242354135824323\n",
      "iteration  140  :    training acc:  0.11946050096339114    training loss:  4.212861424243671\n",
      "iteration  160  :    training acc:  0.12427745664739884    training loss:  4.278551615742697\n",
      "iteration  180  :    training acc:  0.10115606936416185    training loss:  4.443066041355812\n",
      "iteration  200  :    training acc:  0.1396917148362235    training loss:  4.163369364078823\n",
      "iteration  220  :    training acc:  0.15221579961464354    training loss:  4.096243373283248\n",
      "iteration  240  :    training acc:  0.1329479768786127    training loss:  4.022745391888799\n",
      "iteration  260  :    training acc:  0.11560693641618497    training loss:  5.329541177313925\n",
      "iteration  280  :    training acc:  0.11849710982658959    training loss:  4.6334730399169555\n",
      "iteration  300  :    training acc:  0.11657032755298652    training loss:  4.647574132664412\n",
      "iteration  320  :    training acc:  0.1724470134874759    training loss:  4.189226031835773\n",
      "iteration  340  :    training acc:  0.128131021194605    training loss:  3.877363505246508\n",
      "iteration  360  :    training acc:  0.1599229287090559    training loss:  3.9516011303516705\n",
      "iteration  380  :    training acc:  0.15703275529865124    training loss:  4.553298043294807\n",
      "iteration  400  :    training acc:  0.10886319845857419    training loss:  5.566824713634944\n",
      "iteration  420  :    training acc:  0.16955684007707128    training loss:  3.9570693848775456\n",
      "iteration  440  :    training acc:  0.14739884393063585    training loss:  4.008477992460311\n",
      "iteration  460  :    training acc:  0.1233140655105973    training loss:  4.70453111431335\n",
      "iteration  480  :    training acc:  0.12427745664739884    training loss:  4.286001938916052\n",
      "iteration  500  :    training acc:  0.13391136801541426    training loss:  4.538860039669873\n",
      "iteration  520  :    training acc:  0.1416184971098266    training loss:  4.290820580279768\n",
      "iteration  540  :    training acc:  0.13391136801541426    training loss:  4.750558462705811\n",
      "iteration  560  :    training acc:  0.14258188824662812    training loss:  4.420441843189138\n",
      "iteration  580  :    training acc:  0.18882466281310212    training loss:  3.9554861373119783\n",
      "iteration  600  :    training acc:  0.14836223506743737    training loss:  5.06972041888539\n",
      "iteration  620  :    training acc:  0.1416184971098266    training loss:  4.27982936810058\n",
      "iteration  640  :    training acc:  0.11657032755298652    training loss:  4.321000729613921\n",
      "iteration  660  :    training acc:  0.1936416184971098    training loss:  3.8179169271030813\n",
      "iteration  680  :    training acc:  0.15510597302504817    training loss:  4.149327256173846\n",
      "iteration  700  :    training acc:  0.16473988439306358    training loss:  3.941823889721978\n",
      "iteration  720  :    training acc:  0.20520231213872833    training loss:  3.667852666926194\n",
      "iteration  740  :    training acc:  0.16666666666666666    training loss:  3.7019494068056553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  760  :    training acc:  0.14836223506743737    training loss:  3.7339489331754425\n",
      "iteration  780  :    training acc:  0.12042389210019268    training loss:  4.0651779669397285\n",
      "iteration  800  :    training acc:  0.1396917148362235    training loss:  3.9542483160060202\n",
      "iteration  820  :    training acc:  0.1628131021194605    training loss:  3.7104344936639513\n",
      "iteration  840  :    training acc:  0.151252408477842    training loss:  3.8590820198362943\n",
      "iteration  860  :    training acc:  0.22157996146435452    training loss:  3.5468315917414057\n",
      "iteration  880  :    training acc:  0.18786127167630057    training loss:  3.6174242452414833\n",
      "iteration  900  :    training acc:  0.18689788053949905    training loss:  3.62040411666829\n",
      "iteration  920  :    training acc:  0.18304431599229287    training loss:  3.6122150666333996\n",
      "iteration  940  :    training acc:  0.1859344894026975    training loss:  3.5719801958692514\n",
      "iteration  960  :    training acc:  0.1926782273603083    training loss:  3.547506690392528\n",
      "iteration  980  :    training acc:  0.1994219653179191    training loss:  3.519028559994921\n",
      "iteration  1000  :    training acc:  0.20327552986512523    training loss:  3.4970588377876184\n",
      "iteration  1020  :    training acc:  0.19653179190751446    training loss:  3.485901102082628\n",
      "iteration  1040  :    training acc:  0.18882466281310212    training loss:  3.4920160103529665\n",
      "iteration  1060  :    training acc:  0.17822736030828518    training loss:  3.5188140701525956\n",
      "iteration  1080  :    training acc:  0.19460500963391136    training loss:  3.5240856310025443\n",
      "iteration  1100  :    training acc:  0.1926782273603083    training loss:  3.486511712656137\n",
      "iteration  1120  :    training acc:  0.2003853564547206    training loss:  3.4374755083339963\n",
      "iteration  1140  :    training acc:  0.21579961464354527    training loss:  3.364249532156215\n",
      "iteration  1160  :    training acc:  0.2302504816955684    training loss:  3.2966291631147984\n",
      "iteration  1180  :    training acc:  0.23699421965317918    training loss:  3.2382310641024246\n",
      "iteration  1200  :    training acc:  0.23603082851637766    training loss:  3.196028271826573\n",
      "iteration  1220  :    training acc:  0.2466281310211946    training loss:  3.163332983198513\n",
      "iteration  1240  :    training acc:  0.2581888246628131    training loss:  3.141650734551863\n",
      "iteration  1260  :    training acc:  0.26396917148362237    training loss:  3.139196182725255\n",
      "iteration  1280  :    training acc:  0.26396917148362237    training loss:  3.1604960876023034\n",
      "iteration  1300  :    training acc:  0.26011560693641617    training loss:  3.2126650592845687\n",
      "iteration  1320  :    training acc:  0.2524084778420039    training loss:  3.272564432993565\n",
      "iteration  1340  :    training acc:  0.24470134874759153    training loss:  3.325357586783567\n",
      "iteration  1360  :    training acc:  0.2418111753371869    training loss:  3.3821881731992667\n",
      "iteration  1380  :    training acc:  0.23892100192678228    training loss:  3.4501242723965464\n",
      "iteration  1400  :    training acc:  0.2302504816955684    training loss:  3.653295766854505\n",
      "iteration  1420  :    training acc:  0.22639691714836224    training loss:  3.4779031183479\n",
      "iteration  1440  :    training acc:  0.20520231213872833    training loss:  3.8708270177696087\n",
      "iteration  1460  :    training acc:  0.21483622350674375    training loss:  4.726871276125522\n",
      "iteration  1480  :    training acc:  0.2610789980732177    training loss:  3.160168799120878\n",
      "iteration  1500  :    training acc:  0.24470134874759153    training loss:  4.004045863576368\n",
      "iteration  1520  :    training acc:  0.20520231213872833    training loss:  3.752666679981225\n",
      "iteration  1540  :    training acc:  0.23410404624277456    training loss:  4.596768580476212\n",
      "iteration  1560  :    training acc:  0.2947976878612717    training loss:  3.8472486426958583\n",
      "iteration  1580  :    training acc:  0.22928709055876687    training loss:  6.55841438649778\n",
      "iteration  1600  :    training acc:  0.18208092485549132    training loss:  7.852475488660581\n",
      "iteration  1620  :    training acc:  0.1464354527938343    training loss:  16.037022276681494\n",
      "iteration  1640  :    training acc:  0.151252408477842    training loss:  11.587674183690332\n",
      "iteration  1660  :    training acc:  0.10500963391136801    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.21772639691714837    training loss:  7.318468620886412\n",
      "iteration  1700  :    training acc:  0.2023121387283237    training loss:  12.6500148920787\n",
      "iteration  1720  :    training acc:  0.19460500963391136    training loss:  nan\n",
      "iteration  1740  :    training acc:  0.1955684007707129    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.20905587668593448    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.1599229287090559    training loss:  nan\n",
      "iteration  1800  :    training acc:  0.12909441233140656    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.20616570327552985    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.17630057803468208    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.16666666666666666    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.18208092485549132    training loss:  inf\n",
      "iteration  1900  :    training acc:  0.14065510597302505    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.22254335260115607    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.13198458574181118    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.10982658959537572    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.1252408477842004    training loss:  nan\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.09421000981354269    training loss:  19.459456974332966\n",
      "iteration  20  :    training acc:  0.11776251226692837    training loss:  5.323458859875918\n",
      "iteration  40  :    training acc:  0.13150147203140333    training loss:  4.537180800519\n",
      "iteration  60  :    training acc:  0.14033366045142295    training loss:  5.27047989034424\n",
      "iteration  80  :    training acc:  0.14327772325809618    training loss:  5.519512011449921\n",
      "iteration  100  :    training acc:  0.1452404317958783    training loss:  4.399814747273621\n",
      "iteration  120  :    training acc:  0.1069676153091266    training loss:  5.540535592151402\n",
      "iteration  140  :    training acc:  0.15014720314033367    training loss:  5.163205522760713\n",
      "iteration  160  :    training acc:  0.13346418056918546    training loss:  5.703559499854621\n",
      "iteration  180  :    training acc:  0.13150147203140333    training loss:  6.493504132466753\n",
      "iteration  200  :    training acc:  0.16977428851815504    training loss:  3.9778677045686055\n",
      "iteration  220  :    training acc:  0.1324828263002944    training loss:  5.761186875966423\n",
      "iteration  240  :    training acc:  0.13346418056918546    training loss:  6.844473010476715\n",
      "iteration  260  :    training acc:  0.1648675171736997    training loss:  4.7628134152557795\n",
      "iteration  280  :    training acc:  0.1256133464180569    training loss:  6.814370033699901\n",
      "iteration  300  :    training acc:  0.12266928361138371    training loss:  5.928675246735406\n",
      "iteration  320  :    training acc:  0.15407262021589793    training loss:  5.00299228271145\n",
      "iteration  340  :    training acc:  0.17075564278704614    training loss:  5.218645323367938\n",
      "iteration  360  :    training acc:  0.14425907752698724    training loss:  5.534964252295958\n",
      "iteration  380  :    training acc:  0.13738959764474976    training loss:  8.115101218121488\n",
      "iteration  400  :    training acc:  0.15701668302257116    training loss:  5.147492509066666\n",
      "iteration  420  :    training acc:  0.14327772325809618    training loss:  5.796625551647608\n",
      "iteration  440  :    training acc:  0.1619234543670265    training loss:  5.097426739733797\n",
      "iteration  460  :    training acc:  0.18645731108930325    training loss:  4.430351662391712\n",
      "iteration  480  :    training acc:  0.10402355250245339    training loss:  4.52874826897488\n",
      "iteration  500  :    training acc:  0.12855740922473013    training loss:  6.733417416767658\n",
      "iteration  520  :    training acc:  0.21589793915603533    training loss:  4.078490015746543\n",
      "iteration  540  :    training acc:  0.20902845927379785    training loss:  3.8400422335465016\n",
      "iteration  560  :    training acc:  0.2129538763493621    training loss:  4.216559930868746\n",
      "iteration  580  :    training acc:  0.14720314033366044    training loss:  4.327241710572574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  600  :    training acc:  0.14720314033366044    training loss:  4.870487116824506\n",
      "iteration  620  :    training acc:  0.1844946025515211    training loss:  3.8188754081793177\n",
      "iteration  640  :    training acc:  0.15407262021589793    training loss:  4.329027970369885\n",
      "iteration  660  :    training acc:  0.2129538763493621    training loss:  3.401555647772595\n",
      "iteration  680  :    training acc:  0.15701668302257116    training loss:  4.137725625699676\n",
      "iteration  700  :    training acc:  0.1393523061825319    training loss:  4.0934746529304755\n",
      "iteration  720  :    training acc:  0.15701668302257116    training loss:  4.080296189721301\n",
      "iteration  740  :    training acc:  0.17958783120706576    training loss:  3.8935364839419204\n",
      "iteration  760  :    training acc:  0.24141315014720313    training loss:  3.4343167758073148\n",
      "iteration  780  :    training acc:  0.2041216879293425    training loss:  3.509627327930123\n",
      "iteration  800  :    training acc:  0.1324828263002944    training loss:  4.8555215810483405\n",
      "iteration  820  :    training acc:  0.20019627085377822    training loss:  3.5087405194819934\n",
      "iteration  840  :    training acc:  0.15309126594700687    training loss:  3.7712505856227816\n",
      "iteration  860  :    training acc:  0.17369970559371933    training loss:  4.365509395134558\n",
      "iteration  880  :    training acc:  0.17075564278704614    training loss:  4.13378276914962\n",
      "iteration  900  :    training acc:  0.18351324828263002    training loss:  3.65733703352077\n",
      "iteration  920  :    training acc:  0.1256133464180569    training loss:  5.53308174901428\n",
      "iteration  940  :    training acc:  0.1324828263002944    training loss:  4.153534779005226\n",
      "iteration  960  :    training acc:  0.15407262021589793    training loss:  3.991408959290183\n",
      "iteration  980  :    training acc:  0.1452404317958783    training loss:  3.9863354517048157\n",
      "iteration  1000  :    training acc:  0.17762512266928362    training loss:  3.9414153744786753\n",
      "iteration  1020  :    training acc:  0.16584887144259078    training loss:  3.9632230653963556\n",
      "iteration  1040  :    training acc:  0.25318940137389595    training loss:  3.3792274310142543\n",
      "iteration  1060  :    training acc:  0.17566241413150147    training loss:  3.9644508252366686\n",
      "iteration  1080  :    training acc:  0.17958783120706576    training loss:  4.024695035847303\n",
      "iteration  1100  :    training acc:  0.13444553483807656    training loss:  5.188872749147715\n",
      "iteration  1120  :    training acc:  0.22276741903827282    training loss:  3.642998689635954\n",
      "iteration  1140  :    training acc:  0.17664376840039253    training loss:  3.9277166157606356\n",
      "iteration  1160  :    training acc:  0.14720314033366044    training loss:  4.459840917722698\n",
      "iteration  1180  :    training acc:  0.15014720314033367    training loss:  4.170528442083931\n",
      "iteration  1200  :    training acc:  0.2168792934249264    training loss:  3.665725698958322\n",
      "iteration  1220  :    training acc:  0.1746810598626104    training loss:  3.879981335631905\n",
      "iteration  1240  :    training acc:  0.1324828263002944    training loss:  4.718709064713763\n",
      "iteration  1260  :    training acc:  0.17369970559371933    training loss:  4.035593918403838\n",
      "iteration  1280  :    training acc:  0.28361138370951916    training loss:  3.3989039954390474\n",
      "iteration  1300  :    training acc:  0.2757605495583906    training loss:  3.6476466429858574\n",
      "iteration  1320  :    training acc:  0.20019627085377822    training loss:  3.5709846383169697\n",
      "iteration  1340  :    training acc:  0.19234543670264967    training loss:  3.823991731186349\n",
      "iteration  1360  :    training acc:  0.1393523061825319    training loss:  4.503307414123001\n",
      "iteration  1380  :    training acc:  0.22865554465161925    training loss:  3.6342528328163772\n",
      "iteration  1400  :    training acc:  0.2777232580961727    training loss:  3.560993476699699\n",
      "iteration  1420  :    training acc:  0.31599607458292445    training loss:  3.294679282013015\n",
      "iteration  1440  :    training acc:  0.30814524043179586    training loss:  3.330969589883304\n",
      "iteration  1460  :    training acc:  0.19921491658488713    training loss:  3.6857864592083964\n",
      "iteration  1480  :    training acc:  0.2747791952894995    training loss:  3.630172220903355\n",
      "iteration  1500  :    training acc:  0.1560353287536801    training loss:  4.540096101421442\n",
      "iteration  1520  :    training acc:  0.31108930323846906    training loss:  3.2565542806297842\n",
      "iteration  1540  :    training acc:  0.30618253189401373    training loss:  3.4027321296898854\n",
      "iteration  1560  :    training acc:  0.2914622178606477    training loss:  3.326386022718104\n",
      "iteration  1580  :    training acc:  0.24435721295387636    training loss:  3.615477946919613\n",
      "iteration  1600  :    training acc:  0.1903827281648675    training loss:  3.82529323968827\n",
      "iteration  1620  :    training acc:  0.24533856722276742    training loss:  3.532809935907438\n",
      "iteration  1640  :    training acc:  0.24533856722276742    training loss:  3.764809402485704\n",
      "iteration  1660  :    training acc:  0.2983316977428852    training loss:  3.336162914405765\n",
      "iteration  1680  :    training acc:  0.30814524043179586    training loss:  3.312846328223542\n",
      "iteration  1700  :    training acc:  0.25318940137389595    training loss:  3.4838577918506637\n",
      "iteration  1720  :    training acc:  0.30814524043179586    training loss:  3.3385934115614027\n",
      "iteration  1740  :    training acc:  0.27379784102060845    training loss:  3.491829801422755\n",
      "iteration  1760  :    training acc:  0.2522080471050049    training loss:  3.671935948845564\n",
      "iteration  1780  :    training acc:  0.25417075564278707    training loss:  3.6758379809512536\n",
      "iteration  1800  :    training acc:  0.2875368007850834    training loss:  3.524214636833346\n",
      "iteration  1820  :    training acc:  0.2855740922473013    training loss:  3.355960579058927\n",
      "iteration  1840  :    training acc:  0.1844946025515211    training loss:  4.082660979969312\n",
      "iteration  1860  :    training acc:  0.1648675171736997    training loss:  4.490115051546651\n",
      "iteration  1880  :    training acc:  0.16683022571148184    training loss:  4.427363872146642\n",
      "iteration  1900  :    training acc:  0.1874386653581943    training loss:  4.219870016980374\n",
      "iteration  1920  :    training acc:  0.17664376840039253    training loss:  4.332481501222098\n",
      "iteration  1940  :    training acc:  0.19921491658488713    training loss:  4.105609572586565\n",
      "iteration  1960  :    training acc:  0.18547595682041218    training loss:  4.258485836335829\n",
      "iteration  1980  :    training acc:  0.2011776251226693    training loss:  4.1123684824223545\n"
     ]
    }
   ],
   "source": [
    "cross_validation=5\n",
    "valid_split = np.arange(0.02, 0.3, 0.3)\n",
    "\n",
    "# best_pred = [accuracy, nn, training trend]\n",
    "best_pred = (0, 0, 0, [])\n",
    "\n",
    "# best_split = [accuracy, split]\n",
    "\n",
    "for split in valid_split:\n",
    "  split_acc = 0\n",
    "  for valid in range(cross_validation):\n",
    "      print(\"\\nCross Validation fold \", valid)\n",
    "\n",
    "      # randomly split the dataset into validation and training sets \n",
    "      mask = np.random.rand(train_x.shape[0]) <= split\n",
    "      t_x = train_x[mask]\n",
    "      t_y = train_y[mask]\n",
    "\n",
    "      v_x = train_x[~mask]\n",
    "      v_y = train_y[~mask]\n",
    "\n",
    "      nn = NN(ni=4096, nh=6, no=10)\n",
    "\n",
    "      res = nn.train(t_x, t_y, 2000)\n",
    "\n",
    "      # validate with validation set after the training\n",
    "      v_o = nn.predict(v_x)\n",
    "      pred = np.argmax(v_o, axis=1)\n",
    "      diff = v_y - pred\n",
    "      acc = (diff == 0).sum() / len(v_y)\n",
    "      \n",
    "      split_acc = acc / cross_validation\n",
    "      \n",
    "  if(split_acc > best_pred[0]): best_pred = (acc, split, nn, res) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1751,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 743,
     "status": "ok",
     "timestamp": 1521699048885,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "F_U4a2xhTEyX",
    "outputId": "5db31968-dd5b-47a0-ea29-9e76641a00c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10930166115668748,\n",
       " <__main__.NN at 0x7fac79f2ed30>,\n",
       " [[0.10220440881763528, 20.076070622095294],\n",
       "  [0.11122244488977956, 5.485008666357058],\n",
       "  [0.12024048096192384, 5.018155693779725],\n",
       "  [0.10821643286573146, 4.8825396710688524],\n",
       "  [0.12024048096192384, 4.370013020212215],\n",
       "  [0.11923847695390781, 4.933058051931166],\n",
       "  [0.09218436873747494, 5.414536912112887],\n",
       "  [0.11923847695390781, 4.64240046049883],\n",
       "  [0.1462925851703407, 4.555238653445118],\n",
       "  [0.14128256513026052, 5.7503847162268675],\n",
       "  [0.13927855711422846, 5.920289401356548],\n",
       "  [0.11823647294589178, 7.771202211509818],\n",
       "  [0.11222444889779559, 5.237119051734977],\n",
       "  [0.12625250501002003, 5.44081325834322],\n",
       "  [0.12124248496993988, 4.935997313471258],\n",
       "  [0.1623246492985972, 4.130458129448985],\n",
       "  [0.1533066132264529, 4.9003585931013625],\n",
       "  [0.14428857715430862, 5.752087312723277],\n",
       "  [0.156312625250501, 4.410680563737459],\n",
       "  [0.13226452905811623, 4.3248131660173845],\n",
       "  [0.12925851703406813, 5.338213982519033],\n",
       "  [0.1653306613226453, 4.330261645987499],\n",
       "  [0.12925851703406813, 4.833983071189972],\n",
       "  [0.15731462925851702, 4.6686909142768585],\n",
       "  [0.17034068136272545, 3.9310881899814683],\n",
       "  [0.1282565130260521, 6.396734515099],\n",
       "  [0.19438877755511022, 4.304168102965169],\n",
       "  [0.14829659318637275, 4.7921683644095925],\n",
       "  [0.14128256513026052, 6.074860657602199],\n",
       "  [0.1492985971943888, 4.554929064649639],\n",
       "  [0.18937875751503006, 5.0632366008054],\n",
       "  [0.18837675350701402, 3.8522382243092617],\n",
       "  [0.22044088176352705, 3.7777233793606664],\n",
       "  [0.1933867735470942, 4.401699734357024],\n",
       "  [0.18236472945891782, 3.8418667956871255],\n",
       "  [0.20140280561122245, 4.007759247077215],\n",
       "  [0.18837675350701402, 3.8273170266914307],\n",
       "  [0.20741482965931865, 4.0278893999852805],\n",
       "  [0.2094188376753507, 3.7313552250842306],\n",
       "  [0.15831663326653306, 3.9319251307008893],\n",
       "  [0.20340681362725452, 4.148307496069687],\n",
       "  [0.21943887775551102, 3.771423659648992],\n",
       "  [0.2124248496993988, 3.7270065378669144],\n",
       "  [0.22144288577154309, 3.80234400915976],\n",
       "  [0.1903807615230461, 4.269773334723763],\n",
       "  [0.1843687374749499, 4.050346497061963],\n",
       "  [0.23847695390781562, 3.8269699535560844],\n",
       "  [0.14428857715430862, 4.687038858707627],\n",
       "  [0.19238476953907815, 4.436847523670442],\n",
       "  [0.22344689378757515, 4.008877134643762],\n",
       "  [0.24348697394789579, 4.116795257500651],\n",
       "  [0.24549098196392785, 3.705925063245093],\n",
       "  [0.19438877755511022, 4.225084935573498],\n",
       "  [0.25250501002004005, 4.571185599279875],\n",
       "  [0.23647294589178355, 3.606210961387474],\n",
       "  [0.22545090180360722, 3.8963717138176976],\n",
       "  [0.2545090180360721, 3.651769354043683],\n",
       "  [0.27054108216432865, 3.770008463631866],\n",
       "  [0.2575150300601202, 3.567889322559394],\n",
       "  [0.2565130260521042, 3.8747266945059433],\n",
       "  [0.28256513026052105, 3.745172396055234],\n",
       "  [0.25250501002004005, 3.922817293343461],\n",
       "  [0.2895791583166333, 3.853293233936737],\n",
       "  [0.2755511022044088, 4.239306494092906],\n",
       "  [0.23847695390781562, 4.8175851391068525],\n",
       "  [0.17635270541082165, 5.923625784375851],\n",
       "  [0.26452905811623245, 3.7309207241887097],\n",
       "  [0.2595190380761523, 4.212289354584735],\n",
       "  [0.1933867735470942, 6.211444770908746],\n",
       "  [0.17034068136272545, 6.819820622457574],\n",
       "  [0.23947895791583165, 4.264581361717271],\n",
       "  [0.24549098196392785, 5.68247502440773],\n",
       "  [0.32064128256513025, 3.7744908217713533],\n",
       "  [0.20140280561122245, 4.242703765331539],\n",
       "  [0.3196392785571142, 3.6718698647747816],\n",
       "  [0.2865731462925852, 3.822461310565312],\n",
       "  [0.2535070140280561, 4.236457173297689],\n",
       "  [0.27955911823647295, 4.080218623394396],\n",
       "  [0.31162324649298595, 3.9580428254877167],\n",
       "  [0.17034068136272545, 4.716685165433042],\n",
       "  [0.1713426853707415, 5.441287382336427],\n",
       "  [0.17735470941883769, 6.107962887433738],\n",
       "  [0.2655310621242485, 4.499991823207197],\n",
       "  [0.3316633266533066, 3.8285474700651796],\n",
       "  [0.2565130260521042, 5.957470879115799],\n",
       "  [0.32665330661322645, 3.6950724614765553],\n",
       "  [0.2755511022044088, 4.018831640810356],\n",
       "  [0.24549098196392785, 4.534226603694733],\n",
       "  [0.2845691382765531, 4.342671891434758],\n",
       "  [0.27955911823647295, 4.236846877235315],\n",
       "  [0.19539078156312625, 6.103252580810492],\n",
       "  [0.2965931863727455, 4.620272458737352],\n",
       "  [0.27354709418837675, 4.354895975861632],\n",
       "  [0.2775551102204409, 4.53718801931804],\n",
       "  [0.3186372745490982, 3.711592017111984],\n",
       "  [0.1623246492985972, 7.4695715895606245],\n",
       "  [0.3196392785571142, 4.095902566057788],\n",
       "  [0.31763527054108215, 3.655929688617255],\n",
       "  [0.3376753507014028, 3.7741171854456304],\n",
       "  [0.2715430861723447, 5.157944939323837]])"
      ]
     },
     "execution_count": 90,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "t_tnszi-VMp9"
   },
   "outputs": [],
   "source": [
    "best_split = best_pred[1]\n",
    "\n",
    "best_nn = best_pred[2]\n",
    "\n",
    "pred = best_nn.predict(test_x)\n",
    "pred = np.argmax(v_o, axis=1)\n",
    "\n",
    "arr = np.arange(len(pred))\n",
    "\n",
    "np.savetxt('nn_prediction_param_valid_split.csv', np.dstack((arr, pred))[0], \"%d,%d\", header = \"Id,Label\", comments='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1768,
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3060,
     "status": "ok",
     "timestamp": 1521729061658,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "mHGsbWYLVRhv",
    "outputId": "ae122d3f-220a-4e93-f0c6-304903d6f4a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11718829750311345,\n",
       " 0.02,\n",
       " <__main__.NN at 0x7f4135705f98>,\n",
       " [[0.09421000981354269, 19.459456974332966],\n",
       "  [0.11776251226692837, 5.323458859875918],\n",
       "  [0.13150147203140333, 4.537180800519],\n",
       "  [0.14033366045142295, 5.27047989034424],\n",
       "  [0.14327772325809618, 5.519512011449921],\n",
       "  [0.1452404317958783, 4.399814747273621],\n",
       "  [0.1069676153091266, 5.540535592151402],\n",
       "  [0.15014720314033367, 5.163205522760713],\n",
       "  [0.13346418056918546, 5.703559499854621],\n",
       "  [0.13150147203140333, 6.493504132466753],\n",
       "  [0.16977428851815504, 3.9778677045686055],\n",
       "  [0.1324828263002944, 5.761186875966423],\n",
       "  [0.13346418056918546, 6.844473010476715],\n",
       "  [0.1648675171736997, 4.7628134152557795],\n",
       "  [0.1256133464180569, 6.814370033699901],\n",
       "  [0.12266928361138371, 5.928675246735406],\n",
       "  [0.15407262021589793, 5.00299228271145],\n",
       "  [0.17075564278704614, 5.218645323367938],\n",
       "  [0.14425907752698724, 5.534964252295958],\n",
       "  [0.13738959764474976, 8.115101218121488],\n",
       "  [0.15701668302257116, 5.147492509066666],\n",
       "  [0.14327772325809618, 5.796625551647608],\n",
       "  [0.1619234543670265, 5.097426739733797],\n",
       "  [0.18645731108930325, 4.430351662391712],\n",
       "  [0.10402355250245339, 4.52874826897488],\n",
       "  [0.12855740922473013, 6.733417416767658],\n",
       "  [0.21589793915603533, 4.078490015746543],\n",
       "  [0.20902845927379785, 3.8400422335465016],\n",
       "  [0.2129538763493621, 4.216559930868746],\n",
       "  [0.14720314033366044, 4.327241710572574],\n",
       "  [0.14720314033366044, 4.870487116824506],\n",
       "  [0.1844946025515211, 3.8188754081793177],\n",
       "  [0.15407262021589793, 4.329027970369885],\n",
       "  [0.2129538763493621, 3.401555647772595],\n",
       "  [0.15701668302257116, 4.137725625699676],\n",
       "  [0.1393523061825319, 4.0934746529304755],\n",
       "  [0.15701668302257116, 4.080296189721301],\n",
       "  [0.17958783120706576, 3.8935364839419204],\n",
       "  [0.24141315014720313, 3.4343167758073148],\n",
       "  [0.2041216879293425, 3.509627327930123],\n",
       "  [0.1324828263002944, 4.8555215810483405],\n",
       "  [0.20019627085377822, 3.5087405194819934],\n",
       "  [0.15309126594700687, 3.7712505856227816],\n",
       "  [0.17369970559371933, 4.365509395134558],\n",
       "  [0.17075564278704614, 4.13378276914962],\n",
       "  [0.18351324828263002, 3.65733703352077],\n",
       "  [0.1256133464180569, 5.53308174901428],\n",
       "  [0.1324828263002944, 4.153534779005226],\n",
       "  [0.15407262021589793, 3.991408959290183],\n",
       "  [0.1452404317958783, 3.9863354517048157],\n",
       "  [0.17762512266928362, 3.9414153744786753],\n",
       "  [0.16584887144259078, 3.9632230653963556],\n",
       "  [0.25318940137389595, 3.3792274310142543],\n",
       "  [0.17566241413150147, 3.9644508252366686],\n",
       "  [0.17958783120706576, 4.024695035847303],\n",
       "  [0.13444553483807656, 5.188872749147715],\n",
       "  [0.22276741903827282, 3.642998689635954],\n",
       "  [0.17664376840039253, 3.9277166157606356],\n",
       "  [0.14720314033366044, 4.459840917722698],\n",
       "  [0.15014720314033367, 4.170528442083931],\n",
       "  [0.2168792934249264, 3.665725698958322],\n",
       "  [0.1746810598626104, 3.879981335631905],\n",
       "  [0.1324828263002944, 4.718709064713763],\n",
       "  [0.17369970559371933, 4.035593918403838],\n",
       "  [0.28361138370951916, 3.3989039954390474],\n",
       "  [0.2757605495583906, 3.6476466429858574],\n",
       "  [0.20019627085377822, 3.5709846383169697],\n",
       "  [0.19234543670264967, 3.823991731186349],\n",
       "  [0.1393523061825319, 4.503307414123001],\n",
       "  [0.22865554465161925, 3.6342528328163772],\n",
       "  [0.2777232580961727, 3.560993476699699],\n",
       "  [0.31599607458292445, 3.294679282013015],\n",
       "  [0.30814524043179586, 3.330969589883304],\n",
       "  [0.19921491658488713, 3.6857864592083964],\n",
       "  [0.2747791952894995, 3.630172220903355],\n",
       "  [0.1560353287536801, 4.540096101421442],\n",
       "  [0.31108930323846906, 3.2565542806297842],\n",
       "  [0.30618253189401373, 3.4027321296898854],\n",
       "  [0.2914622178606477, 3.326386022718104],\n",
       "  [0.24435721295387636, 3.615477946919613],\n",
       "  [0.1903827281648675, 3.82529323968827],\n",
       "  [0.24533856722276742, 3.532809935907438],\n",
       "  [0.24533856722276742, 3.764809402485704],\n",
       "  [0.2983316977428852, 3.336162914405765],\n",
       "  [0.30814524043179586, 3.312846328223542],\n",
       "  [0.25318940137389595, 3.4838577918506637],\n",
       "  [0.30814524043179586, 3.3385934115614027],\n",
       "  [0.27379784102060845, 3.491829801422755],\n",
       "  [0.2522080471050049, 3.671935948845564],\n",
       "  [0.25417075564278707, 3.6758379809512536],\n",
       "  [0.2875368007850834, 3.524214636833346],\n",
       "  [0.2855740922473013, 3.355960579058927],\n",
       "  [0.1844946025515211, 4.082660979969312],\n",
       "  [0.1648675171736997, 4.490115051546651],\n",
       "  [0.16683022571148184, 4.427363872146642],\n",
       "  [0.1874386653581943, 4.219870016980374],\n",
       "  [0.17664376840039253, 4.332481501222098],\n",
       "  [0.19921491658488713, 4.105609572586565],\n",
       "  [0.18547595682041218, 4.258485836335829],\n",
       "  [0.2011776251226693, 4.1123684824223545]])"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 52105,
     "output_extras": [
      {
       "item_id": 37
      },
      {
       "item_id": 63
      },
      {
       "item_id": 69
      },
      {
       "item_id": 70
      },
      {
       "item_id": 87
      },
      {
       "item_id": 118
      },
      {
       "item_id": 149
      },
      {
       "item_id": 180
      },
      {
       "item_id": 199
      },
      {
       "item_id": 216
      },
      {
       "item_id": 246
      },
      {
       "item_id": 279
      },
      {
       "item_id": 310
      },
      {
       "item_id": 353
      },
      {
       "item_id": 397
      },
      {
       "item_id": 442
      },
      {
       "item_id": 484
      },
      {
       "item_id": 530
      },
      {
       "item_id": 575
      },
      {
       "item_id": 619
      },
      {
       "item_id": 663
      },
      {
       "item_id": 702
      },
      {
       "item_id": 744
      },
      {
       "item_id": 787
      },
      {
       "item_id": 831
      },
      {
       "item_id": 875
      },
      {
       "item_id": 921
      },
      {
       "item_id": 965
      },
      {
       "item_id": 1010
      },
      {
       "item_id": 1053
      },
      {
       "item_id": 1098
      },
      {
       "item_id": 1143
      },
      {
       "item_id": 1186
      },
      {
       "item_id": 1230
      },
      {
       "item_id": 1275
      },
      {
       "item_id": 1284
      },
      {
       "item_id": 1285
      },
      {
       "item_id": 1330
      },
      {
       "item_id": 1375
      },
      {
       "item_id": 1420
      },
      {
       "item_id": 1464
      },
      {
       "item_id": 1509
      },
      {
       "item_id": 1554
      },
      {
       "item_id": 1602
      },
      {
       "item_id": 1647
      },
      {
       "item_id": 1692
      },
      {
       "item_id": 1737
      },
      {
       "item_id": 1782
      },
      {
       "item_id": 1828
      },
      {
       "item_id": 1873
      },
      {
       "item_id": 1919
      },
      {
       "item_id": 1962
      },
      {
       "item_id": 2006
      },
      {
       "item_id": 2050
      },
      {
       "item_id": 2096
      },
      {
       "item_id": 2140
      },
      {
       "item_id": 2184
      },
      {
       "item_id": 2228
      },
      {
       "item_id": 2270
      },
      {
       "item_id": 2315
      },
      {
       "item_id": 2359
      },
      {
       "item_id": 2405
      },
      {
       "item_id": 2450
      },
      {
       "item_id": 2495
      },
      {
       "item_id": 2540
      },
      {
       "item_id": 2585
      },
      {
       "item_id": 2630
      },
      {
       "item_id": 2675
      },
      {
       "item_id": 2721
      },
      {
       "item_id": 2766
      },
      {
       "item_id": 2782
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4139899,
     "status": "ok",
     "timestamp": 1521736077304,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "yu1e5xeXHrbX",
    "outputId": "2bcec3c9-6d06-4f06-9630-7702caa348f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.10181086519114688    training loss:  13.633564920751903\n",
      "iteration  20  :    training acc:  0.10583501006036217    training loss:  3.396722801114684\n",
      "iteration  40  :    training acc:  0.10583501006036217    training loss:  3.3954663083169816\n",
      "iteration  60  :    training acc:  0.10583501006036217    training loss:  3.3942995916620546\n",
      "iteration  80  :    training acc:  0.10583501006036217    training loss:  3.3932116010885705\n",
      "iteration  100  :    training acc:  0.10583501006036217    training loss:  3.39219293645362\n",
      "iteration  120  :    training acc:  0.10583501006036217    training loss:  3.3912356006156785\n",
      "iteration  140  :    training acc:  0.10583501006036217    training loss:  3.390332777285851\n",
      "iteration  160  :    training acc:  0.10583501006036217    training loss:  3.3894786388846376\n",
      "iteration  180  :    training acc:  0.10583501006036217    training loss:  3.388668184147335\n",
      "iteration  200  :    training acc:  0.10583501006036217    training loss:  3.3878971026988776\n",
      "iteration  220  :    training acc:  0.10583501006036217    training loss:  3.3871616629449584\n",
      "iteration  240  :    training acc:  0.10583501006036217    training loss:  3.3864586195808344\n",
      "iteration  260  :    training acc:  0.10583501006036217    training loss:  3.385785137345821\n",
      "iteration  280  :    training acc:  0.10583501006036217    training loss:  3.3851387281099248\n",
      "iteration  300  :    training acc:  0.10583501006036217    training loss:  3.3845171988502476\n",
      "iteration  320  :    training acc:  0.10583501006036217    training loss:  3.3839186085060367\n",
      "iteration  340  :    training acc:  0.10583501006036217    training loss:  3.3833412320734553\n",
      "iteration  360  :    training acc:  0.10583501006036217    training loss:  3.3827835306121554\n",
      "iteration  380  :    training acc:  0.10583501006036217    training loss:  3.3822441260905998\n",
      "iteration  400  :    training acc:  0.10583501006036217    training loss:  3.38172178020351\n",
      "iteration  420  :    training acc:  0.10583501006036217    training loss:  3.3812153764610486\n",
      "iteration  440  :    training acc:  0.10583501006036217    training loss:  3.380723904982575\n",
      "iteration  460  :    training acc:  0.10583501006036217    training loss:  3.3802464495346336\n",
      "iteration  480  :    training acc:  0.10583501006036217    training loss:  3.379782176438362\n",
      "iteration  500  :    training acc:  0.10583501006036217    training loss:  3.3793303250401987\n",
      "iteration  520  :    training acc:  0.10583501006036217    training loss:  3.3788901994949394\n",
      "iteration  540  :    training acc:  0.10583501006036217    training loss:  3.3784611616547373\n",
      "iteration  560  :    training acc:  0.10583501006036217    training loss:  3.3780426248935944\n",
      "iteration  580  :    training acc:  0.10583501006036217    training loss:  3.377634048726107\n",
      "iteration  600  :    training acc:  0.10583501006036217    training loss:  3.3772349341029644\n",
      "iteration  620  :    training acc:  0.10583501006036217    training loss:  3.3768448192851235\n",
      "iteration  640  :    training acc:  0.10583501006036217    training loss:  3.3764632762144826\n",
      "iteration  660  :    training acc:  0.10583501006036217    training loss:  3.3760899073119317\n",
      "iteration  680  :    training acc:  0.10583501006036217    training loss:  3.375724342644508\n",
      "iteration  700  :    training acc:  0.10583501006036217    training loss:  3.375366237412267\n",
      "iteration  720  :    training acc:  0.10583501006036217    training loss:  3.3750152697129745\n",
      "iteration  740  :    training acc:  0.10583501006036217    training loss:  3.374671138548859\n",
      "iteration  760  :    training acc:  0.10583501006036217    training loss:  3.3743335620449173\n",
      "iteration  780  :    training acc:  0.10583501006036217    training loss:  3.374002275852582\n",
      "iteration  800  :    training acc:  0.10583501006036217    training loss:  3.373677031716223\n",
      "iteration  820  :    training acc:  0.10583501006036217    training loss:  3.373357596183082\n",
      "iteration  840  :    training acc:  0.10583501006036217    training loss:  3.3730437494398546\n",
      "iteration  860  :    training acc:  0.10583501006036217    training loss:  3.3727352842613376\n",
      "iteration  880  :    training acc:  0.10583501006036217    training loss:  3.372432005058513\n",
      "iteration  900  :    training acc:  0.10583501006036217    training loss:  3.3721337270150182\n",
      "iteration  920  :    training acc:  0.10583501006036217    training loss:  3.371840275302378\n",
      "iteration  940  :    training acc:  0.10583501006036217    training loss:  3.3715514843655385\n",
      "iteration  960  :    training acc:  0.10583501006036217    training loss:  3.371267197271326\n",
      "iteration  980  :    training acc:  0.10583501006036217    training loss:  3.3709872651132757\n",
      "iteration  1000  :    training acc:  0.10583501006036217    training loss:  3.370711546467111\n",
      "iteration  1020  :    training acc:  0.10583501006036217    training loss:  3.3704399068917805\n",
      "iteration  1040  :    training acc:  0.10583501006036217    training loss:  3.3701722184715677\n",
      "iteration  1060  :    training acc:  0.10583501006036217    training loss:  3.3699083593952723\n",
      "iteration  1080  :    training acc:  0.10583501006036217    training loss:  3.3696482135689254\n",
      "iteration  1100  :    training acc:  0.10583501006036217    training loss:  3.369391670258887\n",
      "iteration  1120  :    training acc:  0.10583501006036217    training loss:  3.369138623762504\n",
      "iteration  1140  :    training acc:  0.10583501006036217    training loss:  3.368888973103801\n",
      "iteration  1160  :    training acc:  0.10583501006036217    training loss:  3.368642621751978\n",
      "iteration  1180  :    training acc:  0.10583501006036217    training loss:  3.3683994773606636\n",
      "iteration  1200  :    training acc:  0.10583501006036217    training loss:  3.3681594515261324\n",
      "iteration  1220  :    training acc:  0.10583501006036217    training loss:  3.3679224595628408\n",
      "iteration  1240  :    training acc:  0.10583501006036217    training loss:  3.3676884202948165\n",
      "iteration  1260  :    training acc:  0.10583501006036217    training loss:  3.3674572558615687\n",
      "iteration  1280  :    training acc:  0.10583501006036217    training loss:  3.3672288915373247\n",
      "iteration  1300  :    training acc:  0.10583501006036217    training loss:  3.367003255562508\n",
      "iteration  1320  :    training acc:  0.10583501006036217    training loss:  3.366780278986464\n",
      "iteration  1340  :    training acc:  0.10583501006036217    training loss:  3.3665598955205556\n",
      "iteration  1360  :    training acc:  0.10583501006036217    training loss:  3.3663420414008036\n",
      "iteration  1380  :    training acc:  0.10583501006036217    training loss:  3.3661266552593485\n",
      "iteration  1400  :    training acc:  0.10583501006036217    training loss:  3.3659136780040426\n",
      "iteration  1420  :    training acc:  0.10583501006036217    training loss:  3.365703052705576\n",
      "iteration  1440  :    training acc:  0.10583501006036217    training loss:  3.365494724491572\n",
      "iteration  1460  :    training acc:  0.10583501006036217    training loss:  3.3652886404471367\n",
      "iteration  1480  :    training acc:  0.10583501006036217    training loss:  3.3650847495213903\n",
      "iteration  1500  :    training acc:  0.10583501006036217    training loss:  3.3648830024395675\n",
      "iteration  1520  :    training acc:  0.10583501006036217    training loss:  3.364683351620276\n",
      "iteration  1540  :    training acc:  0.10583501006036217    training loss:  3.364485751097555\n",
      "iteration  1560  :    training acc:  0.10583501006036217    training loss:  3.3642901564474204\n",
      "iteration  1580  :    training acc:  0.10583501006036217    training loss:  3.3640965247185495\n",
      "iteration  1600  :    training acc:  0.10583501006036217    training loss:  3.3639048143668755\n",
      "iteration  1620  :    training acc:  0.10583501006036217    training loss:  3.3637149851937895\n",
      "iteration  1640  :    training acc:  0.10583501006036217    training loss:  3.3635269982877363\n",
      "iteration  1660  :    training acc:  0.10583501006036217    training loss:  3.3633408159689666\n",
      "iteration  1680  :    training acc:  0.10583501006036217    training loss:  3.363156401737253\n",
      "iteration  1700  :    training acc:  0.10583501006036217    training loss:  3.3629737202223793\n",
      "iteration  1720  :    training acc:  0.10583501006036217    training loss:  3.3627927371372146\n",
      "iteration  1740  :    training acc:  0.10583501006036217    training loss:  3.3626134192332304\n",
      "iteration  1760  :    training acc:  0.10583501006036217    training loss:  3.3624357342582916\n",
      "iteration  1780  :    training acc:  0.10583501006036217    training loss:  3.3622596509165956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1800  :    training acc:  0.10583501006036217    training loss:  3.362085138830622\n",
      "iteration  1820  :    training acc:  0.10583501006036217    training loss:  3.3619121685049724\n",
      "iteration  1840  :    training acc:  0.10583501006036217    training loss:  3.361740711291986\n",
      "iteration  1860  :    training acc:  0.10583501006036217    training loss:  3.361570739359042\n",
      "iteration  1880  :    training acc:  0.10583501006036217    training loss:  3.3614022256574247\n",
      "iteration  1900  :    training acc:  0.10583501006036217    training loss:  3.3612351438926833\n",
      "iteration  1920  :    training acc:  0.10583501006036217    training loss:  3.361069468496393\n",
      "iteration  1940  :    training acc:  0.10583501006036217    training loss:  3.3609051745992304\n",
      "iteration  1960  :    training acc:  0.10583501006036217    training loss:  3.3607422380053107\n",
      "iteration  1980  :    training acc:  0.10583501006036217    training loss:  3.360580635167689\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.0971563981042654    training loss:  18.014192555666952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in log\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  20  :    training acc:  0.11453396524486571    training loss:  3.696087081946441\n",
      "iteration  40  :    training acc:  0.0971563981042654    training loss:  3.8387131138073145\n",
      "iteration  60  :    training acc:  0.11018957345971564    training loss:  3.976925181674998\n",
      "iteration  80  :    training acc:  0.10584518167456557    training loss:  3.947357387181817\n",
      "iteration  100  :    training acc:  0.11018957345971564    training loss:  3.795010284507426\n",
      "iteration  120  :    training acc:  0.11018957345971564    training loss:  3.8840530525560992\n",
      "iteration  140  :    training acc:  0.11018957345971564    training loss:  3.8217598386128877\n",
      "iteration  160  :    training acc:  0.11018957345971564    training loss:  3.7108421087947194\n",
      "iteration  180  :    training acc:  0.11453396524486571    training loss:  3.81934992951175\n",
      "iteration  200  :    training acc:  0.10584518167456557    training loss:  4.1220104788746825\n",
      "iteration  220  :    training acc:  0.0971563981042654    training loss:  3.7537765148283255\n",
      "iteration  240  :    training acc:  0.11453396524486571    training loss:  3.779057611957703\n",
      "iteration  260  :    training acc:  0.11018957345971564    training loss:  3.6110122338347326\n",
      "iteration  280  :    training acc:  0.11018957345971564    training loss:  3.754053505041657\n",
      "iteration  300  :    training acc:  0.10584518167456557    training loss:  3.9404470979431427\n",
      "iteration  320  :    training acc:  0.11018957345971564    training loss:  3.7841310246813165\n",
      "iteration  340  :    training acc:  0.11018957345971564    training loss:  3.6891021055080353\n",
      "iteration  360  :    training acc:  0.11018957345971564    training loss:  3.729124814671965\n",
      "iteration  380  :    training acc:  0.11018957345971564    training loss:  3.7178727414749093\n",
      "iteration  400  :    training acc:  0.11018957345971564    training loss:  3.92622716479278\n",
      "iteration  420  :    training acc:  0.11018957345971564    training loss:  3.7176119320684977\n",
      "iteration  440  :    training acc:  0.11453396524486571    training loss:  3.8151296758451285\n",
      "iteration  460  :    training acc:  0.10584518167456557    training loss:  3.6585683716696757\n",
      "iteration  480  :    training acc:  0.11453396524486571    training loss:  3.855806663801589\n",
      "iteration  500  :    training acc:  0.11453396524486571    training loss:  3.848726337615651\n",
      "iteration  520  :    training acc:  0.10584518167456557    training loss:  3.9603053830292843\n",
      "iteration  540  :    training acc:  0.11018957345971564    training loss:  3.8219581992932556\n",
      "iteration  560  :    training acc:  0.0971563981042654    training loss:  3.6164110228220205\n",
      "iteration  580  :    training acc:  0.09992101105845182    training loss:  3.609763122045373\n",
      "iteration  600  :    training acc:  0.09992101105845182    training loss:  3.6631520306283605\n",
      "iteration  620  :    training acc:  0.10505529225908374    training loss:  3.763294528460667\n",
      "iteration  640  :    training acc:  0.09992101105845182    training loss:  3.7451379748186624\n",
      "iteration  660  :    training acc:  0.09992101105845182    training loss:  3.696031725417371\n",
      "iteration  680  :    training acc:  0.10505529225908374    training loss:  3.676572342897469\n",
      "iteration  700  :    training acc:  0.10505529225908374    training loss:  3.7314451074259005\n",
      "iteration  720  :    training acc:  0.09518167456556083    training loss:  3.654465284722578\n",
      "iteration  740  :    training acc:  0.09518167456556083    training loss:  3.628301933568813\n",
      "iteration  760  :    training acc:  0.10505529225908374    training loss:  3.796209964257924\n",
      "iteration  780  :    training acc:  0.09518167456556083    training loss:  3.616762000772204\n",
      "iteration  800  :    training acc:  0.10505529225908374    training loss:  3.626787031216605\n",
      "iteration  820  :    training acc:  0.10505529225908374    training loss:  3.7821303068736105\n",
      "iteration  840  :    training acc:  0.10505529225908374    training loss:  3.740980048916235\n",
      "iteration  860  :    training acc:  0.09518167456556083    training loss:  3.668002664963943\n",
      "iteration  880  :    training acc:  0.09518167456556083    training loss:  3.6956133924383425\n",
      "iteration  900  :    training acc:  0.09518167456556083    training loss:  3.6968586278492266\n",
      "iteration  920  :    training acc:  0.09518167456556083    training loss:  3.6731676536700824\n",
      "iteration  940  :    training acc:  0.10505529225908374    training loss:  3.7321638656996683\n",
      "iteration  960  :    training acc:  0.10505529225908374    training loss:  3.658173967949028\n",
      "iteration  980  :    training acc:  0.09518167456556083    training loss:  3.6852355552870226\n",
      "iteration  1000  :    training acc:  0.09518167456556083    training loss:  3.575870629045413\n",
      "iteration  1020  :    training acc:  0.10505529225908374    training loss:  3.7777020233529632\n",
      "iteration  1040  :    training acc:  0.09518167456556083    training loss:  3.6419327502710312\n",
      "iteration  1060  :    training acc:  0.10505529225908374    training loss:  3.7067122618101247\n",
      "iteration  1080  :    training acc:  0.10505529225908374    training loss:  3.7238331197914425\n",
      "iteration  1100  :    training acc:  0.10505529225908374    training loss:  3.7092241873702054\n",
      "iteration  1120  :    training acc:  0.09992101105845182    training loss:  3.7733542022909847\n",
      "iteration  1140  :    training acc:  0.10505529225908374    training loss:  3.7398912050309843\n",
      "iteration  1160  :    training acc:  0.09083728278041074    training loss:  3.587883063709396\n",
      "iteration  1180  :    training acc:  0.09992101105845182    training loss:  3.6568803833773313\n",
      "iteration  1200  :    training acc:  0.11453396524486571    training loss:  3.8013683720370977\n",
      "iteration  1220  :    training acc:  0.09992101105845182    training loss:  3.8042290209198835\n",
      "iteration  1240  :    training acc:  0.11453396524486571    training loss:  3.8780944155947914\n",
      "iteration  1260  :    training acc:  0.09992101105845182    training loss:  3.789619352286737\n",
      "iteration  1280  :    training acc:  0.09992101105845182    training loss:  3.730597167656917\n",
      "iteration  1300  :    training acc:  0.09992101105845182    training loss:  3.6562749718620084\n",
      "iteration  1320  :    training acc:  0.0971563981042654    training loss:  3.693422199229163\n",
      "iteration  1340  :    training acc:  0.09992101105845182    training loss:  3.911084310857464\n",
      "iteration  1360  :    training acc:  0.09992101105845182    training loss:  3.6395558692067898\n",
      "iteration  1380  :    training acc:  0.09992101105845182    training loss:  3.6710051675247257\n",
      "iteration  1400  :    training acc:  0.0971563981042654    training loss:  3.684148624728286\n",
      "iteration  1420  :    training acc:  0.0971563981042654    training loss:  3.843374439581892\n",
      "iteration  1440  :    training acc:  0.09518167456556083    training loss:  3.806712860904368\n",
      "iteration  1460  :    training acc:  0.09518167456556083    training loss:  3.659937766388437\n",
      "iteration  1480  :    training acc:  0.0971563981042654    training loss:  3.883019288554711\n",
      "iteration  1500  :    training acc:  0.0971563981042654    training loss:  3.672617905919529\n",
      "iteration  1520  :    training acc:  0.11453396524486571    training loss:  3.6590972321964217\n",
      "iteration  1540  :    training acc:  0.09992101105845182    training loss:  3.637203173075861\n",
      "iteration  1560  :    training acc:  0.09992101105845182    training loss:  3.6743803494572886\n",
      "iteration  1580  :    training acc:  0.11453396524486571    training loss:  3.7717407253133524\n",
      "iteration  1600  :    training acc:  0.09281200631911532    training loss:  3.6654728680554647\n",
      "iteration  1620  :    training acc:  0.09281200631911532    training loss:  3.8755634484464965\n",
      "iteration  1640  :    training acc:  0.09992101105845182    training loss:  3.664244834296071\n",
      "iteration  1660  :    training acc:  0.09518167456556083    training loss:  3.8004142178407037\n",
      "iteration  1680  :    training acc:  0.09992101105845182    training loss:  3.6806790011972788\n",
      "iteration  1700  :    training acc:  0.09992101105845182    training loss:  3.776316060710154\n",
      "iteration  1720  :    training acc:  0.09281200631911532    training loss:  3.800543408045285\n",
      "iteration  1740  :    training acc:  0.09992101105845182    training loss:  3.706728338530135\n",
      "iteration  1760  :    training acc:  0.09992101105845182    training loss:  3.7286422095724676\n",
      "iteration  1780  :    training acc:  0.09992101105845182    training loss:  3.8755202335921317\n",
      "iteration  1800  :    training acc:  0.09992101105845182    training loss:  3.71951622617855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1820  :    training acc:  0.11453396524486571    training loss:  3.7167397486656797\n",
      "iteration  1840  :    training acc:  0.09992101105845182    training loss:  3.7413927164707985\n",
      "iteration  1860  :    training acc:  0.11453396524486571    training loss:  3.603107072096289\n",
      "iteration  1880  :    training acc:  0.09992101105845182    training loss:  3.8816888583484803\n",
      "iteration  1900  :    training acc:  0.09992101105845182    training loss:  3.800295812909644\n",
      "iteration  1920  :    training acc:  0.09992101105845182    training loss:  3.8462910269922257\n",
      "iteration  1940  :    training acc:  0.09992101105845182    training loss:  3.665770328626257\n",
      "iteration  1960  :    training acc:  0.0971563981042654    training loss:  3.689123112205506\n",
      "iteration  1980  :    training acc:  0.0971563981042654    training loss:  3.6993721706143194\n",
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.10157830837717523    training loss:  15.718963611357259\n",
      "iteration  20  :    training acc:  0.11614730878186968    training loss:  3.2710109809318575\n",
      "iteration  40  :    training acc:  0.1169566976932416    training loss:  3.2635450075990233\n",
      "iteration  60  :    training acc:  0.11655200323755564    training loss:  3.2592300289878535\n",
      "iteration  80  :    training acc:  0.1169566976932416    training loss:  3.2565819734530614\n",
      "iteration  100  :    training acc:  0.1169566976932416    training loss:  3.254907713680298\n",
      "iteration  120  :    training acc:  0.1169566976932416    training loss:  3.2538333354581286\n",
      "iteration  140  :    training acc:  0.1169566976932416    training loss:  3.253139437460728\n",
      "iteration  160  :    training acc:  0.1169566976932416    training loss:  3.2526907519343555\n",
      "iteration  180  :    training acc:  0.1169566976932416    training loss:  3.2524014318101395\n",
      "iteration  200  :    training acc:  0.1169566976932416    training loss:  3.252216085476908\n",
      "iteration  220  :    training acc:  0.11736139214892756    training loss:  3.252098648720623\n",
      "iteration  240  :    training acc:  0.11736139214892756    training loss:  3.2520255359432064\n",
      "iteration  260  :    training acc:  0.11736139214892756    training loss:  3.2519812953367\n",
      "iteration  280  :    training acc:  0.11736139214892756    training loss:  3.2519558020873656\n",
      "iteration  300  :    training acc:  0.11736139214892756    training loss:  3.2519424267776182\n",
      "iteration  320  :    training acc:  0.1169566976932416    training loss:  3.2519368341550416\n",
      "iteration  340  :    training acc:  0.1169566976932416    training loss:  3.2519361937693625\n",
      "iteration  360  :    training acc:  0.1169566976932416    training loss:  3.2519386611384227\n",
      "iteration  380  :    training acc:  0.1169566976932416    training loss:  3.2519430369603533\n",
      "iteration  400  :    training acc:  0.1169566976932416    training loss:  3.2519485435207067\n",
      "iteration  420  :    training acc:  0.1169566976932416    training loss:  3.251954678176525\n",
      "iteration  440  :    training acc:  0.1169566976932416    training loss:  3.2519611174687655\n",
      "iteration  460  :    training acc:  0.1169566976932416    training loss:  3.2519676544441527\n",
      "iteration  480  :    training acc:  0.1169566976932416    training loss:  3.251974157731183\n",
      "iteration  500  :    training acc:  0.1169566976932416    training loss:  3.2519805448487062\n",
      "iteration  520  :    training acc:  0.1169566976932416    training loss:  3.2519867648159884\n",
      "iteration  540  :    training acc:  0.1169566976932416    training loss:  3.25199278683602\n",
      "iteration  560  :    training acc:  0.1169566976932416    training loss:  3.2519985929415016\n",
      "iteration  580  :    training acc:  0.1169566976932416    training loss:  3.2520041732253917\n",
      "iteration  600  :    training acc:  0.1169566976932416    training loss:  3.2520095227572594\n",
      "iteration  620  :    training acc:  0.1169566976932416    training loss:  3.2520146396000924\n",
      "iteration  640  :    training acc:  0.1169566976932416    training loss:  3.252019523546698\n",
      "iteration  660  :    training acc:  0.1169566976932416    training loss:  3.2520241753281525\n",
      "iteration  680  :    training acc:  0.1169566976932416    training loss:  3.252028596133331\n",
      "iteration  700  :    training acc:  0.1169566976932416    training loss:  3.252032787334608\n",
      "iteration  720  :    training acc:  0.11736139214892756    training loss:  3.2520367503508787\n",
      "iteration  740  :    training acc:  0.11736139214892756    training loss:  3.2520404866020813\n",
      "iteration  760  :    training acc:  0.11736139214892756    training loss:  3.252043997523971\n",
      "iteration  780  :    training acc:  0.11776608660461352    training loss:  3.252047284621081\n",
      "iteration  800  :    training acc:  0.11776608660461352    training loss:  3.252050349541635\n",
      "iteration  820  :    training acc:  0.11776608660461352    training loss:  3.2520531941619892\n",
      "iteration  840  :    training acc:  0.11776608660461352    training loss:  3.2520558206709236\n",
      "iteration  860  :    training acc:  0.11776608660461352    training loss:  3.2520582316462567\n",
      "iteration  880  :    training acc:  0.11776608660461352    training loss:  3.252060430118213\n",
      "iteration  900  :    training acc:  0.11776608660461352    training loss:  3.252062419615756\n",
      "iteration  920  :    training acc:  0.11776608660461352    training loss:  3.252064204193823\n",
      "iteration  940  :    training acc:  0.11776608660461352    training loss:  3.252065788440911\n",
      "iteration  960  :    training acc:  0.11776608660461352    training loss:  3.252067177467687\n",
      "iteration  980  :    training acc:  0.11776608660461352    training loss:  3.2520683768782144\n",
      "iteration  1000  :    training acc:  0.11776608660461352    training loss:  3.252069392725856\n",
      "iteration  1020  :    training acc:  0.11776608660461352    training loss:  3.2520702314559853\n",
      "iteration  1040  :    training acc:  0.11776608660461352    training loss:  3.252070899837399\n",
      "iteration  1060  :    training acc:  0.11776608660461352    training loss:  3.2520714048837096\n",
      "iteration  1080  :    training acc:  0.11776608660461352    training loss:  3.252071753765284\n",
      "iteration  1100  :    training acc:  0.11776608660461352    training loss:  3.252071953711468\n",
      "iteration  1120  :    training acc:  0.11776608660461352    training loss:  3.2520720119020234\n",
      "iteration  1140  :    training acc:  0.11817078106029948    training loss:  3.252071935346098\n",
      "iteration  1160  :    training acc:  0.11817078106029948    training loss:  3.2520717307465916\n",
      "iteration  1180  :    training acc:  0.11817078106029948    training loss:  3.2520714043477588\n",
      "iteration  1200  :    training acc:  0.11817078106029948    training loss:  3.252070961764202\n",
      "iteration  1220  :    training acc:  0.11817078106029948    training loss:  3.2520704077902516\n",
      "iteration  1240  :    training acc:  0.11817078106029948    training loss:  3.252069746190178\n",
      "iteration  1260  :    training acc:  0.11817078106029948    training loss:  3.252068979471643\n",
      "iteration  1280  :    training acc:  0.11817078106029948    training loss:  3.252068108647429\n",
      "iteration  1300  :    training acc:  0.11817078106029948    training loss:  3.252067132993644\n",
      "iteration  1320  :    training acc:  0.11817078106029948    training loss:  3.2520660498160843\n",
      "iteration  1340  :    training acc:  0.11817078106029948    training loss:  3.25206485424013\n",
      "iteration  1360  :    training acc:  0.11817078106029948    training loss:  3.2520635390429464\n",
      "iteration  1380  :    training acc:  0.11817078106029948    training loss:  3.2520620945493772\n",
      "iteration  1400  :    training acc:  0.11817078106029948    training loss:  3.2520605086140755\n",
      "iteration  1420  :    training acc:  0.11817078106029948    training loss:  3.252058766711264\n",
      "iteration  1440  :    training acc:  0.11817078106029948    training loss:  3.2520568521491406\n",
      "iteration  1460  :    training acc:  0.11817078106029948    training loss:  3.2520547464176173\n",
      "iteration  1480  :    training acc:  0.11817078106029948    training loss:  3.2520524296652296\n",
      "iteration  1500  :    training acc:  0.11817078106029948    training loss:  3.252049881284204\n",
      "iteration  1520  :    training acc:  0.11857547551598543    training loss:  3.252047080563283\n",
      "iteration  1540  :    training acc:  0.11857547551598543    training loss:  3.252044007349188\n",
      "iteration  1560  :    training acc:  0.11857547551598543    training loss:  3.2520406426438906\n",
      "iteration  1580  :    training acc:  0.11857547551598543    training loss:  3.2520369690610162\n",
      "iteration  1600  :    training acc:  0.11857547551598543    training loss:  3.2520329710742346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1620  :    training acc:  0.11857547551598543    training loss:  3.25202863501407\n",
      "iteration  1640  :    training acc:  0.11857547551598543    training loss:  3.252023948804201\n",
      "iteration  1660  :    training acc:  0.11857547551598543    training loss:  3.2520189014674195\n",
      "iteration  1680  :    training acc:  0.11817078106029948    training loss:  3.2520134824672646\n",
      "iteration  1700  :    training acc:  0.11857547551598543    training loss:  3.2520076809769334\n",
      "iteration  1720  :    training acc:  0.11817078106029948    training loss:  3.2520014851781673\n",
      "iteration  1740  :    training acc:  0.11817078106029948    training loss:  3.251994881688835\n",
      "iteration  1760  :    training acc:  0.11817078106029948    training loss:  3.2519878552007886\n",
      "iteration  1780  :    training acc:  0.11817078106029948    training loss:  3.251980388382522\n",
      "iteration  1800  :    training acc:  0.11817078106029948    training loss:  3.25197246206681\n",
      "iteration  1820  :    training acc:  0.11817078106029948    training loss:  3.2519640557028606\n",
      "iteration  1840  :    training acc:  0.11817078106029948    training loss:  3.25195514800477\n",
      "iteration  1860  :    training acc:  0.11817078106029948    training loss:  3.2519457176720588\n",
      "iteration  1880  :    training acc:  0.11817078106029948    training loss:  3.251935743994819\n",
      "iteration  1900  :    training acc:  0.11817078106029948    training loss:  3.2519252070931124\n",
      "iteration  1920  :    training acc:  0.11857547551598543    training loss:  3.25191408749712\n",
      "iteration  1940  :    training acc:  0.11857547551598543    training loss:  3.2519023647861958\n",
      "iteration  1960  :    training acc:  0.11898016997167139    training loss:  3.2518900151201766\n",
      "iteration  1980  :    training acc:  0.11898016997167139    training loss:  3.2518770077606773\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.10989440750879938    training loss:  13.289667213126258\n",
      "iteration  20  :    training acc:  0.12201798983183418    training loss:  3.249411020688835\n",
      "iteration  40  :    training acc:  0.12123582323034807    training loss:  3.2481395856207493\n",
      "iteration  60  :    training acc:  0.12045365662886194    training loss:  3.2465304584521215\n",
      "iteration  80  :    training acc:  0.12084473992960501    training loss:  3.2448141450370414\n",
      "iteration  100  :    training acc:  0.12436448963629253    training loss:  3.243301174646515\n",
      "iteration  120  :    training acc:  0.12397340633554947    training loss:  3.241952158838095\n",
      "iteration  140  :    training acc:  0.12514665623777865    training loss:  3.240699527514362\n",
      "iteration  160  :    training acc:  0.1255377395385217    training loss:  3.2394820359696723\n",
      "iteration  180  :    training acc:  0.12436448963629253    training loss:  3.238277726405211\n",
      "iteration  200  :    training acc:  0.12592882283926476    training loss:  3.237076060647618\n",
      "iteration  220  :    training acc:  0.12788423934298004    training loss:  3.2358945883060035\n",
      "iteration  240  :    training acc:  0.1302307391474384    training loss:  3.2347431103388726\n",
      "iteration  260  :    training acc:  0.13140398904966757    training loss:  3.2336312653353603\n",
      "iteration  280  :    training acc:  0.13101290574892452    training loss:  3.2325829375646555\n",
      "iteration  300  :    training acc:  0.1321861556511537    training loss:  3.2316366715002856\n",
      "iteration  320  :    training acc:  0.13296832225263983    training loss:  3.2308018902073123\n",
      "iteration  340  :    training acc:  0.13375048885412594    training loss:  3.2300377381265997\n",
      "iteration  360  :    training acc:  0.13570590535784122    training loss:  3.2293015781263836\n",
      "iteration  380  :    training acc:  0.13766132186155652    training loss:  3.228570695275004\n",
      "iteration  400  :    training acc:  0.13844348846304264    training loss:  3.2278347775192344\n",
      "iteration  420  :    training acc:  0.1388345717637857    training loss:  3.227087816330531\n",
      "iteration  440  :    training acc:  0.13844348846304264    training loss:  3.226326383653829\n",
      "iteration  460  :    training acc:  0.1396167383652718    training loss:  3.2255497282361656\n",
      "iteration  480  :    training acc:  0.14000782166601486    training loss:  3.224759860931711\n",
      "iteration  500  :    training acc:  0.14039890496675791    training loss:  3.2239601155456734\n",
      "iteration  520  :    training acc:  0.14157215486898708    training loss:  3.2231542925412273\n",
      "iteration  540  :    training acc:  0.14196323816973017    training loss:  3.2223470726725125\n",
      "iteration  560  :    training acc:  0.14196323816973017    training loss:  3.221544359640949\n",
      "iteration  580  :    training acc:  0.14196323816973017    training loss:  3.2207523914270504\n",
      "iteration  600  :    training acc:  0.14235432147047322    training loss:  3.219975859766677\n",
      "iteration  620  :    training acc:  0.14235432147047322    training loss:  3.2192165019897088\n",
      "iteration  640  :    training acc:  0.14274540477121628    training loss:  3.2184732726041907\n",
      "iteration  660  :    training acc:  0.1435275713727024    training loss:  3.217743740566792\n",
      "iteration  680  :    training acc:  0.1450919045756746    training loss:  3.217025464816433\n",
      "iteration  700  :    training acc:  0.14548298787641767    training loss:  3.2163164839596536\n",
      "iteration  720  :    training acc:  0.14665623777864686    training loss:  3.215615159070969\n",
      "iteration  740  :    training acc:  0.14743840438013298    training loss:  3.2149201296970746\n",
      "iteration  760  :    training acc:  0.1482205709816191    training loss:  3.2142305010066945\n",
      "iteration  780  :    training acc:  0.1490027375831052    training loss:  3.213545948469711\n",
      "iteration  800  :    training acc:  0.14939382088384826    training loss:  3.212866712846182\n",
      "iteration  820  :    training acc:  0.15017598748533437    training loss:  3.212193476983572\n",
      "iteration  840  :    training acc:  0.14939382088384826    training loss:  3.2115271141064095\n",
      "iteration  860  :    training acc:  0.1497849041845913    training loss:  3.2108684436876493\n",
      "iteration  880  :    training acc:  0.14939382088384826    training loss:  3.210218068159714\n",
      "iteration  900  :    training acc:  0.15017598748533437    training loss:  3.209576254943275\n",
      "iteration  920  :    training acc:  0.15017598748533437    training loss:  3.20894285205971\n",
      "iteration  940  :    training acc:  0.1509581540868205    training loss:  3.2083172880531845\n",
      "iteration  960  :    training acc:  0.15213140398904967    training loss:  3.2076987020501\n",
      "iteration  980  :    training acc:  0.15252248728979273    training loss:  3.207086187637437\n",
      "iteration  1000  :    training acc:  0.15330465389127884    training loss:  3.2064790794099887\n",
      "iteration  1020  :    training acc:  0.1536957371920219    training loss:  3.2058771863680446\n",
      "iteration  1040  :    training acc:  0.15408682049276495    training loss:  3.205280865665031\n",
      "iteration  1060  :    training acc:  0.15408682049276495    training loss:  3.204690830463032\n",
      "iteration  1080  :    training acc:  0.154477903793508    training loss:  3.204107646161568\n",
      "iteration  1100  :    training acc:  0.15526007039499412    training loss:  3.203531055412165\n",
      "iteration  1120  :    training acc:  0.15526007039499412    training loss:  3.202959510357335\n",
      "iteration  1140  :    training acc:  0.1556511536957372    training loss:  3.2023902959791797\n",
      "iteration  1160  :    training acc:  0.1556511536957372    training loss:  3.2018202108960616\n",
      "iteration  1180  :    training acc:  0.15604223699648026    training loss:  3.2012463066407144\n",
      "iteration  1200  :    training acc:  0.15682440359796637    training loss:  3.200666246580952\n",
      "iteration  1220  :    training acc:  0.15682440359796637    training loss:  3.2000782654345556\n",
      "iteration  1240  :    training acc:  0.15760657019945248    training loss:  3.1994809518078577\n",
      "iteration  1260  :    training acc:  0.15799765350019554    training loss:  3.198873053890856\n",
      "iteration  1280  :    training acc:  0.1583887368009386    training loss:  3.1982534113280114\n",
      "iteration  1300  :    training acc:  0.15956198670316776    training loss:  3.1976210398398144\n",
      "iteration  1320  :    training acc:  0.15956198670316776    training loss:  3.196975318892941\n",
      "iteration  1340  :    training acc:  0.15995307000391085    training loss:  3.19631612919592\n",
      "iteration  1360  :    training acc:  0.15995307000391085    training loss:  3.1956437458913123\n",
      "iteration  1380  :    training acc:  0.15995307000391085    training loss:  3.194958478543945\n",
      "iteration  1400  :    training acc:  0.1603441533046539    training loss:  3.194260341890171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1420  :    training acc:  0.1603441533046539    training loss:  3.193549040607617\n",
      "iteration  1440  :    training acc:  0.16073523660539696    training loss:  3.1928242493439636\n",
      "iteration  1460  :    training acc:  0.16112631990614001    training loss:  3.192085952409115\n",
      "iteration  1480  :    training acc:  0.16112631990614001    training loss:  3.1913346046843722\n",
      "iteration  1500  :    training acc:  0.16229956980836918    training loss:  3.1905709893529135\n",
      "iteration  1520  :    training acc:  0.16347281971059835    training loss:  3.1897958384037084\n",
      "iteration  1540  :    training acc:  0.16347281971059835    training loss:  3.1890094647980076\n",
      "iteration  1560  :    training acc:  0.16425498631208446    training loss:  3.1882116566091026\n",
      "iteration  1580  :    training acc:  0.16464606961282754    training loss:  3.1874018758286726\n",
      "iteration  1600  :    training acc:  0.16464606961282754    training loss:  3.186579585735092\n",
      "iteration  1620  :    training acc:  0.1650371529135706    training loss:  3.185744480864686\n",
      "iteration  1640  :    training acc:  0.1650371529135706    training loss:  3.1848965050672584\n",
      "iteration  1660  :    training acc:  0.1658193195150567    training loss:  3.1840356950610555\n",
      "iteration  1680  :    training acc:  0.16660148611654282    training loss:  3.183161976870763\n",
      "iteration  1700  :    training acc:  0.16660148611654282    training loss:  3.1822750389811083\n",
      "iteration  1720  :    training acc:  0.16738365271802894    training loss:  3.1813743367932665\n",
      "iteration  1740  :    training acc:  0.16816581931951505    training loss:  3.1804592058566423\n",
      "iteration  1760  :    training acc:  0.1685569026202581    training loss:  3.1795290263347664\n",
      "iteration  1780  :    training acc:  0.1689479859210012    training loss:  3.178583393984201\n",
      "iteration  1800  :    training acc:  0.1689479859210012    training loss:  3.177622279206535\n",
      "iteration  1820  :    training acc:  0.17012123582323035    training loss:  3.1766461631765672\n",
      "iteration  1840  :    training acc:  0.17168556902620258    training loss:  3.175656124539528\n",
      "iteration  1860  :    training acc:  0.17207665232694563    training loss:  3.17465382704762\n",
      "iteration  1880  :    training acc:  0.17207665232694563    training loss:  3.1736413494340616\n",
      "iteration  1900  :    training acc:  0.1724677356276887    training loss:  3.1726208345800937\n",
      "iteration  1920  :    training acc:  0.1732499022291748    training loss:  3.1715940394618882\n",
      "iteration  1940  :    training acc:  0.17364098552991789    training loss:  3.1705619825862055\n",
      "iteration  1960  :    training acc:  0.17481423543214705    training loss:  3.169524869959317\n",
      "iteration  1980  :    training acc:  0.1752053187328901    training loss:  3.1684823104903397\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.10082709728239464    training loss:  13.568615808970868\n",
      "iteration  20  :    training acc:  0.11067349350137849    training loss:  3.249413450492001\n",
      "iteration  40  :    training acc:  0.11067349350137849    training loss:  3.249410927251409\n",
      "iteration  60  :    training acc:  0.11067349350137849    training loss:  3.2494089863094207\n",
      "iteration  80  :    training acc:  0.11067349350137849    training loss:  3.2494076076565612\n",
      "iteration  100  :    training acc:  0.11067349350137849    training loss:  3.249406705013917\n",
      "iteration  120  :    training acc:  0.11067349350137849    training loss:  3.2494062050946604\n",
      "iteration  140  :    training acc:  0.11067349350137849    training loss:  3.2494060456060714\n",
      "iteration  160  :    training acc:  0.11067349350137849    training loss:  3.2494061735704762\n",
      "iteration  180  :    training acc:  0.11067349350137849    training loss:  3.249406543908433\n",
      "iteration  200  :    training acc:  0.11067349350137849    training loss:  3.2494071182420323\n",
      "iteration  220  :    training acc:  0.11067349350137849    training loss:  3.2494078638831763\n",
      "iteration  240  :    training acc:  0.11067349350137849    training loss:  3.2494087529775024\n",
      "iteration  260  :    training acc:  0.11067349350137849    training loss:  3.2494097617794524\n",
      "iteration  280  :    training acc:  0.11067349350137849    training loss:  3.2494108700379023\n",
      "iteration  300  :    training acc:  0.11067349350137849    training loss:  3.249412060475162\n",
      "iteration  320  :    training acc:  0.11067349350137849    training loss:  3.249413318344841\n",
      "iteration  340  :    training acc:  0.11067349350137849    training loss:  3.2494146310564194\n",
      "iteration  360  :    training acc:  0.11067349350137849    training loss:  3.2494159878562696\n",
      "iteration  380  :    training acc:  0.11067349350137849    training loss:  3.2494173795564887\n",
      "iteration  400  :    training acc:  0.11067349350137849    training loss:  3.249418798304268\n",
      "iteration  420  :    training acc:  0.11067349350137849    training loss:  3.2494202373856185\n",
      "iteration  440  :    training acc:  0.11067349350137849    training loss:  3.2494216910582736\n",
      "iteration  460  :    training acc:  0.11067349350137849    training loss:  3.249423154409362\n",
      "iteration  480  :    training acc:  0.11067349350137849    training loss:  3.2494246232341184\n",
      "iteration  500  :    training acc:  0.11067349350137849    training loss:  3.249426093932491\n",
      "iteration  520  :    training acc:  0.11067349350137849    training loss:  3.249427563420958\n",
      "iteration  540  :    training acc:  0.11067349350137849    training loss:  3.2494290290572914\n",
      "iteration  560  :    training acc:  0.11067349350137849    training loss:  3.249430488576351\n",
      "iteration  580  :    training acc:  0.11067349350137849    training loss:  3.2494319400352496\n",
      "iteration  600  :    training acc:  0.11067349350137849    training loss:  3.2494333817665306\n",
      "iteration  620  :    training acc:  0.11067349350137849    training loss:  3.2494348123381536\n",
      "iteration  640  :    training acc:  0.11067349350137849    training loss:  3.2494362305192985\n",
      "iteration  660  :    training acc:  0.11067349350137849    training loss:  3.2494376352511263\n",
      "iteration  680  :    training acc:  0.11067349350137849    training loss:  3.2494390256217693\n",
      "iteration  700  :    training acc:  0.11067349350137849    training loss:  3.249440400844937\n",
      "iteration  720  :    training acc:  0.11067349350137849    training loss:  3.249441760241611\n",
      "iteration  740  :    training acc:  0.11067349350137849    training loss:  3.249443103224362\n",
      "iteration  760  :    training acc:  0.11067349350137849    training loss:  3.2494444292839333\n",
      "iteration  780  :    training acc:  0.11067349350137849    training loss:  3.2494457379777426\n",
      "iteration  800  :    training acc:  0.11067349350137849    training loss:  3.249447028920031\n",
      "iteration  820  :    training acc:  0.11067349350137849    training loss:  3.249448301773421\n",
      "iteration  840  :    training acc:  0.11067349350137849    training loss:  3.249449556241681\n",
      "iteration  860  :    training acc:  0.11067349350137849    training loss:  3.2494507920635227\n",
      "iteration  880  :    training acc:  0.11067349350137849    training loss:  3.2494520090072743\n",
      "iteration  900  :    training acc:  0.11067349350137849    training loss:  3.2494532068663267\n",
      "iteration  920  :    training acc:  0.11067349350137849    training loss:  3.2494543854552127\n",
      "iteration  940  :    training acc:  0.11067349350137849    training loss:  3.249455544606253\n",
      "iteration  960  :    training acc:  0.11067349350137849    training loss:  3.2494566841666765\n",
      "iteration  980  :    training acc:  0.11067349350137849    training loss:  3.24945780399615\n",
      "iteration  1000  :    training acc:  0.11067349350137849    training loss:  3.2494589039646633\n",
      "iteration  1020  :    training acc:  0.11067349350137849    training loss:  3.2494599839507097\n",
      "iteration  1040  :    training acc:  0.11067349350137849    training loss:  3.249461043839735\n",
      "iteration  1060  :    training acc:  0.11067349350137849    training loss:  3.2494620835228045\n",
      "iteration  1080  :    training acc:  0.11067349350137849    training loss:  3.2494631028954633\n",
      "iteration  1100  :    training acc:  0.11067349350137849    training loss:  3.249464101856765\n",
      "iteration  1120  :    training acc:  0.11067349350137849    training loss:  3.2494650803084357\n",
      "iteration  1140  :    training acc:  0.11067349350137849    training loss:  3.249466038154172\n",
      "iteration  1160  :    training acc:  0.11067349350137849    training loss:  3.2494669752990344\n",
      "iteration  1180  :    training acc:  0.11067349350137849    training loss:  3.249467891648935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1200  :    training acc:  0.11067349350137849    training loss:  3.2494687871102093\n",
      "iteration  1220  :    training acc:  0.11067349350137849    training loss:  3.2494696615892438\n",
      "iteration  1240  :    training acc:  0.11067349350137849    training loss:  3.2494705149921748\n",
      "iteration  1260  :    training acc:  0.11067349350137849    training loss:  3.249471347224625\n",
      "iteration  1280  :    training acc:  0.11067349350137849    training loss:  3.249472158191491\n",
      "iteration  1300  :    training acc:  0.11067349350137849    training loss:  3.2494729477967645\n",
      "iteration  1320  :    training acc:  0.11067349350137849    training loss:  3.249473715943382\n",
      "iteration  1340  :    training acc:  0.11067349350137849    training loss:  3.2494744625331045\n",
      "iteration  1360  :    training acc:  0.11067349350137849    training loss:  3.2494751874664156\n",
      "iteration  1380  :    training acc:  0.11067349350137849    training loss:  3.2494758906424384\n",
      "iteration  1400  :    training acc:  0.11067349350137849    training loss:  3.2494765719588696\n",
      "iteration  1420  :    training acc:  0.11067349350137849    training loss:  3.249477231311922\n",
      "iteration  1440  :    training acc:  0.11067349350137849    training loss:  3.249477868596274\n",
      "iteration  1460  :    training acc:  0.11067349350137849    training loss:  3.249478483705035\n",
      "iteration  1480  :    training acc:  0.11067349350137849    training loss:  3.249479076529703\n",
      "iteration  1500  :    training acc:  0.11067349350137849    training loss:  3.2494796469601326\n",
      "iteration  1520  :    training acc:  0.11067349350137849    training loss:  3.2494801948844985\n",
      "iteration  1540  :    training acc:  0.11067349350137849    training loss:  3.2494807201892644\n",
      "iteration  1560  :    training acc:  0.11067349350137849    training loss:  3.2494812227591376\n",
      "iteration  1580  :    training acc:  0.11067349350137849    training loss:  3.249481702477034\n",
      "iteration  1600  :    training acc:  0.11067349350137849    training loss:  3.249482159224026\n",
      "iteration  1620  :    training acc:  0.11067349350137849    training loss:  3.249482592879287\n",
      "iteration  1640  :    training acc:  0.11067349350137849    training loss:  3.249483003320027\n",
      "iteration  1660  :    training acc:  0.11067349350137849    training loss:  3.2494833904214273\n",
      "iteration  1680  :    training acc:  0.11067349350137849    training loss:  3.2494837540565507\n",
      "iteration  1700  :    training acc:  0.11067349350137849    training loss:  3.2494840940962515\n",
      "iteration  1720  :    training acc:  0.11067349350137849    training loss:  3.2494844104090745\n",
      "iteration  1740  :    training acc:  0.11067349350137849    training loss:  3.2494847028611322\n",
      "iteration  1760  :    training acc:  0.11067349350137849    training loss:  3.2494849713159795\n",
      "iteration  1780  :    training acc:  0.11067349350137849    training loss:  3.249485215634471\n",
      "iteration  1800  :    training acc:  0.11067349350137849    training loss:  3.249485435674598\n",
      "iteration  1820  :    training acc:  0.11067349350137849    training loss:  3.249485631291323\n",
      "iteration  1840  :    training acc:  0.11067349350137849    training loss:  3.24948580233639\n",
      "iteration  1860  :    training acc:  0.11067349350137849    training loss:  3.2494859486581222\n",
      "iteration  1880  :    training acc:  0.11067349350137849    training loss:  3.2494860701012054\n",
      "iteration  1900  :    training acc:  0.11067349350137849    training loss:  3.2494861665064567\n",
      "iteration  1920  :    training acc:  0.11067349350137849    training loss:  3.2494862377105704\n",
      "iteration  1940  :    training acc:  0.11067349350137849    training loss:  3.2494862835458607\n",
      "iteration  1960  :    training acc:  0.11067349350137849    training loss:  3.249486303839975\n",
      "iteration  1980  :    training acc:  0.11067349350137849    training loss:  3.2494862984156025\n",
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.09665871121718377    training loss:  13.225217607293452\n",
      "iteration  20  :    training acc:  0.08989657915672236    training loss:  3.643591731322882\n",
      "iteration  40  :    training acc:  0.08989657915672236    training loss:  3.6179952645990534\n",
      "iteration  60  :    training acc:  0.08910103420843278    training loss:  3.6032254464616007\n",
      "iteration  80  :    training acc:  0.08870326173428798    training loss:  3.5934253517787376\n",
      "iteration  100  :    training acc:  0.08910103420843278    training loss:  3.586442006765178\n",
      "iteration  120  :    training acc:  0.08949880668257756    training loss:  3.5812716790348005\n",
      "iteration  140  :    training acc:  0.08910103420843278    training loss:  3.5773779944708886\n",
      "iteration  160  :    training acc:  0.08989657915672236    training loss:  3.5744437640150792\n",
      "iteration  180  :    training acc:  0.08910103420843278    training loss:  3.572265698728204\n",
      "iteration  200  :    training acc:  0.08989657915672236    training loss:  3.5707048532254024\n",
      "iteration  220  :    training acc:  0.08989657915672236    training loss:  3.569661437946575\n",
      "iteration  240  :    training acc:  0.08989657915672236    training loss:  3.569061189036697\n",
      "iteration  260  :    training acc:  0.09069212410501193    training loss:  3.568847545458218\n",
      "iteration  280  :    training acc:  0.09228321400159109    training loss:  3.5689769097583555\n",
      "iteration  300  :    training acc:  0.09387430389817025    training loss:  3.5694156734448153\n",
      "iteration  320  :    training acc:  0.09228321400159109    training loss:  3.5701383479918483\n",
      "iteration  340  :    training acc:  0.09307875894988067    training loss:  3.5711264380435823\n",
      "iteration  360  :    training acc:  0.09347653142402546    training loss:  3.572367815656585\n",
      "iteration  380  :    training acc:  0.09347653142402546    training loss:  3.5738564028904243\n",
      "iteration  400  :    training acc:  0.09387430389817025    training loss:  3.575591991248352\n",
      "iteration  420  :    training acc:  0.09307875894988067    training loss:  3.577580034655002\n",
      "iteration  440  :    training acc:  0.09188544152744631    training loss:  3.579831243755296\n",
      "iteration  460  :    training acc:  0.09307875894988067    training loss:  3.5823607743205748\n",
      "iteration  480  :    training acc:  0.09506762132060462    training loss:  3.585186743283667\n",
      "iteration  500  :    training acc:  0.09347653142402546    training loss:  3.588327752014435\n",
      "iteration  520  :    training acc:  0.09387430389817025    training loss:  3.5917991183537947\n",
      "iteration  540  :    training acc:  0.09268098647573587    training loss:  3.5956077322384647\n",
      "iteration  560  :    training acc:  0.09307875894988067    training loss:  3.5997460124048377\n",
      "iteration  580  :    training acc:  0.09268098647573587    training loss:  3.6041864947024442\n",
      "iteration  600  :    training acc:  0.09148766905330151    training loss:  3.6088798829257493\n",
      "iteration  620  :    training acc:  0.09228321400159109    training loss:  3.6137596036759794\n",
      "iteration  640  :    training acc:  0.09108989657915673    training loss:  3.6187531791178693\n",
      "iteration  660  :    training acc:  0.09029435163086715    training loss:  3.623796019752414\n",
      "iteration  680  :    training acc:  0.09029435163086715    training loss:  3.62884123346759\n",
      "iteration  700  :    training acc:  0.09268098647573587    training loss:  3.6338620614121084\n",
      "iteration  720  :    training acc:  0.09148766905330151    training loss:  3.6388478733276433\n",
      "iteration  740  :    training acc:  0.09148766905330151    training loss:  3.643796219162687\n",
      "iteration  760  :    training acc:  0.09188544152744631    training loss:  3.64870306821977\n",
      "iteration  780  :    training acc:  0.09307875894988067    training loss:  3.6535549378454566\n",
      "iteration  800  :    training acc:  0.09228321400159109    training loss:  3.658329784064711\n",
      "iteration  820  :    training acc:  0.09307875894988067    training loss:  3.6630087126842246\n",
      "iteration  840  :    training acc:  0.09148766905330151    training loss:  3.6675873632231863\n",
      "iteration  860  :    training acc:  0.09228321400159109    training loss:  3.6720758995289358\n",
      "iteration  880  :    training acc:  0.09347653142402546    training loss:  3.6764909751171517\n",
      "iteration  900  :    training acc:  0.09387430389817025    training loss:  3.680848482568958\n",
      "iteration  920  :    training acc:  0.09427207637231504    training loss:  3.685160498548169\n",
      "iteration  940  :    training acc:  0.09506762132060462    training loss:  3.6894352680684652\n",
      "iteration  960  :    training acc:  0.09506762132060462    training loss:  3.6936783579276455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  980  :    training acc:  0.09506762132060462    training loss:  3.697893902576784\n",
      "iteration  1000  :    training acc:  0.09466984884645982    training loss:  3.7020855784248266\n",
      "iteration  1020  :    training acc:  0.09387430389817025    training loss:  3.706257284133038\n",
      "iteration  1040  :    training acc:  0.09466984884645982    training loss:  3.7104136075791403\n",
      "iteration  1060  :    training acc:  0.09427207637231504    training loss:  3.714560161297192\n",
      "iteration  1080  :    training acc:  0.0954653937947494    training loss:  3.718703835441434\n",
      "iteration  1100  :    training acc:  0.09626093874303898    training loss:  3.722852971835344\n",
      "iteration  1120  :    training acc:  0.09626093874303898    training loss:  3.7270174030604606\n",
      "iteration  1140  :    training acc:  0.0958631662688942    training loss:  3.731208219790914\n",
      "iteration  1160  :    training acc:  0.0958631662688942    training loss:  3.735437036743166\n",
      "iteration  1180  :    training acc:  0.0958631662688942    training loss:  3.739714490748004\n",
      "iteration  1200  :    training acc:  0.0954653937947494    training loss:  3.7440479036753413\n",
      "iteration  1220  :    training acc:  0.09347653142402546    training loss:  3.7484387095166074\n",
      "iteration  1240  :    training acc:  0.09347653142402546    training loss:  3.7528812485104015\n",
      "iteration  1260  :    training acc:  0.09387430389817025    training loss:  3.7573648462203346\n",
      "iteration  1280  :    training acc:  0.09466984884645982    training loss:  3.7618795396710656\n",
      "iteration  1300  :    training acc:  0.09466984884645982    training loss:  3.766423296045372\n",
      "iteration  1320  :    training acc:  0.0954653937947494    training loss:  3.7710078931038806\n",
      "iteration  1340  :    training acc:  0.09387430389817025    training loss:  3.775662507930206\n",
      "iteration  1360  :    training acc:  0.0958631662688942    training loss:  3.7804358580226807\n",
      "iteration  1380  :    training acc:  0.09626093874303898    training loss:  3.785396366318915\n",
      "iteration  1400  :    training acc:  0.09626093874303898    training loss:  3.790622773732806\n",
      "iteration  1420  :    training acc:  0.09705648369132856    training loss:  3.796167361954668\n",
      "iteration  1440  :    training acc:  0.09665871121718377    training loss:  3.8019933492457842\n",
      "iteration  1460  :    training acc:  0.09466984884645982    training loss:  3.8079639053063103\n",
      "iteration  1480  :    training acc:  0.09506762132060462    training loss:  3.8139261535425995\n",
      "iteration  1500  :    training acc:  0.0958631662688942    training loss:  3.8197779358938067\n",
      "iteration  1520  :    training acc:  0.0958631662688942    training loss:  3.8254707670298562\n",
      "iteration  1540  :    training acc:  0.09785202863961814    training loss:  3.83099779773729\n",
      "iteration  1560  :    training acc:  0.09864757358790771    training loss:  3.836380559715088\n",
      "iteration  1580  :    training acc:  0.09904534606205251    training loss:  3.841654266324059\n",
      "iteration  1600  :    training acc:  0.09904534606205251    training loss:  3.846855842625161\n",
      "iteration  1620  :    training acc:  0.09904534606205251    training loss:  3.852015530232745\n",
      "iteration  1640  :    training acc:  0.09984089101034209    training loss:  3.857151160752431\n",
      "iteration  1660  :    training acc:  0.10023866348448687    training loss:  3.8622658153219023\n",
      "iteration  1680  :    training acc:  0.10023866348448687    training loss:  3.867351741405001\n",
      "iteration  1700  :    training acc:  0.10143198090692124    training loss:  3.8724039613546024\n",
      "iteration  1720  :    training acc:  0.10063643595863166    training loss:  3.8774463234498975\n",
      "iteration  1740  :    training acc:  0.10222752585521082    training loss:  3.8825725689161223\n",
      "iteration  1760  :    training acc:  0.10182975338106603    training loss:  3.887960800867382\n",
      "iteration  1780  :    training acc:  0.10143198090692124    training loss:  3.8935766380255767\n",
      "iteration  1800  :    training acc:  0.10143198090692124    training loss:  3.8988906342373855\n",
      "iteration  1820  :    training acc:  0.10023866348448687    training loss:  3.903685798064949\n",
      "iteration  1840  :    training acc:  0.10103420843277645    training loss:  3.908171407724272\n",
      "iteration  1860  :    training acc:  0.10023866348448687    training loss:  3.9125677394241114\n",
      "iteration  1880  :    training acc:  0.10023866348448687    training loss:  3.9170012551290796\n",
      "iteration  1900  :    training acc:  0.10063643595863166    training loss:  3.9215490177560928\n",
      "iteration  1920  :    training acc:  0.10023866348448687    training loss:  3.9262920423951795\n",
      "iteration  1940  :    training acc:  0.10063643595863166    training loss:  3.9312895096976885\n",
      "iteration  1960  :    training acc:  0.10103420843277645    training loss:  3.936353759431995\n",
      "iteration  1980  :    training acc:  0.10182975338106603    training loss:  3.9408959497292626\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.1116524563540398    training loss:  15.602737222309692\n",
      "iteration  20  :    training acc:  0.1173365814047909    training loss:  3.2524757045545103\n",
      "iteration  40  :    training acc:  0.11246447421843281    training loss:  3.2473870334206216\n",
      "iteration  60  :    training acc:  0.11368250101502234    training loss:  3.2465986776630813\n",
      "iteration  80  :    training acc:  0.1173365814047909    training loss:  3.2463797973155373\n",
      "iteration  100  :    training acc:  0.11814859926918392    training loss:  3.2462346550608543\n",
      "iteration  120  :    training acc:  0.11814859926918392    training loss:  3.2460599709845184\n",
      "iteration  140  :    training acc:  0.11774259033698742    training loss:  3.2458618387950344\n",
      "iteration  160  :    training acc:  0.12342671538773853    training loss:  3.2456721846391576\n",
      "iteration  180  :    training acc:  0.12545676004872108    training loss:  3.2455097264171333\n",
      "iteration  200  :    training acc:  0.12505075111652456    training loss:  3.2453739246681104\n",
      "iteration  220  :    training acc:  0.12505075111652456    training loss:  3.245261949230861\n",
      "iteration  240  :    training acc:  0.12505075111652456    training loss:  3.2451862924529054\n",
      "iteration  260  :    training acc:  0.1274868047097036    training loss:  3.2451908030716843\n",
      "iteration  280  :    training acc:  0.1278928136419001    training loss:  3.245341687152094\n",
      "iteration  300  :    training acc:  0.1315468940316687    training loss:  3.2457047582506626\n",
      "iteration  320  :    training acc:  0.1311408850994722    training loss:  3.2463023005634897\n",
      "iteration  340  :    training acc:  0.1319529029638652    training loss:  3.2470802722747076\n",
      "iteration  360  :    training acc:  0.1311408850994722    training loss:  3.2479732863350104\n",
      "iteration  380  :    training acc:  0.12870483150629314    training loss:  3.2489914279547354\n",
      "iteration  400  :    training acc:  0.12911084043848964    training loss:  3.2501726166167453\n",
      "iteration  420  :    training acc:  0.12911084043848964    training loss:  3.2515127210135035\n",
      "iteration  440  :    training acc:  0.1315468940316687    training loss:  3.2529692985297576\n",
      "iteration  460  :    training acc:  0.12829882257409664    training loss:  3.254498018901016\n",
      "iteration  480  :    training acc:  0.12870483150629314    training loss:  3.25609423591553\n",
      "iteration  500  :    training acc:  0.13235891189606172    training loss:  3.257796516371897\n",
      "iteration  520  :    training acc:  0.1311408850994722    training loss:  3.259660653343999\n",
      "iteration  540  :    training acc:  0.13276492082825822    training loss:  3.2617504855747783\n",
      "iteration  560  :    training acc:  0.1307348761672757    training loss:  3.2641283388505675\n",
      "iteration  580  :    training acc:  0.1311408850994722    training loss:  3.266820031654803\n",
      "iteration  600  :    training acc:  0.13235891189606172    training loss:  3.269799253349425\n",
      "iteration  620  :    training acc:  0.1319529029638652    training loss:  3.273017461150611\n",
      "iteration  640  :    training acc:  0.1307348761672757    training loss:  3.2764435115239916\n",
      "iteration  660  :    training acc:  0.13032886723507917    training loss:  3.2800465021324143\n",
      "iteration  680  :    training acc:  0.1311408850994722    training loss:  3.2837649765568715\n",
      "iteration  700  :    training acc:  0.1311408850994722    training loss:  3.2875410439261095\n",
      "iteration  720  :    training acc:  0.1311408850994722    training loss:  3.291366513928419\n",
      "iteration  740  :    training acc:  0.12951684937068617    training loss:  3.2952704039550738\n",
      "iteration  760  :    training acc:  0.12911084043848964    training loss:  3.299270081509366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  780  :    training acc:  0.1270807957775071    training loss:  3.3033559161649353\n",
      "iteration  800  :    training acc:  0.12870483150629314    training loss:  3.3075157791216205\n",
      "iteration  820  :    training acc:  0.12870483150629314    training loss:  3.3117471938629306\n",
      "iteration  840  :    training acc:  0.12829882257409664    training loss:  3.3160525754892087\n",
      "iteration  860  :    training acc:  0.1274868047097036    training loss:  3.3204335931700206\n",
      "iteration  880  :    training acc:  0.12586276898091758    training loss:  3.324888073374565\n",
      "iteration  900  :    training acc:  0.1270807957775071    training loss:  3.3294104752191958\n",
      "iteration  920  :    training acc:  0.12667478684531058    training loss:  3.333997267028134\n",
      "iteration  940  :    training acc:  0.12586276898091758    training loss:  3.3386508094660314\n",
      "iteration  960  :    training acc:  0.12911084043848964    training loss:  3.3433736192650345\n",
      "iteration  980  :    training acc:  0.12911084043848964    training loss:  3.348161511218103\n",
      "iteration  1000  :    training acc:  0.12870483150629314    training loss:  3.3530049049627157\n",
      "iteration  1020  :    training acc:  0.12911084043848964    training loss:  3.3578933398271227\n",
      "iteration  1040  :    training acc:  0.1307348761672757    training loss:  3.362816918727827\n",
      "iteration  1060  :    training acc:  0.1319529029638652    training loss:  3.367765525425451\n",
      "iteration  1080  :    training acc:  0.13438895655704425    training loss:  3.372729170313495\n",
      "iteration  1100  :    training acc:  0.13520097442143728    training loss:  3.377700378976537\n",
      "iteration  1120  :    training acc:  0.13479496548924078    training loss:  3.3826774460222433\n",
      "iteration  1140  :    training acc:  0.13398294762484775    training loss:  3.387667062872989\n",
      "iteration  1160  :    training acc:  0.13438895655704425    training loss:  3.392684468006209\n",
      "iteration  1180  :    training acc:  0.13276492082825822    training loss:  3.397747461323589\n",
      "iteration  1200  :    training acc:  0.13235891189606172    training loss:  3.402862363603335\n",
      "iteration  1220  :    training acc:  0.1319529029638652    training loss:  3.408017581684024\n",
      "iteration  1240  :    training acc:  0.1307348761672757    training loss:  3.4132011649949097\n",
      "iteration  1260  :    training acc:  0.1319529029638652    training loss:  3.4184149880602344\n",
      "iteration  1280  :    training acc:  0.1319529029638652    training loss:  3.4236656010323525\n",
      "iteration  1300  :    training acc:  0.13032886723507917    training loss:  3.4289506824005107\n",
      "iteration  1320  :    training acc:  0.1307348761672757    training loss:  3.4342560555519763\n",
      "iteration  1340  :    training acc:  0.1319529029638652    training loss:  3.4395622360608744\n",
      "iteration  1360  :    training acc:  0.1319529029638652    training loss:  3.444852762466358\n",
      "iteration  1380  :    training acc:  0.13276492082825822    training loss:  3.4501178764818663\n",
      "iteration  1400  :    training acc:  0.1311408850994722    training loss:  3.455353386551417\n",
      "iteration  1420  :    training acc:  0.1307348761672757    training loss:  3.460558226561505\n",
      "iteration  1440  :    training acc:  0.12992285830288267    training loss:  3.4657331179676705\n",
      "iteration  1460  :    training acc:  0.13032886723507917    training loss:  3.4708805765351234\n",
      "iteration  1480  :    training acc:  0.12992285830288267    training loss:  3.476005413659469\n",
      "iteration  1500  :    training acc:  0.1311408850994722    training loss:  3.4811148143765647\n",
      "iteration  1520  :    training acc:  0.1315468940316687    training loss:  3.486217547683973\n",
      "iteration  1540  :    training acc:  0.1307348761672757    training loss:  3.491322500449549\n",
      "iteration  1560  :    training acc:  0.1311408850994722    training loss:  3.496437017427822\n",
      "iteration  1580  :    training acc:  0.1315468940316687    training loss:  3.5015651460180535\n",
      "iteration  1600  :    training acc:  0.12911084043848964    training loss:  3.506705477426145\n",
      "iteration  1620  :    training acc:  0.12911084043848964    training loss:  3.511849171098643\n",
      "iteration  1640  :    training acc:  0.12992285830288267    training loss:  3.516980072033162\n",
      "iteration  1660  :    training acc:  0.12951684937068617    training loss:  3.522077628373184\n",
      "iteration  1680  :    training acc:  0.12951684937068617    training loss:  3.5271207678269914\n",
      "iteration  1700  :    training acc:  0.13032886723507917    training loss:  3.532089388155624\n",
      "iteration  1720  :    training acc:  0.12870483150629314    training loss:  3.5369610328721053\n",
      "iteration  1740  :    training acc:  0.12951684937068617    training loss:  3.5417125141485157\n",
      "iteration  1760  :    training acc:  0.13276492082825822    training loss:  3.5463489175335687\n",
      "iteration  1780  :    training acc:  0.13235891189606172    training loss:  3.5509255967468443\n",
      "iteration  1800  :    training acc:  0.1315468940316687    training loss:  3.5555198126971512\n",
      "iteration  1820  :    training acc:  0.1307348761672757    training loss:  3.560202019800597\n",
      "iteration  1840  :    training acc:  0.1311408850994722    training loss:  3.565018710604306\n",
      "iteration  1860  :    training acc:  0.1311408850994722    training loss:  3.56998604590717\n",
      "iteration  1880  :    training acc:  0.13235891189606172    training loss:  3.575098768660428\n",
      "iteration  1900  :    training acc:  0.1319529029638652    training loss:  3.5803423184477787\n",
      "iteration  1920  :    training acc:  0.13235891189606172    training loss:  3.5857001034875022\n",
      "iteration  1940  :    training acc:  0.13276492082825822    training loss:  3.5911561873304843\n",
      "iteration  1960  :    training acc:  0.13235891189606172    training loss:  3.5966955169482486\n",
      "iteration  1980  :    training acc:  0.1319529029638652    training loss:  3.6023031096611096\n",
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.10545308740978349    training loss:  15.106093760991794\n",
      "iteration  20  :    training acc:  0.10384923817161187    training loss:  3.4615780761685233\n",
      "iteration  40  :    training acc:  0.10384923817161187    training loss:  3.443439058724744\n",
      "iteration  60  :    training acc:  0.10505212510024058    training loss:  3.4324959845050267\n",
      "iteration  80  :    training acc:  0.10264635124298316    training loss:  3.4255288180428614\n",
      "iteration  100  :    training acc:  0.10184442662389735    training loss:  3.4210845891886073\n",
      "iteration  120  :    training acc:  0.10264635124298316    training loss:  3.418411453827337\n",
      "iteration  140  :    training acc:  0.10344827586206896    training loss:  3.4170896093343948\n",
      "iteration  160  :    training acc:  0.10224538893344026    training loss:  3.4168674562819126\n",
      "iteration  180  :    training acc:  0.10425020048115477    training loss:  3.417582521761432\n",
      "iteration  200  :    training acc:  0.10264635124298316    training loss:  3.4191210149260978\n",
      "iteration  220  :    training acc:  0.10024057738572574    training loss:  3.421395806806644\n",
      "iteration  240  :    training acc:  0.10264635124298316    training loss:  3.424333744654199\n",
      "iteration  260  :    training acc:  0.10344827586206896    training loss:  3.4278694827536125\n",
      "iteration  280  :    training acc:  0.10344827586206896    training loss:  3.4319452554333685\n",
      "iteration  300  :    training acc:  0.10344827586206896    training loss:  3.436514678587067\n",
      "iteration  320  :    training acc:  0.09983961507618284    training loss:  3.4415472361263006\n",
      "iteration  340  :    training acc:  0.10344827586206896    training loss:  3.4470310922735394\n",
      "iteration  360  :    training acc:  0.10344827586206896    training loss:  3.4529738897908433\n",
      "iteration  380  :    training acc:  0.10304731355252607    training loss:  3.459401477875728\n",
      "iteration  400  :    training acc:  0.10264635124298316    training loss:  3.4663520091875815\n",
      "iteration  420  :    training acc:  0.10144346431435446    training loss:  3.4738590751990746\n",
      "iteration  440  :    training acc:  0.10064153969526865    training loss:  3.4819180635899394\n",
      "iteration  460  :    training acc:  0.09943865276663993    training loss:  3.4904451244764676\n",
      "iteration  480  :    training acc:  0.10264635124298316    training loss:  3.499264486043111\n",
      "iteration  500  :    training acc:  0.10024057738572574    training loss:  3.5081533993675684\n",
      "iteration  520  :    training acc:  0.10104250200481155    training loss:  3.516917110932549\n",
      "iteration  540  :    training acc:  0.10184442662389735    training loss:  3.5254305720876435\n",
      "iteration  560  :    training acc:  0.10184442662389735    training loss:  3.5336121991498075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  580  :    training acc:  0.10224538893344026    training loss:  3.5413739373855218\n",
      "iteration  600  :    training acc:  0.10224538893344026    training loss:  3.548642700559883\n",
      "iteration  620  :    training acc:  0.10304731355252607    training loss:  3.5554298722616626\n",
      "iteration  640  :    training acc:  0.10304731355252607    training loss:  3.5618262693115685\n",
      "iteration  660  :    training acc:  0.10425020048115477    training loss:  3.567937305058867\n",
      "iteration  680  :    training acc:  0.10224538893344026    training loss:  3.5738423074081105\n",
      "iteration  700  :    training acc:  0.10304731355252607    training loss:  3.579590753359715\n",
      "iteration  720  :    training acc:  0.10144346431435446    training loss:  3.585212278185832\n",
      "iteration  740  :    training acc:  0.10224538893344026    training loss:  3.5907262438143617\n",
      "iteration  760  :    training acc:  0.10264635124298316    training loss:  3.5961473934556705\n",
      "iteration  780  :    training acc:  0.10184442662389735    training loss:  3.601488037357494\n",
      "iteration  800  :    training acc:  0.10344827586206896    training loss:  3.6067585279245606\n",
      "iteration  820  :    training acc:  0.10304731355252607    training loss:  3.611968304605666\n",
      "iteration  840  :    training acc:  0.10465116279069768    training loss:  3.6171285171611807\n",
      "iteration  860  :    training acc:  0.10465116279069768    training loss:  3.6222545385415237\n",
      "iteration  880  :    training acc:  0.10304731355252607    training loss:  3.627365620239761\n",
      "iteration  900  :    training acc:  0.10545308740978349    training loss:  3.63248088604656\n",
      "iteration  920  :    training acc:  0.10585404971932638    training loss:  3.6376132609052085\n",
      "iteration  940  :    training acc:  0.10465116279069768    training loss:  3.6427636173872924\n",
      "iteration  960  :    training acc:  0.10545308740978349    training loss:  3.6479175614132937\n",
      "iteration  980  :    training acc:  0.10465116279069768    training loss:  3.653048501757345\n",
      "iteration  1000  :    training acc:  0.10505212510024058    training loss:  3.658129557563164\n",
      "iteration  1020  :    training acc:  0.10465116279069768    training loss:  3.6631488887955874\n",
      "iteration  1040  :    training acc:  0.10625501202886929    training loss:  3.668115903511219\n",
      "iteration  1060  :    training acc:  0.10625501202886929    training loss:  3.6730508207496877\n",
      "iteration  1080  :    training acc:  0.107457898957498    training loss:  3.677967273202055\n",
      "iteration  1100  :    training acc:  0.10946271050521252    training loss:  3.682876771520771\n",
      "iteration  1120  :    training acc:  0.1082598235765838    training loss:  3.687821878235463\n",
      "iteration  1140  :    training acc:  0.1070569366479551    training loss:  3.6928982860303443\n",
      "iteration  1160  :    training acc:  0.10665597433841219    training loss:  3.698246578713415\n",
      "iteration  1180  :    training acc:  0.11106655974338413    training loss:  3.704001920227726\n",
      "iteration  1200  :    training acc:  0.10946271050521252    training loss:  3.71021571093161\n",
      "iteration  1220  :    training acc:  0.11026463512429832    training loss:  3.7167963131668103\n",
      "iteration  1240  :    training acc:  0.1090617481956696    training loss:  3.723256323151331\n",
      "iteration  1260  :    training acc:  0.11186848436246993    training loss:  3.7292380632830513\n",
      "iteration  1280  :    training acc:  0.11106655974338413    training loss:  3.7349353801220855\n",
      "iteration  1300  :    training acc:  0.11267040898155574    training loss:  3.740540043052674\n",
      "iteration  1320  :    training acc:  0.11267040898155574    training loss:  3.7461157227766573\n",
      "iteration  1340  :    training acc:  0.11347233360064155    training loss:  3.751653182710965\n",
      "iteration  1360  :    training acc:  0.11267040898155574    training loss:  3.757094418382822\n",
      "iteration  1380  :    training acc:  0.11347233360064155    training loss:  3.7623614242774073\n",
      "iteration  1400  :    training acc:  0.11347233360064155    training loss:  3.7674062879982304\n",
      "iteration  1420  :    training acc:  0.11387329591018444    training loss:  3.7722230510791026\n",
      "iteration  1440  :    training acc:  0.11226944667201283    training loss:  3.7768076570215983\n",
      "iteration  1460  :    training acc:  0.11387329591018444    training loss:  3.7811258173800155\n",
      "iteration  1480  :    training acc:  0.11467522052927025    training loss:  3.785145799988133\n",
      "iteration  1500  :    training acc:  0.11427425821972735    training loss:  3.7888935286744863\n",
      "iteration  1520  :    training acc:  0.11547714514835605    training loss:  3.7924335620288336\n",
      "iteration  1540  :    training acc:  0.11708099438652766    training loss:  3.7958184217529825\n",
      "iteration  1560  :    training acc:  0.11668003207698477    training loss:  3.7990709914657512\n",
      "iteration  1580  :    training acc:  0.11587810745789896    training loss:  3.802193267958295\n",
      "iteration  1600  :    training acc:  0.11507618283881316    training loss:  3.8051834734752887\n",
      "iteration  1620  :    training acc:  0.11668003207698477    training loss:  3.808049232172182\n",
      "iteration  1640  :    training acc:  0.11507618283881316    training loss:  3.8108090947760154\n",
      "iteration  1660  :    training acc:  0.11267040898155574    training loss:  3.8134863975882958\n",
      "iteration  1680  :    training acc:  0.11226944667201283    training loss:  3.816104539401738\n",
      "iteration  1700  :    training acc:  0.11387329591018444    training loss:  3.818685716047328\n",
      "iteration  1720  :    training acc:  0.11347233360064155    training loss:  3.8212286104293147\n",
      "iteration  1740  :    training acc:  0.11587810745789896    training loss:  3.8235648331598036\n",
      "iteration  1760  :    training acc:  0.11467522052927025    training loss:  3.825403640077087\n",
      "iteration  1780  :    training acc:  0.11708099438652766    training loss:  3.82673989323827\n",
      "iteration  1800  :    training acc:  0.11668003207698477    training loss:  3.8276872777338693\n",
      "iteration  1820  :    training acc:  0.11708099438652766    training loss:  3.828318903196177\n",
      "iteration  1840  :    training acc:  0.11948676824378508    training loss:  3.828681688468732\n",
      "iteration  1860  :    training acc:  0.11908580593424219    training loss:  3.8288330282038343\n",
      "iteration  1880  :    training acc:  0.11708099438652766    training loss:  3.8288381829762668\n",
      "iteration  1900  :    training acc:  0.11668003207698477    training loss:  3.8287483924751906\n",
      "iteration  1920  :    training acc:  0.11708099438652766    training loss:  3.8285978347849348\n",
      "iteration  1940  :    training acc:  0.11587810745789896    training loss:  3.8284130635193128\n",
      "iteration  1960  :    training acc:  0.11507618283881316    training loss:  3.8282216167614553\n",
      "iteration  1980  :    training acc:  0.11226944667201283    training loss:  3.8280519280197742\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.10259481037924152    training loss:  15.599469286591429\n",
      "iteration  20  :    training acc:  0.1029940119760479    training loss:  3.3573151663705816\n",
      "iteration  40  :    training acc:  0.10419161676646707    training loss:  3.337617776025473\n",
      "iteration  60  :    training acc:  0.10339321357285429    training loss:  3.331553741490871\n",
      "iteration  80  :    training acc:  0.10379241516966067    training loss:  3.3302792318356746\n",
      "iteration  100  :    training acc:  0.10339321357285429    training loss:  3.331565787817942\n",
      "iteration  120  :    training acc:  0.10419161676646707    training loss:  3.334411267840007\n",
      "iteration  140  :    training acc:  0.10419161676646707    training loss:  3.338215218844146\n",
      "iteration  160  :    training acc:  0.10419161676646707    training loss:  3.3425733109305344\n",
      "iteration  180  :    training acc:  0.10538922155688622    training loss:  3.3472091864972913\n",
      "iteration  200  :    training acc:  0.10139720558882236    training loss:  3.351930326107632\n",
      "iteration  220  :    training acc:  0.10139720558882236    training loss:  3.3565964591896167\n",
      "iteration  240  :    training acc:  0.0998003992015968    training loss:  3.3611080959019835\n",
      "iteration  260  :    training acc:  0.10099800399201597    training loss:  3.3654075725135884\n",
      "iteration  280  :    training acc:  0.10099800399201597    training loss:  3.369477153962888\n",
      "iteration  300  :    training acc:  0.1029940119760479    training loss:  3.3733286340619477\n",
      "iteration  320  :    training acc:  0.10459081836327346    training loss:  3.376989531831387\n",
      "iteration  340  :    training acc:  0.10658682634730539    training loss:  3.380491900270983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  360  :    training acc:  0.10578842315369262    training loss:  3.383865824858651\n",
      "iteration  380  :    training acc:  0.10578842315369262    training loss:  3.3871368034518574\n",
      "iteration  400  :    training acc:  0.10578842315369262    training loss:  3.3903254408499404\n",
      "iteration  420  :    training acc:  0.10658682634730539    training loss:  3.393448203753091\n",
      "iteration  440  :    training acc:  0.10658682634730539    training loss:  3.396518488736626\n",
      "iteration  460  :    training acc:  0.10658682634730539    training loss:  3.399547636126759\n",
      "iteration  480  :    training acc:  0.10738522954091817    training loss:  3.4025457465476157\n",
      "iteration  500  :    training acc:  0.10578842315369262    training loss:  3.4055222730507384\n",
      "iteration  520  :    training acc:  0.10459081836327346    training loss:  3.4084864160907324\n",
      "iteration  540  :    training acc:  0.10499001996007984    training loss:  3.411447369518945\n",
      "iteration  560  :    training acc:  0.10578842315369262    training loss:  3.4144144683195727\n",
      "iteration  580  :    training acc:  0.10499001996007984    training loss:  3.4173972806878967\n",
      "iteration  600  :    training acc:  0.10379241516966067    training loss:  3.420405671825664\n",
      "iteration  620  :    training acc:  0.10459081836327346    training loss:  3.4234498450328505\n",
      "iteration  640  :    training acc:  0.10459081836327346    training loss:  3.4265403347014556\n",
      "iteration  660  :    training acc:  0.1029940119760479    training loss:  3.42968788001222\n",
      "iteration  680  :    training acc:  0.10259481037924152    training loss:  3.4329030414949777\n",
      "iteration  700  :    training acc:  0.10019960079840319    training loss:  3.4361953398706144\n",
      "iteration  720  :    training acc:  0.10059880239520957    training loss:  3.4395716440252566\n",
      "iteration  740  :    training acc:  0.10139720558882236    training loss:  3.4430336595225457\n",
      "iteration  760  :    training acc:  0.10139720558882236    training loss:  3.4465749551227574\n",
      "iteration  780  :    training acc:  0.10139720558882236    training loss:  3.4501791884242983\n",
      "iteration  800  :    training acc:  0.10219560878243512    training loss:  3.453822207536014\n",
      "iteration  820  :    training acc:  0.10099800399201597    training loss:  3.45747912860312\n",
      "iteration  840  :    training acc:  0.10059880239520957    training loss:  3.4611327905713223\n",
      "iteration  860  :    training acc:  0.10099800399201597    training loss:  3.464777636900359\n",
      "iteration  880  :    training acc:  0.10139720558882236    training loss:  3.4684172553063943\n",
      "iteration  900  :    training acc:  0.10219560878243512    training loss:  3.4720590134057647\n",
      "iteration  920  :    training acc:  0.1029940119760479    training loss:  3.475709624708672\n",
      "iteration  940  :    training acc:  0.1029940119760479    training loss:  3.479372937439744\n",
      "iteration  960  :    training acc:  0.10139720558882236    training loss:  3.483049375843388\n",
      "iteration  980  :    training acc:  0.10019960079840319    training loss:  3.486736049266624\n",
      "iteration  1000  :    training acc:  0.10059880239520957    training loss:  3.49042687805968\n",
      "iteration  1020  :    training acc:  0.10259481037924152    training loss:  3.4941126433111163\n",
      "iteration  1040  :    training acc:  0.10179640718562874    training loss:  3.497781434493215\n",
      "iteration  1060  :    training acc:  0.10099800399201597    training loss:  3.501420195160153\n",
      "iteration  1080  :    training acc:  0.10059880239520957    training loss:  3.505017427449722\n",
      "iteration  1100  :    training acc:  0.10099800399201597    training loss:  3.508565835047893\n",
      "iteration  1120  :    training acc:  0.0998003992015968    training loss:  3.5120632398005576\n",
      "iteration  1140  :    training acc:  0.09940119760479042    training loss:  3.515511343064554\n",
      "iteration  1160  :    training acc:  0.09820359281437126    training loss:  3.518913450228863\n",
      "iteration  1180  :    training acc:  0.09780439121756487    training loss:  3.522272559428133\n",
      "iteration  1200  :    training acc:  0.09820359281437126    training loss:  3.5255904557800393\n",
      "iteration  1220  :    training acc:  0.09820359281437126    training loss:  3.528867690033428\n",
      "iteration  1240  :    training acc:  0.09860279441117764    training loss:  3.532104023305927\n",
      "iteration  1260  :    training acc:  0.09860279441117764    training loss:  3.5352989640187915\n",
      "iteration  1280  :    training acc:  0.09860279441117764    training loss:  3.5384521697017917\n",
      "iteration  1300  :    training acc:  0.09780439121756487    training loss:  3.541563597917417\n",
      "iteration  1320  :    training acc:  0.09740518962075849    training loss:  3.544633328977813\n",
      "iteration  1340  :    training acc:  0.09660678642714571    training loss:  3.5476609695353347\n",
      "iteration  1360  :    training acc:  0.09780439121756487    training loss:  3.5506445637024724\n",
      "iteration  1380  :    training acc:  0.09900199600798404    training loss:  3.55357916343731\n",
      "iteration  1400  :    training acc:  0.09860279441117764    training loss:  3.5564558096134924\n",
      "iteration  1420  :    training acc:  0.09780439121756487    training loss:  3.559262297322082\n",
      "iteration  1440  :    training acc:  0.09860279441117764    training loss:  3.5619864839122446\n",
      "iteration  1460  :    training acc:  0.09900199600798404    training loss:  3.5646205558390394\n",
      "iteration  1480  :    training acc:  0.09820359281437126    training loss:  3.567163216179202\n",
      "iteration  1500  :    training acc:  0.09860279441117764    training loss:  3.5696186134933026\n",
      "iteration  1520  :    training acc:  0.09900199600798404    training loss:  3.5719935890677093\n",
      "iteration  1540  :    training acc:  0.09900199600798404    training loss:  3.5742952663845653\n",
      "iteration  1560  :    training acc:  0.0998003992015968    training loss:  3.576529763875587\n",
      "iteration  1580  :    training acc:  0.10019960079840319    training loss:  3.5787018062590286\n",
      "iteration  1600  :    training acc:  0.09900199600798404    training loss:  3.580814768714144\n",
      "iteration  1620  :    training acc:  0.09900199600798404    training loss:  3.582870828191588\n",
      "iteration  1640  :    training acc:  0.09940119760479042    training loss:  3.584871078020001\n",
      "iteration  1660  :    training acc:  0.10059880239520957    training loss:  3.5868155865446627\n",
      "iteration  1680  :    training acc:  0.10059880239520957    training loss:  3.5887034623805154\n",
      "iteration  1700  :    training acc:  0.10099800399201597    training loss:  3.5905330484954074\n",
      "iteration  1720  :    training acc:  0.10059880239520957    training loss:  3.592302383079121\n",
      "iteration  1740  :    training acc:  0.0998003992015968    training loss:  3.594009976926445\n",
      "iteration  1760  :    training acc:  0.10019960079840319    training loss:  3.5956557451098567\n",
      "iteration  1780  :    training acc:  0.10059880239520957    training loss:  3.5972417212066055\n",
      "iteration  1800  :    training acc:  0.10179640718562874    training loss:  3.598772194378353\n",
      "iteration  1820  :    training acc:  0.10179640718562874    training loss:  3.600253197360953\n",
      "iteration  1840  :    training acc:  0.10179640718562874    training loss:  3.6016916075843564\n",
      "iteration  1860  :    training acc:  0.10219560878243512    training loss:  3.6030942430845543\n",
      "iteration  1880  :    training acc:  0.1029940119760479    training loss:  3.6044672170256753\n",
      "iteration  1900  :    training acc:  0.10419161676646707    training loss:  3.605815620371133\n",
      "iteration  1920  :    training acc:  0.10419161676646707    training loss:  3.607143468996106\n",
      "iteration  1940  :    training acc:  0.10538922155688622    training loss:  3.608453809193583\n",
      "iteration  1960  :    training acc:  0.10578842315369262    training loss:  3.6097488902633685\n",
      "iteration  1980  :    training acc:  0.10419161676646707    training loss:  3.6110303456028268\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.10151878497202238    training loss:  16.381860528630654\n",
      "iteration  20  :    training acc:  0.11910471622701839    training loss:  3.537177984177349\n",
      "iteration  40  :    training acc:  0.11590727418065548    training loss:  3.496322108596629\n",
      "iteration  60  :    training acc:  0.11630695443645084    training loss:  3.47785504191434\n",
      "iteration  80  :    training acc:  0.11830535571542766    training loss:  3.4667607723357805\n",
      "iteration  100  :    training acc:  0.11710631494804156    training loss:  3.4594621588542487\n",
      "iteration  120  :    training acc:  0.11750599520383694    training loss:  3.45425524900374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  140  :    training acc:  0.11830535571542766    training loss:  3.45025677544157\n",
      "iteration  160  :    training acc:  0.11790567545963229    training loss:  3.4470261502360824\n",
      "iteration  180  :    training acc:  0.11750599520383694    training loss:  3.444326668635712\n",
      "iteration  200  :    training acc:  0.11710631494804156    training loss:  3.4420186117399325\n",
      "iteration  220  :    training acc:  0.11670663469224621    training loss:  3.4400128628322264\n",
      "iteration  240  :    training acc:  0.11630695443645084    training loss:  3.4382491000842257\n",
      "iteration  260  :    training acc:  0.11710631494804156    training loss:  3.4366845898928857\n",
      "iteration  280  :    training acc:  0.11670663469224621    training loss:  3.435287967243758\n",
      "iteration  300  :    training acc:  0.11590727418065548    training loss:  3.434035557226351\n",
      "iteration  320  :    training acc:  0.11710631494804156    training loss:  3.43290908220135\n",
      "iteration  340  :    training acc:  0.11710631494804156    training loss:  3.4318941685323727\n",
      "iteration  360  :    training acc:  0.11670663469224621    training loss:  3.430979337037989\n",
      "iteration  380  :    training acc:  0.11670663469224621    training loss:  3.430155297972299\n",
      "iteration  400  :    training acc:  0.11710631494804156    training loss:  3.4294144443334207\n",
      "iteration  420  :    training acc:  0.11750599520383694    training loss:  3.4287504781833165\n",
      "iteration  440  :    training acc:  0.11870503597122302    training loss:  3.4281581285336813\n",
      "iteration  460  :    training acc:  0.11910471622701839    training loss:  3.4276329338133458\n",
      "iteration  480  :    training acc:  0.11870503597122302    training loss:  3.4271710709934218\n",
      "iteration  500  :    training acc:  0.11870503597122302    training loss:  3.426769219319877\n",
      "iteration  520  :    training acc:  0.11790567545963229    training loss:  3.4264244505687556\n",
      "iteration  540  :    training acc:  0.11790567545963229    training loss:  3.4261341405736783\n",
      "iteration  560  :    training acc:  0.11750599520383694    training loss:  3.42589589896482\n",
      "iteration  580  :    training acc:  0.11830535571542766    training loss:  3.4257075158918955\n",
      "iteration  600  :    training acc:  0.11790567545963229    training loss:  3.4255669260988255\n",
      "iteration  620  :    training acc:  0.11790567545963229    training loss:  3.4254721920231663\n",
      "iteration  640  :    training acc:  0.11830535571542766    training loss:  3.425421508401394\n",
      "iteration  660  :    training acc:  0.11790567545963229    training loss:  3.425413230861242\n",
      "iteration  680  :    training acc:  0.11710631494804156    training loss:  3.4254459298512727\n",
      "iteration  700  :    training acc:  0.11630695443645084    training loss:  3.425518468735263\n",
      "iteration  720  :    training acc:  0.11710631494804156    training loss:  3.4256301007508307\n",
      "iteration  740  :    training acc:  0.11750599520383694    training loss:  3.425780573488325\n",
      "iteration  760  :    training acc:  0.11750599520383694    training loss:  3.4259702210502625\n",
      "iteration  780  :    training acc:  0.11710631494804156    training loss:  3.4262000127027763\n",
      "iteration  800  :    training acc:  0.11630695443645084    training loss:  3.4264715144083904\n",
      "iteration  820  :    training acc:  0.11710631494804156    training loss:  3.4267867163176082\n",
      "iteration  840  :    training acc:  0.11830535571542766    training loss:  3.4271477185581047\n",
      "iteration  860  :    training acc:  0.11670663469224621    training loss:  3.427556435554325\n",
      "iteration  880  :    training acc:  0.11550759392486011    training loss:  3.428014962880456\n",
      "iteration  900  :    training acc:  0.11590727418065548    training loss:  3.428528416820892\n",
      "iteration  920  :    training acc:  0.11550759392486011    training loss:  3.429114588974127\n",
      "iteration  940  :    training acc:  0.11510791366906475    training loss:  3.4298295520115016\n",
      "iteration  960  :    training acc:  0.11550759392486011    training loss:  3.430822366655142\n",
      "iteration  980  :    training acc:  0.11191047162270183    training loss:  3.432401697336666\n",
      "iteration  1000  :    training acc:  0.1123101518784972    training loss:  3.4349255725391457\n",
      "iteration  1020  :    training acc:  0.11430855315747403    training loss:  3.4382173145597252\n",
      "iteration  1040  :    training acc:  0.11510791366906475    training loss:  3.441300940793394\n",
      "iteration  1060  :    training acc:  0.11550759392486011    training loss:  3.4434636929628133\n",
      "iteration  1080  :    training acc:  0.11670663469224621    training loss:  3.4448010218340457\n",
      "iteration  1100  :    training acc:  0.11590727418065548    training loss:  3.445671647748981\n",
      "iteration  1120  :    training acc:  0.11670663469224621    training loss:  3.446332123331077\n",
      "iteration  1140  :    training acc:  0.11670663469224621    training loss:  3.4469130460810717\n",
      "iteration  1160  :    training acc:  0.11710631494804156    training loss:  3.4474728561723604\n",
      "iteration  1180  :    training acc:  0.11670663469224621    training loss:  3.448036284162113\n",
      "iteration  1200  :    training acc:  0.11750599520383694    training loss:  3.448613472712943\n",
      "iteration  1220  :    training acc:  0.11670663469224621    training loss:  3.449208495238453\n",
      "iteration  1240  :    training acc:  0.11630695443645084    training loss:  3.449823020648909\n",
      "iteration  1260  :    training acc:  0.11470823341326938    training loss:  3.450457886700307\n",
      "iteration  1280  :    training acc:  0.11550759392486011    training loss:  3.451113793277263\n",
      "iteration  1300  :    training acc:  0.11510791366906475    training loss:  3.45179163207403\n",
      "iteration  1320  :    training acc:  0.11390887290167866    training loss:  3.45249267220093\n",
      "iteration  1340  :    training acc:  0.11390887290167866    training loss:  3.4532186957407136\n",
      "iteration  1360  :    training acc:  0.11470823341326938    training loss:  3.4539721230233753\n",
      "iteration  1380  :    training acc:  0.1135091926458833    training loss:  3.454756140606828\n",
      "iteration  1400  :    training acc:  0.1135091926458833    training loss:  3.455574824762361\n",
      "iteration  1420  :    training acc:  0.11270983213429256    training loss:  3.4564332266138087\n",
      "iteration  1440  :    training acc:  0.11191047162270183    training loss:  3.4573373407035413\n",
      "iteration  1460  :    training acc:  0.1123101518784972    training loss:  3.458293809006597\n",
      "iteration  1480  :    training acc:  0.1135091926458833    training loss:  3.459309129702684\n",
      "iteration  1500  :    training acc:  0.1135091926458833    training loss:  3.4603881174142925\n",
      "iteration  1520  :    training acc:  0.11310951239008793    training loss:  3.4615315740134593\n",
      "iteration  1540  :    training acc:  0.11310951239008793    training loss:  3.462733758003718\n",
      "iteration  1560  :    training acc:  0.11191047162270183    training loss:  3.463981062206982\n",
      "iteration  1580  :    training acc:  0.11151079136690648    training loss:  3.4652533696108487\n",
      "iteration  1600  :    training acc:  0.11031175059952038    training loss:  3.4665281253312368\n",
      "iteration  1620  :    training acc:  0.11071143085531575    training loss:  3.467785330017954\n",
      "iteration  1640  :    training acc:  0.1111111111111111    training loss:  3.4690112961238486\n",
      "iteration  1660  :    training acc:  0.10951239008792965    training loss:  3.470200098871867\n",
      "iteration  1680  :    training acc:  0.11031175059952038    training loss:  3.4713528190201552\n",
      "iteration  1700  :    training acc:  0.11031175059952038    training loss:  3.4724753030655684\n",
      "iteration  1720  :    training acc:  0.10991207034372502    training loss:  3.473575408151398\n",
      "iteration  1740  :    training acc:  0.10951239008792965    training loss:  3.4746606327838254\n",
      "iteration  1760  :    training acc:  0.10991207034372502    training loss:  3.475736677809385\n",
      "iteration  1780  :    training acc:  0.1091127098321343    training loss:  3.476807011170486\n",
      "iteration  1800  :    training acc:  0.10831334932054357    training loss:  3.4778731587489538\n",
      "iteration  1820  :    training acc:  0.10831334932054357    training loss:  3.478935324811129\n",
      "iteration  1840  :    training acc:  0.10871302957633892    training loss:  3.479993019064846\n",
      "iteration  1860  :    training acc:  0.10871302957633892    training loss:  3.481045520606092\n",
      "iteration  1880  :    training acc:  0.1079136690647482    training loss:  3.482092144695429\n",
      "iteration  1900  :    training acc:  0.10751398880895284    training loss:  3.4831323539027768\n",
      "iteration  1920  :    training acc:  0.10751398880895284    training loss:  3.4841657754185538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1940  :    training acc:  0.10831334932054357    training loss:  3.4851921756349715\n",
      "iteration  1960  :    training acc:  0.10751398880895284    training loss:  3.4862114235604698\n",
      "iteration  1980  :    training acc:  0.10751398880895284    training loss:  3.487223458114869\n",
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.11174458380843785    training loss:  17.6205678215087\n",
      "iteration  20  :    training acc:  0.11592550361079437    training loss:  3.69280956013132\n",
      "iteration  40  :    training acc:  0.11174458380843785    training loss:  3.6287189007087743\n",
      "iteration  60  :    training acc:  0.11174458380843785    training loss:  3.5981943040043505\n",
      "iteration  80  :    training acc:  0.11250475104522994    training loss:  3.5794017330992793\n",
      "iteration  100  :    training acc:  0.10946408209806158    training loss:  3.5666085272342523\n",
      "iteration  120  :    training acc:  0.10832383124287344    training loss:  3.557471834212815\n",
      "iteration  140  :    training acc:  0.11060433295324971    training loss:  3.551044741227237\n",
      "iteration  160  :    training acc:  0.11060433295324971    training loss:  3.552994870259304\n",
      "iteration  180  :    training acc:  0.10756366400608133    training loss:  3.6549913526086217\n",
      "iteration  200  :    training acc:  0.10604332953249715    training loss:  3.6774610698469314\n",
      "iteration  220  :    training acc:  0.10680349676928924    training loss:  3.6816972215117336\n",
      "iteration  240  :    training acc:  0.1056632459141011    training loss:  3.685108666850663\n",
      "iteration  260  :    training acc:  0.10414291144051692    training loss:  3.6879228675368867\n",
      "iteration  280  :    training acc:  0.10528316229570506    training loss:  3.6902469867354353\n",
      "iteration  300  :    training acc:  0.1056632459141011    training loss:  3.69216642423968\n",
      "iteration  320  :    training acc:  0.1056632459141011    training loss:  3.693751170501752\n",
      "iteration  340  :    training acc:  0.10604332953249715    training loss:  3.695058505840199\n",
      "iteration  360  :    training acc:  0.10680349676928924    training loss:  3.6961333486335692\n",
      "iteration  380  :    training acc:  0.1056632459141011    training loss:  3.697006667865962\n",
      "iteration  400  :    training acc:  0.10414291144051692    training loss:  3.6976918810204373\n",
      "iteration  420  :    training acc:  0.10338274420372481    training loss:  3.6981788973491905\n",
      "iteration  440  :    training acc:  0.10300266058532877    training loss:  3.6984263506885267\n",
      "iteration  460  :    training acc:  0.10528316229570506    training loss:  3.698357128784592\n",
      "iteration  480  :    training acc:  0.1064234131508932    training loss:  3.6978759604957694\n",
      "iteration  500  :    training acc:  0.10718358038768529    training loss:  3.696943353813498\n",
      "iteration  520  :    training acc:  0.10490307867730901    training loss:  3.6956857682991036\n",
      "iteration  540  :    training acc:  0.1056632459141011    training loss:  3.694395173041599\n",
      "iteration  560  :    training acc:  0.10338274420372481    training loss:  3.693340449378151\n",
      "iteration  580  :    training acc:  0.10186240973014063    training loss:  3.6926535691978493\n",
      "iteration  600  :    training acc:  0.10376282782212087    training loss:  3.6924339986550656\n",
      "iteration  620  :    training acc:  0.10452299505891297    training loss:  3.6928424808758153\n",
      "iteration  640  :    training acc:  0.10262257696693272    training loss:  3.694032015384849\n",
      "iteration  660  :    training acc:  0.10338274420372481    training loss:  3.6959342999124862\n",
      "iteration  680  :    training acc:  0.10338274420372481    training loss:  3.6982869061916754\n",
      "iteration  700  :    training acc:  0.10300266058532877    training loss:  3.7008515276670275\n",
      "iteration  720  :    training acc:  0.1056632459141011    training loss:  3.7034874448451265\n",
      "iteration  740  :    training acc:  0.10680349676928924    training loss:  3.7061225408125913\n",
      "iteration  760  :    training acc:  0.10604332953249715    training loss:  3.708732081780699\n",
      "iteration  780  :    training acc:  0.10756366400608133    training loss:  3.7113306293119255\n",
      "iteration  800  :    training acc:  0.10718358038768529    training loss:  3.7139498318632445\n",
      "iteration  820  :    training acc:  0.10718358038768529    training loss:  3.7166022939984846\n",
      "iteration  840  :    training acc:  0.10794374762447738    training loss:  3.7192712782626924\n",
      "iteration  860  :    training acc:  0.10794374762447738    training loss:  3.7219293916493905\n",
      "iteration  880  :    training acc:  0.10794374762447738    training loss:  3.7245548429720507\n",
      "iteration  900  :    training acc:  0.10756366400608133    training loss:  3.727135645872838\n",
      "iteration  920  :    training acc:  0.10718358038768529    training loss:  3.7296678177528126\n",
      "iteration  940  :    training acc:  0.10718358038768529    training loss:  3.7321523369889693\n",
      "iteration  960  :    training acc:  0.10832383124287344    training loss:  3.7345923925782007\n",
      "iteration  980  :    training acc:  0.10832383124287344    training loss:  3.7369912834593397\n",
      "iteration  1000  :    training acc:  0.10680349676928924    training loss:  3.7393513257303073\n",
      "iteration  1020  :    training acc:  0.1064234131508932    training loss:  3.741674033518956\n",
      "iteration  1040  :    training acc:  0.1064234131508932    training loss:  3.743961125552564\n",
      "iteration  1060  :    training acc:  0.10528316229570506    training loss:  3.7462153936211324\n",
      "iteration  1080  :    training acc:  0.10528316229570506    training loss:  3.748440863056618\n",
      "iteration  1100  :    training acc:  0.1056632459141011    training loss:  3.7506423925832344\n",
      "iteration  1120  :    training acc:  0.10604332953249715    training loss:  3.75282513078482\n",
      "iteration  1140  :    training acc:  0.10680349676928924    training loss:  3.754994081217196\n",
      "iteration  1160  :    training acc:  0.10718358038768529    training loss:  3.7571538077554045\n",
      "iteration  1180  :    training acc:  0.10718358038768529    training loss:  3.759308228182993\n",
      "iteration  1200  :    training acc:  0.10756366400608133    training loss:  3.7614604649297054\n",
      "iteration  1220  :    training acc:  0.10832383124287344    training loss:  3.763612760765673\n",
      "iteration  1240  :    training acc:  0.10832383124287344    training loss:  3.765766477807964\n",
      "iteration  1260  :    training acc:  0.10946408209806158    training loss:  3.7679221807232186\n",
      "iteration  1280  :    training acc:  0.10946408209806158    training loss:  3.77007978129169\n",
      "iteration  1300  :    training acc:  0.10946408209806158    training loss:  3.7722387090143235\n",
      "iteration  1320  :    training acc:  0.10832383124287344    training loss:  3.774398073900095\n",
      "iteration  1340  :    training acc:  0.10946408209806158    training loss:  3.7765567972427125\n",
      "iteration  1360  :    training acc:  0.10870391486126948    training loss:  3.7787137005876668\n",
      "iteration  1380  :    training acc:  0.10794374762447738    training loss:  3.7808675632977247\n",
      "iteration  1400  :    training acc:  0.1064234131508932    training loss:  3.7830171840234117\n",
      "iteration  1420  :    training acc:  0.10680349676928924    training loss:  3.7851615003659975\n",
      "iteration  1440  :    training acc:  0.10718358038768529    training loss:  3.7872998136166474\n",
      "iteration  1460  :    training acc:  0.10718358038768529    training loss:  3.7894321074648025\n",
      "iteration  1480  :    training acc:  0.10718358038768529    training loss:  3.791559329201666\n",
      "iteration  1500  :    training acc:  0.10756366400608133    training loss:  3.793683366694713\n",
      "iteration  1520  :    training acc:  0.10794374762447738    training loss:  3.7958064680140478\n",
      "iteration  1540  :    training acc:  0.10832383124287344    training loss:  3.7979301816918816\n",
      "iteration  1560  :    training acc:  0.10908399847966553    training loss:  3.800054354413408\n",
      "iteration  1580  :    training acc:  0.10908399847966553    training loss:  3.8021767635366657\n",
      "iteration  1600  :    training acc:  0.10870391486126948    training loss:  3.804293463623394\n",
      "iteration  1620  :    training acc:  0.10832383124287344    training loss:  3.8063994665541285\n",
      "iteration  1640  :    training acc:  0.10794374762447738    training loss:  3.8084893606038737\n",
      "iteration  1660  :    training acc:  0.10832383124287344    training loss:  3.810557711779639\n",
      "iteration  1680  :    training acc:  0.10870391486126948    training loss:  3.812599276150816\n",
      "iteration  1700  :    training acc:  0.10870391486126948    training loss:  3.814609107754303\n",
      "iteration  1720  :    training acc:  0.10832383124287344    training loss:  3.8165826307157027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1740  :    training acc:  0.10908399847966553    training loss:  3.8185157117885615\n",
      "iteration  1760  :    training acc:  0.11060433295324971    training loss:  3.8204047417026983\n",
      "iteration  1780  :    training acc:  0.11174458380843785    training loss:  3.822246713625228\n",
      "iteration  1800  :    training acc:  0.1121246674268339    training loss:  3.824039276203667\n",
      "iteration  1820  :    training acc:  0.11174458380843785    training loss:  3.825780745168153\n",
      "iteration  1840  :    training acc:  0.11288483466362599    training loss:  3.827470096005806\n",
      "iteration  1860  :    training acc:  0.1113645001900418    training loss:  3.829107041396835\n",
      "iteration  1880  :    training acc:  0.10984416571645762    training loss:  3.830692403850968\n",
      "iteration  1900  :    training acc:  0.10946408209806158    training loss:  3.8322290519371465\n",
      "iteration  1920  :    training acc:  0.10870391486126948    training loss:  3.8337235419864175\n",
      "iteration  1940  :    training acc:  0.10870391486126948    training loss:  3.8351881694132954\n",
      "iteration  1960  :    training acc:  0.10832383124287344    training loss:  3.836642354106675\n",
      "iteration  1980  :    training acc:  0.10870391486126948    training loss:  3.83811138342014\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.09289400555776102    training loss:  18.227699787932266\n",
      "iteration  20  :    training acc:  0.10520047637951568    training loss:  3.776077694185788\n",
      "iteration  40  :    training acc:  0.10400952759031362    training loss:  3.7975674214225803\n",
      "iteration  60  :    training acc:  0.10400952759031362    training loss:  3.7851994237238857\n",
      "iteration  80  :    training acc:  0.10639142516871775    training loss:  3.776820595452256\n",
      "iteration  100  :    training acc:  0.10639142516871775    training loss:  3.777232887216649\n",
      "iteration  120  :    training acc:  0.1055974593092497    training loss:  3.772730172341522\n",
      "iteration  140  :    training acc:  0.10639142516871775    training loss:  3.7654318117449512\n",
      "iteration  160  :    training acc:  0.10758237395791981    training loss:  3.7604180187980094\n",
      "iteration  180  :    training acc:  0.10996427153632393    training loss:  3.7575830232284573\n",
      "iteration  200  :    training acc:  0.11075823739579198    training loss:  3.7545671016059248\n",
      "iteration  220  :    training acc:  0.11115522032552601    training loss:  3.7526983399547262\n",
      "iteration  240  :    training acc:  0.11115522032552601    training loss:  3.749993496133127\n",
      "iteration  260  :    training acc:  0.11155220325526002    training loss:  3.749827609385888\n",
      "iteration  280  :    training acc:  0.11274315204446209    training loss:  3.745246711330405\n",
      "iteration  300  :    training acc:  0.11314013497419612    training loss:  3.7507542061613584\n",
      "iteration  320  :    training acc:  0.11075823739579198    training loss:  3.7391644414209537\n",
      "iteration  340  :    training acc:  0.11274315204446209    training loss:  3.7513497628728425\n",
      "iteration  360  :    training acc:  0.11115522032552601    training loss:  3.7385597533648736\n",
      "iteration  380  :    training acc:  0.11274315204446209    training loss:  3.751297735058088\n",
      "iteration  400  :    training acc:  0.11036125446605796    training loss:  3.7391865537818068\n",
      "iteration  420  :    training acc:  0.11155220325526002    training loss:  3.7517199263375858\n",
      "iteration  440  :    training acc:  0.10758237395791981    training loss:  3.740409846995077\n",
      "iteration  460  :    training acc:  0.10956728860658992    training loss:  3.752201378387097\n",
      "iteration  480  :    training acc:  0.10599444223898373    training loss:  3.7416383024000592\n",
      "iteration  500  :    training acc:  0.10877332274712187    training loss:  3.7525014807811776\n",
      "iteration  520  :    training acc:  0.10639142516871775    training loss:  3.7426375116404893\n",
      "iteration  540  :    training acc:  0.10956728860658992    training loss:  3.752622126357577\n",
      "iteration  560  :    training acc:  0.10678840809845176    training loss:  3.7434963921948405\n",
      "iteration  580  :    training acc:  0.10797935688765382    training loss:  3.7526696571577807\n",
      "iteration  600  :    training acc:  0.10758237395791981    training loss:  3.744447497568506\n",
      "iteration  620  :    training acc:  0.10718539102818579    training loss:  3.75274024086226\n",
      "iteration  640  :    training acc:  0.1055974593092497    training loss:  3.7457217112133216\n",
      "iteration  660  :    training acc:  0.10678840809845176    training loss:  3.752848423360491\n",
      "iteration  680  :    training acc:  0.10599444223898373    training loss:  3.7473542905478703\n",
      "iteration  700  :    training acc:  0.10639142516871775    training loss:  3.7528427778834565\n",
      "iteration  720  :    training acc:  0.10678840809845176    training loss:  3.7489235493127993\n",
      "iteration  740  :    training acc:  0.10718539102818579    training loss:  3.7525103604399126\n",
      "iteration  760  :    training acc:  0.10599444223898373    training loss:  3.7499131938557886\n",
      "iteration  780  :    training acc:  0.10440651052004764    training loss:  3.7519154650569297\n",
      "iteration  800  :    training acc:  0.10361254466057959    training loss:  3.7502973067345007\n",
      "iteration  820  :    training acc:  0.10440651052004764    training loss:  3.751253418634359\n",
      "iteration  840  :    training acc:  0.1055974593092497    training loss:  3.7502624682317096\n",
      "iteration  860  :    training acc:  0.10599444223898373    training loss:  3.7505716251824177\n",
      "iteration  880  :    training acc:  0.10678840809845176    training loss:  3.749890966118714\n",
      "iteration  900  :    training acc:  0.10718539102818579    training loss:  3.7497670334893054\n",
      "iteration  920  :    training acc:  0.10678840809845176    training loss:  3.7491366472560417\n",
      "iteration  940  :    training acc:  0.10718539102818579    training loss:  3.748660777606166\n",
      "iteration  960  :    training acc:  0.10718539102818579    training loss:  3.747917919191454\n",
      "iteration  980  :    training acc:  0.10639142516871775    training loss:  3.7472140379760046\n",
      "iteration  1000  :    training acc:  0.1055974593092497    training loss:  3.746491536164422\n",
      "iteration  1020  :    training acc:  0.10678840809845176    training loss:  3.745911520342667\n",
      "iteration  1040  :    training acc:  0.10599444223898373    training loss:  3.7454423014510265\n",
      "iteration  1060  :    training acc:  0.10797935688765382    training loss:  3.7450535617431364\n",
      "iteration  1080  :    training acc:  0.11075823739579198    training loss:  3.7446686707661105\n",
      "iteration  1100  :    training acc:  0.11353711790393013    training loss:  3.7442398479737795\n",
      "iteration  1120  :    training acc:  0.11155220325526002    training loss:  3.743757553332596\n",
      "iteration  1140  :    training acc:  0.11194918618499404    training loss:  3.743242141042336\n",
      "iteration  1160  :    training acc:  0.11155220325526002    training loss:  3.7427180244107507\n",
      "iteration  1180  :    training acc:  0.11234616911472807    training loss:  3.74220611662965\n",
      "iteration  1200  :    training acc:  0.11194918618499404    training loss:  3.741725697572574\n",
      "iteration  1220  :    training acc:  0.10956728860658992    training loss:  3.741295677412861\n",
      "iteration  1240  :    training acc:  0.1091703056768559    training loss:  3.740933644560348\n",
      "iteration  1260  :    training acc:  0.10797935688765382    training loss:  3.7406546605781985\n",
      "iteration  1280  :    training acc:  0.10797935688765382    training loss:  3.740470808943976\n",
      "iteration  1300  :    training acc:  0.1091703056768559    training loss:  3.7403914274230194\n",
      "iteration  1320  :    training acc:  0.11075823739579198    training loss:  3.740423725256924\n",
      "iteration  1340  :    training acc:  0.10956728860658992    training loss:  3.740573526283627\n",
      "iteration  1360  :    training acc:  0.11075823739579198    training loss:  3.740845858493352\n",
      "iteration  1380  :    training acc:  0.11155220325526002    training loss:  3.741245120266465\n",
      "iteration  1400  :    training acc:  0.10996427153632393    training loss:  3.741774765144235\n",
      "iteration  1420  :    training acc:  0.10996427153632393    training loss:  3.7424368563372576\n",
      "iteration  1440  :    training acc:  0.11036125446605796    training loss:  3.743232108081888\n",
      "iteration  1460  :    training acc:  0.10996427153632393    training loss:  3.7441605143277794\n",
      "iteration  1480  :    training acc:  0.10877332274712187    training loss:  3.745221126697483\n",
      "iteration  1500  :    training acc:  0.1091703056768559    training loss:  3.7464081521264005\n",
      "iteration  1520  :    training acc:  0.10797935688765382    training loss:  3.747701733555114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1540  :    training acc:  0.10758237395791981    training loss:  3.749057306347719\n",
      "iteration  1560  :    training acc:  0.10639142516871775    training loss:  3.750403487356427\n",
      "iteration  1580  :    training acc:  0.1091703056768559    training loss:  3.7516541434438224\n",
      "iteration  1600  :    training acc:  0.10837633981738785    training loss:  3.752727330311934\n",
      "iteration  1620  :    training acc:  0.11036125446605796    training loss:  3.7535633439243967\n",
      "iteration  1640  :    training acc:  0.11155220325526002    training loss:  3.7541390555847465\n",
      "iteration  1660  :    training acc:  0.11115522032552601    training loss:  3.754467934552159\n",
      "iteration  1680  :    training acc:  0.11075823739579198    training loss:  3.754583193625224\n",
      "iteration  1700  :    training acc:  0.11075823739579198    training loss:  3.7545188677171137\n",
      "iteration  1720  :    training acc:  0.11234616911472807    training loss:  3.754298978844854\n",
      "iteration  1740  :    training acc:  0.11274315204446209    training loss:  3.7539344146735725\n",
      "iteration  1760  :    training acc:  0.11314013497419612    training loss:  3.7534247006897785\n",
      "iteration  1780  :    training acc:  0.1147280666931322    training loss:  3.752762432744385\n",
      "iteration  1800  :    training acc:  0.11433108376339818    training loss:  3.7519383291586896\n",
      "iteration  1820  :    training acc:  0.11353711790393013    training loss:  3.750945236923656\n",
      "iteration  1840  :    training acc:  0.11314013497419612    training loss:  3.7497802002910885\n",
      "iteration  1860  :    training acc:  0.11155220325526002    training loss:  3.7484447342229\n",
      "iteration  1880  :    training acc:  0.11115522032552601    training loss:  3.746945571976543\n",
      "iteration  1900  :    training acc:  0.11234616911472807    training loss:  3.7453009972735765\n",
      "iteration  1920  :    training acc:  0.11314013497419612    training loss:  3.7435532741050155\n",
      "iteration  1940  :    training acc:  0.1147280666931322    training loss:  3.7417708246163106\n",
      "iteration  1960  :    training acc:  0.1147280666931322    training loss:  3.740029068579403\n",
      "iteration  1980  :    training acc:  0.1147280666931322    training loss:  3.738389841250393\n",
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.09541511771995044    training loss:  17.617316775119946\n",
      "iteration  20  :    training acc:  0.1036761668731929    training loss:  3.712751194178326\n",
      "iteration  40  :    training acc:  0.10863279636513837    training loss:  3.6821204424522342\n",
      "iteration  60  :    training acc:  0.11028500619578686    training loss:  3.6646949610144306\n",
      "iteration  80  :    training acc:  0.10780669144981413    training loss:  3.6532128056133883\n",
      "iteration  100  :    training acc:  0.107393638992152    training loss:  3.64514434548712\n",
      "iteration  120  :    training acc:  0.10698058653448989    training loss:  3.6392667975772004\n",
      "iteration  140  :    training acc:  0.10780669144981413    training loss:  3.634942494978073\n",
      "iteration  160  :    training acc:  0.10615448161916563    training loss:  3.6323205540614376\n",
      "iteration  180  :    training acc:  0.10285006195786865    training loss:  3.636874012503087\n",
      "iteration  200  :    training acc:  0.10491532424617926    training loss:  3.655776360256753\n",
      "iteration  220  :    training acc:  0.1036761668731929    training loss:  3.665495816311268\n",
      "iteration  240  :    training acc:  0.10450227178851715    training loss:  3.6680253391004167\n",
      "iteration  260  :    training acc:  0.10491532424617926    training loss:  3.669256308484876\n",
      "iteration  280  :    training acc:  0.10450227178851715    training loss:  3.6702450739816843\n",
      "iteration  300  :    training acc:  0.10532837670384139    training loss:  3.6711436344610693\n",
      "iteration  320  :    training acc:  0.10615448161916563    training loss:  3.6719815357560366\n",
      "iteration  340  :    training acc:  0.10698058653448989    training loss:  3.672760234059874\n",
      "iteration  360  :    training acc:  0.10780669144981413    training loss:  3.6734651025930356\n",
      "iteration  380  :    training acc:  0.10821974390747625    training loss:  3.6740692897825697\n",
      "iteration  400  :    training acc:  0.107393638992152    training loss:  3.674539083776394\n",
      "iteration  420  :    training acc:  0.10656753407682776    training loss:  3.6748431676424764\n",
      "iteration  440  :    training acc:  0.10656753407682776    training loss:  3.6749672280360137\n",
      "iteration  460  :    training acc:  0.10698058653448989    training loss:  3.674934415533523\n",
      "iteration  480  :    training acc:  0.107393638992152    training loss:  3.6748203317650057\n",
      "iteration  500  :    training acc:  0.107393638992152    training loss:  3.674736660216363\n",
      "iteration  520  :    training acc:  0.107393638992152    training loss:  3.6747871399036973\n",
      "iteration  540  :    training acc:  0.10780669144981413    training loss:  3.6750396211064253\n",
      "iteration  560  :    training acc:  0.10780669144981413    training loss:  3.675530211857837\n",
      "iteration  580  :    training acc:  0.10532837670384139    training loss:  3.676279182257035\n",
      "iteration  600  :    training acc:  0.10698058653448989    training loss:  3.6773008767553224\n",
      "iteration  620  :    training acc:  0.10698058653448989    training loss:  3.6786028144588294\n",
      "iteration  640  :    training acc:  0.10780669144981413    training loss:  3.680180946155553\n",
      "iteration  660  :    training acc:  0.10698058653448989    training loss:  3.682021058649993\n",
      "iteration  680  :    training acc:  0.10532837670384139    training loss:  3.684105964996212\n",
      "iteration  700  :    training acc:  0.10408921933085502    training loss:  3.6864228102240517\n",
      "iteration  720  :    training acc:  0.10574142916150352    training loss:  3.688967983557786\n",
      "iteration  740  :    training acc:  0.10656753407682776    training loss:  3.691749454830404\n",
      "iteration  760  :    training acc:  0.10698058653448989    training loss:  3.6947852936223446\n",
      "iteration  780  :    training acc:  0.1090458488228005    training loss:  3.698095663681632\n",
      "iteration  800  :    training acc:  0.1090458488228005    training loss:  3.7016906873193576\n",
      "iteration  820  :    training acc:  0.1127633209417596    training loss:  3.705567096804927\n",
      "iteration  840  :    training acc:  0.11235026848409747    training loss:  3.709716363216812\n",
      "iteration  860  :    training acc:  0.1127633209417596    training loss:  3.7141275066199015\n",
      "iteration  880  :    training acc:  0.1127633209417596    training loss:  3.7187894183868675\n",
      "iteration  900  :    training acc:  0.1090458488228005    training loss:  3.7237074598680686\n",
      "iteration  920  :    training acc:  0.1111111111111111    training loss:  3.7289057498716054\n",
      "iteration  940  :    training acc:  0.1090458488228005    training loss:  3.7343915224930497\n",
      "iteration  960  :    training acc:  0.10945890128046262    training loss:  3.7401276385045845\n",
      "iteration  980  :    training acc:  0.11235026848409747    training loss:  3.7460453395177455\n",
      "iteration  1000  :    training acc:  0.1127633209417596    training loss:  3.752078956548735\n",
      "iteration  1020  :    training acc:  0.1127633209417596    training loss:  3.7582012810690566\n",
      "iteration  1040  :    training acc:  0.11152416356877323    training loss:  3.7644315953238823\n",
      "iteration  1060  :    training acc:  0.11069805865344899    training loss:  3.7708040703656653\n",
      "iteration  1080  :    training acc:  0.1111111111111111    training loss:  3.7773383764905892\n",
      "iteration  1100  :    training acc:  0.11069805865344899    training loss:  3.784037540024336\n",
      "iteration  1120  :    training acc:  0.1111111111111111    training loss:  3.7908962270207183\n",
      "iteration  1140  :    training acc:  0.10987195373812474    training loss:  3.79790507180707\n",
      "iteration  1160  :    training acc:  0.10987195373812474    training loss:  3.80504988303392\n",
      "iteration  1180  :    training acc:  0.11069805865344899    training loss:  3.81230981923604\n",
      "iteration  1200  :    training acc:  0.10945890128046262    training loss:  3.8196594760471503\n",
      "iteration  1220  :    training acc:  0.10945890128046262    training loss:  3.8270759109446524\n",
      "iteration  1240  :    training acc:  0.10987195373812474    training loss:  3.834544733047408\n",
      "iteration  1260  :    training acc:  0.10945890128046262    training loss:  3.842060447655619\n",
      "iteration  1280  :    training acc:  0.11028500619578686    training loss:  3.8496233020354187\n",
      "iteration  1300  :    training acc:  0.11069805865344899    training loss:  3.8572346044187795\n",
      "iteration  1320  :    training acc:  0.11193721602643536    training loss:  3.8648910773677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1340  :    training acc:  0.11235026848409747    training loss:  3.872582173516511\n",
      "iteration  1360  :    training acc:  0.11152416356877323    training loss:  3.8802921334508085\n",
      "iteration  1380  :    training acc:  0.11193721602643536    training loss:  3.888003365979278\n",
      "iteration  1400  :    training acc:  0.10987195373812474    training loss:  3.895698530152898\n",
      "iteration  1420  :    training acc:  0.10863279636513837    training loss:  3.903361843269502\n",
      "iteration  1440  :    training acc:  0.1090458488228005    training loss:  3.9109798501672146\n",
      "iteration  1460  :    training acc:  0.10821974390747625    training loss:  3.9185408669050346\n",
      "iteration  1480  :    training acc:  0.10780669144981413    training loss:  3.9260340116463204\n",
      "iteration  1500  :    training acc:  0.107393638992152    training loss:  3.9334503029571306\n",
      "iteration  1520  :    training acc:  0.10532837670384139    training loss:  3.940786258679106\n",
      "iteration  1540  :    training acc:  0.10408921933085502    training loss:  3.9480473144959354\n",
      "iteration  1560  :    training acc:  0.10408921933085502    training loss:  3.955248798504954\n",
      "iteration  1580  :    training acc:  0.10450227178851715    training loss:  3.962415042059491\n",
      "iteration  1600  :    training acc:  0.10450227178851715    training loss:  3.969578373633878\n",
      "iteration  1620  :    training acc:  0.1036761668731929    training loss:  3.976778618873631\n",
      "iteration  1640  :    training acc:  0.10532837670384139    training loss:  3.9840617815416053\n",
      "iteration  1660  :    training acc:  0.10574142916150352    training loss:  3.9914743609395633\n",
      "iteration  1680  :    training acc:  0.10656753407682776    training loss:  3.9990493665839377\n",
      "iteration  1700  :    training acc:  0.10656753407682776    training loss:  4.006787947912774\n",
      "iteration  1720  :    training acc:  0.10615448161916563    training loss:  4.014652481630845\n",
      "iteration  1740  :    training acc:  0.10780669144981413    training loss:  4.022579494147364\n",
      "iteration  1760  :    training acc:  0.107393638992152    training loss:  4.030501768809869\n",
      "iteration  1780  :    training acc:  0.10821974390747625    training loss:  4.038365704458067\n",
      "iteration  1800  :    training acc:  0.10863279636513837    training loss:  4.0461370866434105\n",
      "iteration  1820  :    training acc:  0.10987195373812474    training loss:  4.053797916015223\n",
      "iteration  1840  :    training acc:  0.10863279636513837    training loss:  4.061341157295657\n",
      "iteration  1860  :    training acc:  0.10821974390747625    training loss:  4.068766927065498\n",
      "iteration  1880  :    training acc:  0.1090458488228005    training loss:  4.076080025968311\n",
      "iteration  1900  :    training acc:  0.10945890128046262    training loss:  4.083288097593824\n",
      "iteration  1920  :    training acc:  0.10945890128046262    training loss:  4.090400234258975\n",
      "iteration  1940  :    training acc:  0.10821974390747625    training loss:  4.097426061499746\n",
      "iteration  1960  :    training acc:  0.10863279636513837    training loss:  4.104375223288477\n",
      "iteration  1980  :    training acc:  0.10945890128046262    training loss:  4.111257091815943\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.09706814580031696    training loss:  13.901257275149023\n",
      "iteration  20  :    training acc:  0.08597464342313788    training loss:  4.430388008718013\n",
      "iteration  40  :    training acc:  0.08874801901743265    training loss:  4.461570685757578\n",
      "iteration  60  :    training acc:  0.09350237717908082    training loss:  4.655554829308619\n",
      "iteration  80  :    training acc:  0.08874801901743265    training loss:  4.4935288555129205\n",
      "iteration  100  :    training acc:  0.09587955625990491    training loss:  4.497725202354252\n",
      "iteration  120  :    training acc:  0.09350237717908082    training loss:  4.4739722826709\n",
      "iteration  140  :    training acc:  0.09587955625990491    training loss:  4.606356459369752\n",
      "iteration  160  :    training acc:  0.0938985736925515    training loss:  4.715395390680568\n",
      "iteration  180  :    training acc:  0.09350237717908082    training loss:  4.703924528131508\n",
      "iteration  200  :    training acc:  0.09469096671949287    training loss:  4.722105832738764\n",
      "iteration  220  :    training acc:  0.09112519809825674    training loss:  4.565571779493503\n",
      "iteration  240  :    training acc:  0.09706814580031696    training loss:  4.611853184370662\n",
      "iteration  260  :    training acc:  0.09033280507131537    training loss:  4.449327101302326\n",
      "iteration  280  :    training acc:  0.09706814580031696    training loss:  4.511461058427463\n",
      "iteration  300  :    training acc:  0.10182250396196514    training loss:  4.5249106166543465\n",
      "iteration  320  :    training acc:  0.09667194928684628    training loss:  4.731997366355549\n",
      "iteration  340  :    training acc:  0.09508716323296355    training loss:  4.7609217583076155\n",
      "iteration  360  :    training acc:  0.10063391442155309    training loss:  4.518352432315248\n",
      "iteration  380  :    training acc:  0.08993660855784469    training loss:  4.472651218391399\n",
      "iteration  400  :    training acc:  0.09508716323296355    training loss:  4.772863464420917\n",
      "iteration  420  :    training acc:  0.10142630744849446    training loss:  4.522038528566489\n",
      "iteration  440  :    training acc:  0.09548335974643424    training loss:  4.67457158607557\n",
      "iteration  460  :    training acc:  0.09508716323296355    training loss:  4.535683146966332\n",
      "iteration  480  :    training acc:  0.09508716323296355    training loss:  4.723187271901207\n",
      "iteration  500  :    training acc:  0.0938985736925515    training loss:  4.775801109118222\n",
      "iteration  520  :    training acc:  0.0938985736925515    training loss:  4.642205834866553\n",
      "iteration  540  :    training acc:  0.09429477020602219    training loss:  4.525995130663686\n",
      "iteration  560  :    training acc:  0.09231378763866878    training loss:  4.778301273642353\n",
      "iteration  580  :    training acc:  0.09746434231378764    training loss:  4.539605647854778\n",
      "iteration  600  :    training acc:  0.09310618066561015    training loss:  4.738768915853363\n",
      "iteration  620  :    training acc:  0.08795562599049128    training loss:  4.574753476380614\n",
      "iteration  640  :    training acc:  0.10182250396196514    training loss:  4.525158334499955\n",
      "iteration  660  :    training acc:  0.09152139461172741    training loss:  4.772558846810245\n",
      "iteration  680  :    training acc:  0.09984152139461172    training loss:  4.525519382316168\n",
      "iteration  700  :    training acc:  0.09072900158478606    training loss:  4.769863224018577\n",
      "iteration  720  :    training acc:  0.09231378763866878    training loss:  4.787116286639807\n",
      "iteration  740  :    training acc:  0.09231378763866878    training loss:  4.672629140351794\n",
      "iteration  760  :    training acc:  0.08716323296354993    training loss:  4.457144137243697\n",
      "iteration  780  :    training acc:  0.09112519809825674    training loss:  4.52664139315497\n",
      "iteration  800  :    training acc:  0.09152139461172741    training loss:  4.784427160096661\n",
      "iteration  820  :    training acc:  0.09310618066561015    training loss:  4.551607512195943\n",
      "iteration  840  :    training acc:  0.09270998415213946    training loss:  4.526559387761516\n",
      "iteration  860  :    training acc:  0.09112519809825674    training loss:  4.537848156775886\n",
      "iteration  880  :    training acc:  0.08557844690966719    training loss:  4.448409876176623\n",
      "iteration  900  :    training acc:  0.08280507131537243    training loss:  4.558084924011163\n",
      "iteration  920  :    training acc:  0.08835182250396197    training loss:  4.775216315944465\n",
      "iteration  940  :    training acc:  0.08399366085578447    training loss:  4.464311707515726\n",
      "iteration  960  :    training acc:  0.08637083993660856    training loss:  4.53243228683518\n",
      "iteration  980  :    training acc:  0.10063391442155309    training loss:  4.523688595881539\n",
      "iteration  1000  :    training acc:  0.09072900158478606    training loss:  4.535812942980138\n",
      "iteration  1020  :    training acc:  0.08795562599049128    training loss:  4.549945899625212\n",
      "iteration  1040  :    training acc:  0.08874801901743265    training loss:  4.766480551441732\n",
      "iteration  1060  :    training acc:  0.08716323296354993    training loss:  4.431735999936075\n",
      "iteration  1080  :    training acc:  0.089540412044374    training loss:  4.696062359848039\n",
      "iteration  1100  :    training acc:  0.08874801901743265    training loss:  4.63793711217178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1120  :    training acc:  0.09152139461172741    training loss:  4.536921009372598\n",
      "iteration  1140  :    training acc:  0.08914421553090333    training loss:  4.751360789798872\n",
      "iteration  1160  :    training acc:  0.09072900158478606    training loss:  4.5231512211238\n",
      "iteration  1180  :    training acc:  0.089540412044374    training loss:  4.749309832344035\n",
      "iteration  1200  :    training acc:  0.08914421553090333    training loss:  4.706905814726074\n",
      "iteration  1220  :    training acc:  0.08795562599049128    training loss:  4.777541220583201\n",
      "iteration  1240  :    training acc:  0.08874801901743265    training loss:  4.427859470339449\n",
      "iteration  1260  :    training acc:  0.089540412044374    training loss:  4.423067201527807\n",
      "iteration  1280  :    training acc:  0.08716323296354993    training loss:  4.635039794003592\n",
      "iteration  1300  :    training acc:  0.08557844690966719    training loss:  4.447177090340157\n",
      "iteration  1320  :    training acc:  0.08835182250396197    training loss:  4.683503048139814\n",
      "iteration  1340  :    training acc:  0.08835182250396197    training loss:  4.446154124132931\n",
      "iteration  1360  :    training acc:  0.08518225039619651    training loss:  4.7323806006870415\n",
      "iteration  1380  :    training acc:  0.08557844690966719    training loss:  4.775197069105963\n",
      "iteration  1400  :    training acc:  0.08874801901743265    training loss:  4.439339895084433\n",
      "iteration  1420  :    training acc:  0.08716323296354993    training loss:  4.58106674755465\n",
      "iteration  1440  :    training acc:  0.08914421553090333    training loss:  4.787596268985503\n",
      "iteration  1460  :    training acc:  0.08835182250396197    training loss:  4.776770262108219\n",
      "iteration  1480  :    training acc:  0.08914421553090333    training loss:  4.775434222622894\n",
      "iteration  1500  :    training acc:  0.09706814580031696    training loss:  4.523900021004564\n",
      "iteration  1520  :    training acc:  0.08874801901743265    training loss:  4.485952300706326\n",
      "iteration  1540  :    training acc:  0.08716323296354993    training loss:  4.772655529550132\n",
      "iteration  1560  :    training acc:  0.089540412044374    training loss:  4.755481883725842\n",
      "iteration  1580  :    training acc:  0.0875594294770206    training loss:  4.756677030403321\n",
      "iteration  1600  :    training acc:  0.08795562599049128    training loss:  4.487810181207889\n",
      "iteration  1620  :    training acc:  0.08795562599049128    training loss:  4.588617417294045\n",
      "iteration  1640  :    training acc:  0.1026148969889065    training loss:  4.5039975333831\n",
      "iteration  1660  :    training acc:  0.08835182250396197    training loss:  4.705284297028817\n",
      "iteration  1680  :    training acc:  0.08557844690966719    training loss:  4.546097096310576\n",
      "iteration  1700  :    training acc:  0.09627575277337559    training loss:  4.512190544782024\n",
      "iteration  1720  :    training acc:  0.09072900158478606    training loss:  4.603998980089674\n",
      "iteration  1740  :    training acc:  0.09350237717908082    training loss:  4.530970422062961\n",
      "iteration  1760  :    training acc:  0.09310618066561015    training loss:  4.57207988596842\n",
      "iteration  1780  :    training acc:  0.09746434231378764    training loss:  4.574595806784792\n",
      "iteration  1800  :    training acc:  0.09508716323296355    training loss:  4.482218663916233\n",
      "iteration  1820  :    training acc:  0.09429477020602219    training loss:  4.542191874152868\n",
      "iteration  1840  :    training acc:  0.09429477020602219    training loss:  4.484726260010131\n",
      "iteration  1860  :    training acc:  0.09667194928684628    training loss:  4.550349016827651\n",
      "iteration  1880  :    training acc:  0.09508716323296355    training loss:  4.676941082676519\n",
      "iteration  1900  :    training acc:  0.09231378763866878    training loss:  4.4964116248410795\n",
      "iteration  1920  :    training acc:  0.0919175911251981    training loss:  4.4977501911065145\n",
      "iteration  1940  :    training acc:  0.0919175911251981    training loss:  4.482823997174177\n",
      "iteration  1960  :    training acc:  0.09786053882725831    training loss:  4.465474761593027\n",
      "iteration  1980  :    training acc:  0.09865293185419968    training loss:  4.528992375252396\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.10151878497202238    training loss:  17.920497809544734\n",
      "iteration  20  :    training acc:  0.0991207034372502    training loss:  3.770383526642748\n",
      "iteration  40  :    training acc:  0.09832134292565947    training loss:  3.7295772795576267\n",
      "iteration  60  :    training acc:  0.10071942446043165    training loss:  3.714629200489779\n",
      "iteration  80  :    training acc:  0.1035171862509992    training loss:  3.707327703921491\n",
      "iteration  100  :    training acc:  0.10591526778577139    training loss:  3.7037998941600843\n",
      "iteration  120  :    training acc:  0.10671462829736211    training loss:  3.703121424962914\n",
      "iteration  140  :    training acc:  0.10391686650679456    training loss:  3.7048764451071965\n",
      "iteration  160  :    training acc:  0.10271782573940848    training loss:  3.7082516496400326\n",
      "iteration  180  :    training acc:  0.10311750599520383    training loss:  3.71268399510696\n",
      "iteration  200  :    training acc:  0.10191846522781775    training loss:  3.718108318491021\n",
      "iteration  220  :    training acc:  0.1035171862509992    training loss:  3.724415738420286\n",
      "iteration  240  :    training acc:  0.10551558752997602    training loss:  3.73149432228755\n",
      "iteration  260  :    training acc:  0.10991207034372502    training loss:  3.7393628760877187\n",
      "iteration  280  :    training acc:  0.11191047162270183    training loss:  3.7478296468524657\n",
      "iteration  300  :    training acc:  0.11191047162270183    training loss:  3.756571129073527\n",
      "iteration  320  :    training acc:  0.1123101518784972    training loss:  3.7654912117640786\n",
      "iteration  340  :    training acc:  0.1135091926458833    training loss:  3.7746241661132807\n",
      "iteration  360  :    training acc:  0.11270983213429256    training loss:  3.783975786993033\n",
      "iteration  380  :    training acc:  0.1091127098321343    training loss:  3.7934486761614563\n",
      "iteration  400  :    training acc:  0.10871302957633892    training loss:  3.8029398284193525\n",
      "iteration  420  :    training acc:  0.10711430855315747    training loss:  3.812416976690313\n",
      "iteration  440  :    training acc:  0.10551558752997602    training loss:  3.8217815486401467\n",
      "iteration  460  :    training acc:  0.10951239008792965    training loss:  3.8306928274765872\n",
      "iteration  480  :    training acc:  0.1079136690647482    training loss:  3.839100140832386\n",
      "iteration  500  :    training acc:  0.11031175059952038    training loss:  3.8475217856152097\n",
      "iteration  520  :    training acc:  0.10951239008792965    training loss:  3.856539611842505\n",
      "iteration  540  :    training acc:  0.11071143085531575    training loss:  3.8663072249700345\n",
      "iteration  560  :    training acc:  0.10951239008792965    training loss:  3.8759858255035344\n",
      "iteration  580  :    training acc:  0.10871302957633892    training loss:  3.8834071920733386\n",
      "iteration  600  :    training acc:  0.10831334932054357    training loss:  3.8875848733404776\n",
      "iteration  620  :    training acc:  0.11071143085531575    training loss:  3.889460173903431\n",
      "iteration  640  :    training acc:  0.10951239008792965    training loss:  3.8892288928090237\n",
      "iteration  660  :    training acc:  0.1135091926458833    training loss:  3.888051530536313\n",
      "iteration  680  :    training acc:  0.11590727418065548    training loss:  3.8856073034287233\n",
      "iteration  700  :    training acc:  0.11510791366906475    training loss:  3.879398461249126\n",
      "iteration  720  :    training acc:  0.11750599520383694    training loss:  3.868394303021124\n",
      "iteration  740  :    training acc:  0.12310151878497202    training loss:  3.8508632084839256\n",
      "iteration  760  :    training acc:  0.12829736211031176    training loss:  3.829382528448746\n",
      "iteration  780  :    training acc:  0.1278976818545164    training loss:  3.820138532242354\n",
      "iteration  800  :    training acc:  0.1286970423661071    training loss:  3.817992256373683\n",
      "iteration  820  :    training acc:  0.13309352517985612    training loss:  3.8187751032212818\n",
      "iteration  840  :    training acc:  0.13309352517985612    training loss:  3.821217661296191\n",
      "iteration  860  :    training acc:  0.13469224620303757    training loss:  3.8232715121098053\n",
      "iteration  880  :    training acc:  0.13589128697042366    training loss:  3.8220602755984134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  900  :    training acc:  0.1386890487609912    training loss:  3.820744931994769\n",
      "iteration  920  :    training acc:  0.14228617106314947    training loss:  3.819593671279251\n",
      "iteration  940  :    training acc:  0.1398880895283773    training loss:  3.8175837037729226\n",
      "iteration  960  :    training acc:  0.1410871302957634    training loss:  3.8111645187220273\n",
      "iteration  980  :    training acc:  0.1386890487609912    training loss:  3.8025370806792544\n",
      "iteration  1000  :    training acc:  0.1366906474820144    training loss:  3.7863926463861133\n",
      "iteration  1020  :    training acc:  0.1410871302957634    training loss:  3.763493856430608\n",
      "iteration  1040  :    training acc:  0.14388489208633093    training loss:  3.7298924769459507\n",
      "iteration  1060  :    training acc:  0.14588329336530775    training loss:  3.6864695138121952\n",
      "iteration  1080  :    training acc:  0.14268585131894485    training loss:  3.7152330902283723\n",
      "iteration  1100  :    training acc:  0.14908073541167066    training loss:  3.6408841472068763\n",
      "iteration  1120  :    training acc:  0.15667466027178256    training loss:  4.139672549968818\n",
      "iteration  1140  :    training acc:  0.15227817745803357    training loss:  3.7342002089257478\n",
      "iteration  1160  :    training acc:  0.11830535571542766    training loss:  5.461796275910457\n",
      "iteration  1180  :    training acc:  0.16266986410871304    training loss:  3.602503936829975\n",
      "iteration  1200  :    training acc:  0.1718625099920064    training loss:  3.5116180135590045\n",
      "iteration  1220  :    training acc:  0.1750599520383693    training loss:  3.5234282379459327\n",
      "iteration  1240  :    training acc:  0.17945643485211832    training loss:  3.6683087275043005\n",
      "iteration  1260  :    training acc:  0.1510791366906475    training loss:  4.856390393877998\n",
      "iteration  1280  :    training acc:  0.15387689848121502    training loss:  4.245914678155198\n",
      "iteration  1300  :    training acc:  0.15467625899280577    training loss:  4.010145712173472\n",
      "iteration  1320  :    training acc:  0.10831334932054357    training loss:  6.212115043814818\n",
      "iteration  1340  :    training acc:  0.10671462829736211    training loss:  11.158543385351347\n",
      "iteration  1360  :    training acc:  0.16386890487609912    training loss:  4.753425923375903\n",
      "iteration  1380  :    training acc:  0.17026378896882494    training loss:  5.127777651354225\n",
      "iteration  1400  :    training acc:  0.16266986410871304    training loss:  5.183912576405079\n",
      "iteration  1420  :    training acc:  0.16107114308553158    training loss:  6.989541028123205\n",
      "iteration  1440  :    training acc:  0.11910471622701839    training loss:  14.849220470311428\n",
      "iteration  1460  :    training acc:  0.15027977617905675    training loss:  6.561997334612023\n",
      "iteration  1480  :    training acc:  0.12150279776179057    training loss:  5.586571700149195\n",
      "iteration  1500  :    training acc:  0.1642685851318945    training loss:  7.13528705183046\n",
      "iteration  1520  :    training acc:  0.14788169464428458    training loss:  8.119029152705394\n",
      "iteration  1540  :    training acc:  0.13709032773780974    training loss:  inf\n",
      "iteration  1560  :    training acc:  0.14628297362110312    training loss:  12.425447794467397\n",
      "iteration  1580  :    training acc:  0.12430055955235811    training loss:  7.6359954636470535\n",
      "iteration  1600  :    training acc:  0.12310151878497202    training loss:  7.723829426746667\n",
      "iteration  1620  :    training acc:  0.14068745003996802    training loss:  10.20557982771408\n",
      "iteration  1640  :    training acc:  0.1223021582733813    training loss:  nan\n",
      "iteration  1660  :    training acc:  0.10591526778577139    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.12430055955235811    training loss:  nan\n",
      "iteration  1700  :    training acc:  0.1310951239008793    training loss:  nan\n",
      "iteration  1720  :    training acc:  0.10711430855315747    training loss:  inf\n",
      "iteration  1740  :    training acc:  0.14068745003996802    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.11830535571542766    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.15987210231814547    training loss:  25.77830459040623\n",
      "iteration  1800  :    training acc:  0.09752198241406874    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.13149480415667467    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.13629096722621903    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.10511590727418066    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.09512390087929656    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.10191846522781775    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.11550759392486011    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.10671462829736211    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.12629896083133493    training loss:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1980  :    training acc:  0.1286970423661071    training loss:  nan\n",
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.09097918272937548    training loss:  17.965076798496195\n",
      "iteration  20  :    training acc:  0.09290670778720124    training loss:  3.8952615657978344\n",
      "iteration  40  :    training acc:  0.09560524286815729    training loss:  3.8346673529416995\n",
      "iteration  60  :    training acc:  0.09059367771781034    training loss:  4.001510086541444\n",
      "iteration  80  :    training acc:  0.09213569776407093    training loss:  4.025071611902155\n",
      "iteration  100  :    training acc:  0.09290670778720124    training loss:  4.037031642133415\n",
      "iteration  120  :    training acc:  0.09521973785659213    training loss:  4.04392148777949\n",
      "iteration  140  :    training acc:  0.09444872783346184    training loss:  4.047233247509685\n",
      "iteration  160  :    training acc:  0.09521973785659213    training loss:  4.047981930560143\n",
      "iteration  180  :    training acc:  0.09560524286815729    training loss:  4.047101651576735\n",
      "iteration  200  :    training acc:  0.09560524286815729    training loss:  4.045551327018608\n",
      "iteration  220  :    training acc:  0.09637625289128758    training loss:  4.044149810046626\n",
      "iteration  240  :    training acc:  0.09753276792598303    training loss:  4.043388287542801\n",
      "iteration  260  :    training acc:  0.09676175790285274    training loss:  4.043409011830628\n",
      "iteration  280  :    training acc:  0.09560524286815729    training loss:  4.044128306742296\n",
      "iteration  300  :    training acc:  0.09560524286815729    training loss:  4.0453903287909725\n",
      "iteration  320  :    training acc:  0.09637625289128758    training loss:  4.047051871075879\n",
      "iteration  340  :    training acc:  0.09521973785659213    training loss:  4.048993637949718\n",
      "iteration  360  :    training acc:  0.09560524286815729    training loss:  4.0511073124495836\n",
      "iteration  380  :    training acc:  0.09560524286815729    training loss:  4.0532880505022515\n",
      "iteration  400  :    training acc:  0.09560524286815729    training loss:  4.055434700546626\n",
      "iteration  420  :    training acc:  0.09868928296067848    training loss:  4.057452794588873\n",
      "iteration  440  :    training acc:  0.10100231303006939    training loss:  4.059257759707903\n",
      "iteration  460  :    training acc:  0.10100231303006939    training loss:  4.060779715101753\n",
      "iteration  480  :    training acc:  0.10061680801850424    training loss:  4.061974299514095\n",
      "iteration  500  :    training acc:  0.10100231303006939    training loss:  4.062841737806966\n",
      "iteration  520  :    training acc:  0.1017733230531997    training loss:  4.063443180213558\n",
      "iteration  540  :    training acc:  0.10061680801850424    training loss:  4.063894249955171\n",
      "iteration  560  :    training acc:  0.10215882806476484    training loss:  4.064335700093415\n",
      "iteration  580  :    training acc:  0.10215882806476484    training loss:  4.064901026708111\n",
      "iteration  600  :    training acc:  0.10331534309946029    training loss:  4.0656959571366125\n",
      "iteration  620  :    training acc:  0.10447185813415574    training loss:  4.066803701752016\n",
      "iteration  640  :    training acc:  0.10524286815728605    training loss:  4.068313194688335\n",
      "iteration  660  :    training acc:  0.10524286815728605    training loss:  4.070339541297903\n",
      "iteration  680  :    training acc:  0.110254433307633    training loss:  4.073068114567624\n",
      "iteration  700  :    training acc:  0.11141094834232845    training loss:  4.076838726939554\n",
      "iteration  720  :    training acc:  0.11063993831919815    training loss:  4.081908000757963\n",
      "iteration  740  :    training acc:  0.1110254433307633    training loss:  4.088104077770867\n",
      "iteration  760  :    training acc:  0.1125674633770239    training loss:  4.095097119892385\n",
      "iteration  780  :    training acc:  0.1133384734001542    training loss:  4.102765579488486\n",
      "iteration  800  :    training acc:  0.1141094834232845    training loss:  4.1107563471345605\n",
      "iteration  820  :    training acc:  0.1133384734001542    training loss:  4.118400165536631\n",
      "iteration  840  :    training acc:  0.1125674633770239    training loss:  4.125770349953756\n",
      "iteration  860  :    training acc:  0.1133384734001542    training loss:  4.132939080187102\n",
      "iteration  880  :    training acc:  0.11218195836545874    training loss:  4.139574141432015\n",
      "iteration  900  :    training acc:  0.11372397841171936    training loss:  4.145612685518932\n",
      "iteration  920  :    training acc:  0.11218195836545874    training loss:  4.151300091093132\n",
      "iteration  940  :    training acc:  0.11449498843484965    training loss:  4.156414653124116\n",
      "iteration  960  :    training acc:  0.1141094834232845    training loss:  4.160352829366296\n",
      "iteration  980  :    training acc:  0.11295296838858905    training loss:  4.162510900295833\n",
      "iteration  1000  :    training acc:  0.11218195836545874    training loss:  4.162793260168207\n",
      "iteration  1020  :    training acc:  0.1110254433307633    training loss:  4.1621761875319745\n",
      "iteration  1040  :    training acc:  0.11141094834232845    training loss:  4.161060367353903\n",
      "iteration  1060  :    training acc:  0.110254433307633    training loss:  4.159089158802867\n",
      "iteration  1080  :    training acc:  0.10832690824980724    training loss:  4.155937508653398\n",
      "iteration  1100  :    training acc:  0.10832690824980724    training loss:  4.151139996109967\n",
      "iteration  1120  :    training acc:  0.1117964533538936    training loss:  4.144225217536267\n",
      "iteration  1140  :    training acc:  0.10909791827293755    training loss:  4.136036119665913\n",
      "iteration  1160  :    training acc:  0.110254433307633    training loss:  4.126428561933738\n",
      "iteration  1180  :    training acc:  0.10755589822667695    training loss:  4.115125230040969\n",
      "iteration  1200  :    training acc:  0.1094834232845027    training loss:  4.1011701672642\n",
      "iteration  1220  :    training acc:  0.10755589822667695    training loss:  4.084970734181892\n",
      "iteration  1240  :    training acc:  0.11141094834232845    training loss:  4.067494647900728\n",
      "iteration  1260  :    training acc:  0.11141094834232845    training loss:  4.047836818835746\n",
      "iteration  1280  :    training acc:  0.11218195836545874    training loss:  4.027943341595081\n",
      "iteration  1300  :    training acc:  0.11449498843484965    training loss:  4.005921105792814\n",
      "iteration  1320  :    training acc:  0.11912104857363146    training loss:  3.9814849779080754\n",
      "iteration  1340  :    training acc:  0.12220508866615266    training loss:  3.956647232224852\n",
      "iteration  1360  :    training acc:  0.12336160370084812    training loss:  3.932154042085298\n",
      "iteration  1380  :    training acc:  0.12259059367771781    training loss:  3.9061207217681653\n",
      "iteration  1400  :    training acc:  0.1202775636083269    training loss:  3.879086450181618\n",
      "iteration  1420  :    training acc:  0.11912104857363146    training loss:  3.8531615560825014\n",
      "iteration  1440  :    training acc:  0.11449498843484965    training loss:  3.829328604468552\n",
      "iteration  1460  :    training acc:  0.11719352351580571    training loss:  3.8084378723029424\n",
      "iteration  1480  :    training acc:  0.1164225134926754    training loss:  3.786173970374023\n",
      "iteration  1500  :    training acc:  0.11757902852737086    training loss:  3.764563469838263\n",
      "iteration  1520  :    training acc:  0.12143407864302236    training loss:  3.7408959286635457\n",
      "iteration  1540  :    training acc:  0.12336160370084812    training loss:  3.6999833928437904\n",
      "iteration  1560  :    training acc:  0.12875867386276021    training loss:  3.6544574773317433\n",
      "iteration  1580  :    training acc:  0.12567463377023902    training loss:  3.67497661755174\n",
      "iteration  1600  :    training acc:  0.12259059367771781    training loss:  4.213261415475699\n",
      "iteration  1620  :    training acc:  0.1272166538164996    training loss:  4.113442613661298\n",
      "iteration  1640  :    training acc:  0.13145720894371626    training loss:  4.0593390745576645\n",
      "iteration  1660  :    training acc:  0.13723978411719354    training loss:  4.039387843170424\n",
      "iteration  1680  :    training acc:  0.13839629915188897    training loss:  4.016218660164461\n",
      "iteration  1700  :    training acc:  0.13762528912875868    training loss:  3.9821522664321436\n",
      "iteration  1720  :    training acc:  0.13569776407093292    training loss:  3.9512563501157953\n",
      "iteration  1740  :    training acc:  0.13646877409406322    training loss:  3.929406162689409\n",
      "iteration  1760  :    training acc:  0.13492675404780263    training loss:  3.8104149691709694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1780  :    training acc:  0.13762528912875868    training loss:  3.5034059741899988\n",
      "iteration  1800  :    training acc:  0.15381649961449498    training loss:  3.4578646196448277\n",
      "iteration  1820  :    training acc:  0.12143407864302236    training loss:  5.698936515862685\n",
      "iteration  1840  :    training acc:  0.13338473400154202    training loss:  3.612833314129596\n",
      "iteration  1860  :    training acc:  0.169236700077101    training loss:  3.3486525365413184\n",
      "iteration  1880  :    training acc:  0.12413261372397841    training loss:  5.239118415060345\n",
      "iteration  1900  :    training acc:  0.1653816499614495    training loss:  3.401881884325291\n",
      "iteration  1920  :    training acc:  0.14957594448727835    training loss:  3.788943994747197\n",
      "iteration  1940  :    training acc:  0.15959907478797225    training loss:  3.612037478902552\n",
      "iteration  1960  :    training acc:  0.11603700848111026    training loss:  6.543252226228051\n",
      "iteration  1980  :    training acc:  0.15343099460292983    training loss:  3.5071570289263514\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.08303956130043087    training loss:  17.887760675475924\n",
      "iteration  20  :    training acc:  0.09244026635330983    training loss:  3.922658880194492\n",
      "iteration  40  :    training acc:  0.09596553074813945    training loss:  3.859851591176536\n",
      "iteration  60  :    training acc:  0.09831570701135918    training loss:  3.829338455363658\n",
      "iteration  80  :    training acc:  0.10144927536231885    training loss:  3.8115966282565203\n",
      "iteration  100  :    training acc:  0.10301605953779867    training loss:  3.803993454379276\n",
      "iteration  120  :    training acc:  0.09439874657265962    training loss:  3.9825661216437904\n",
      "iteration  140  :    training acc:  0.09674892283587935    training loss:  3.993346601364868\n",
      "iteration  160  :    training acc:  0.09753231492361927    training loss:  3.9938451994567403\n",
      "iteration  180  :    training acc:  0.09831570701135918    training loss:  3.992860432343748\n",
      "iteration  200  :    training acc:  0.09949079514296906    training loss:  3.990884926262719\n",
      "iteration  220  :    training acc:  0.099882491186839    training loss:  3.9882606104540095\n",
      "iteration  240  :    training acc:  0.09870740305522914    training loss:  3.985219276613594\n",
      "iteration  260  :    training acc:  0.0963572267920094    training loss:  3.9819109457238793\n",
      "iteration  280  :    training acc:  0.09831570701135918    training loss:  3.9784282005699687\n",
      "iteration  300  :    training acc:  0.0963572267920094    training loss:  3.974825901526999\n",
      "iteration  320  :    training acc:  0.09557383470426949    training loss:  3.9711351936744252\n",
      "iteration  340  :    training acc:  0.0963572267920094    training loss:  3.967371771928458\n",
      "iteration  360  :    training acc:  0.09596553074813945    training loss:  3.963538732428169\n",
      "iteration  380  :    training acc:  0.0963572267920094    training loss:  3.9596256163244115\n",
      "iteration  400  :    training acc:  0.09674892283587935    training loss:  3.955608203246684\n",
      "iteration  420  :    training acc:  0.09792401096748923    training loss:  3.9514569760623677\n",
      "iteration  440  :    training acc:  0.0963572267920094    training loss:  3.9471620959472453\n",
      "iteration  460  :    training acc:  0.09439874657265962    training loss:  3.9427783123916775\n",
      "iteration  480  :    training acc:  0.09596553074813945    training loss:  3.9384826562712845\n",
      "iteration  500  :    training acc:  0.09479044261652958    training loss:  3.934571582468293\n",
      "iteration  520  :    training acc:  0.09557383470426949    training loss:  3.9313228917957947\n",
      "iteration  540  :    training acc:  0.09479044261652958    training loss:  3.9288613602117772\n",
      "iteration  560  :    training acc:  0.09596553074813945    training loss:  3.92716532127328\n",
      "iteration  580  :    training acc:  0.09870740305522914    training loss:  3.926151900684229\n",
      "iteration  600  :    training acc:  0.09714061887974931    training loss:  3.9257242384045417\n",
      "iteration  620  :    training acc:  0.0990990990990991    training loss:  3.925760070053529\n",
      "iteration  640  :    training acc:  0.10027418723070897    training loss:  3.926119548203584\n",
      "iteration  660  :    training acc:  0.10066588327457893    training loss:  3.9267063258542088\n",
      "iteration  680  :    training acc:  0.0990990990990991    training loss:  3.92754028356498\n",
      "iteration  700  :    training acc:  0.09792401096748923    training loss:  3.9287781933645745\n",
      "iteration  720  :    training acc:  0.10066588327457893    training loss:  3.9305936264916475\n",
      "iteration  740  :    training acc:  0.1018409714061888    training loss:  3.932974616959348\n",
      "iteration  760  :    training acc:  0.1018409714061888    training loss:  3.935743454436411\n",
      "iteration  780  :    training acc:  0.099882491186839    training loss:  3.938794293421596\n",
      "iteration  800  :    training acc:  0.10027418723070897    training loss:  3.9422417795739553\n",
      "iteration  820  :    training acc:  0.10027418723070897    training loss:  3.946288361675677\n",
      "iteration  840  :    training acc:  0.10301605953779867    training loss:  3.9510225990926195\n",
      "iteration  860  :    training acc:  0.10379945162553858    training loss:  3.95640356852805\n",
      "iteration  880  :    training acc:  0.10379945162553858    training loss:  3.962318310061196\n",
      "iteration  900  :    training acc:  0.1053662358010184    training loss:  3.968634376406826\n",
      "iteration  920  :    training acc:  0.10497453975714845    training loss:  3.9752724600235343\n",
      "iteration  940  :    training acc:  0.10654132393262829    training loss:  3.982223382774945\n",
      "iteration  960  :    training acc:  0.1073247160203682    training loss:  3.9894761870082065\n",
      "iteration  980  :    training acc:  0.1073247160203682    training loss:  3.996963171788552\n",
      "iteration  1000  :    training acc:  0.1073247160203682    training loss:  4.004594883829567\n",
      "iteration  1020  :    training acc:  0.10575793184488837    training loss:  4.012324315698107\n",
      "iteration  1040  :    training acc:  0.1053662358010184    training loss:  4.020159532285295\n",
      "iteration  1060  :    training acc:  0.10497453975714845    training loss:  4.028094085858967\n",
      "iteration  1080  :    training acc:  0.1045828437132785    training loss:  4.036031168214405\n",
      "iteration  1100  :    training acc:  0.10379945162553858    training loss:  4.043783861131571\n",
      "iteration  1120  :    training acc:  0.10379945162553858    training loss:  4.051082047309018\n",
      "iteration  1140  :    training acc:  0.10301605953779867    training loss:  4.0575191572152995\n",
      "iteration  1160  :    training acc:  0.1053662358010184    training loss:  4.062666196644694\n",
      "iteration  1180  :    training acc:  0.1045828437132785    training loss:  4.06626396242865\n",
      "iteration  1200  :    training acc:  0.1053662358010184    training loss:  4.068522067104705\n",
      "iteration  1220  :    training acc:  0.10419114766940854    training loss:  4.070229628205355\n",
      "iteration  1240  :    training acc:  0.10771641206423815    training loss:  4.071885784017892\n",
      "iteration  1260  :    training acc:  0.10771641206423815    training loss:  4.074733251734948\n",
      "iteration  1280  :    training acc:  0.11045828437132785    training loss:  4.0802420369630195\n",
      "iteration  1300  :    training acc:  0.11006658832745789    training loss:  4.089311052871426\n",
      "iteration  1320  :    training acc:  0.1128084606345476    training loss:  4.100598436617384\n",
      "iteration  1340  :    training acc:  0.11398354876615746    training loss:  4.112379834207998\n",
      "iteration  1360  :    training acc:  0.1135918527222875    training loss:  4.124702346353576\n",
      "iteration  1380  :    training acc:  0.11124167645906777    training loss:  4.137433570232833\n",
      "iteration  1400  :    training acc:  0.11163337250293771    training loss:  4.149763440134671\n",
      "iteration  1420  :    training acc:  0.11163337250293771    training loss:  4.161290557019432\n",
      "iteration  1440  :    training acc:  0.11124167645906777    training loss:  4.17277401787847\n",
      "iteration  1460  :    training acc:  0.10889150019584802    training loss:  4.185298879637498\n",
      "iteration  1480  :    training acc:  0.11006658832745789    training loss:  4.198518872021618\n",
      "iteration  1500  :    training acc:  0.10889150019584802    training loss:  4.210901322094504\n",
      "iteration  1520  :    training acc:  0.10928319623971798    training loss:  4.221140857134257\n",
      "iteration  1540  :    training acc:  0.11084998041519781    training loss:  4.228784744852593\n",
      "iteration  1560  :    training acc:  0.11124167645906777    training loss:  4.234353281858998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1580  :    training acc:  0.11241676459067763    training loss:  4.238195164311597\n",
      "iteration  1600  :    training acc:  0.1128084606345476    training loss:  4.241274842828908\n",
      "iteration  1620  :    training acc:  0.11515863689776733    training loss:  4.2404208340320935\n",
      "iteration  1640  :    training acc:  0.11985898942420682    training loss:  4.236290045439922\n",
      "iteration  1660  :    training acc:  0.12534273403838622    training loss:  4.233373347152272\n",
      "iteration  1680  :    training acc:  0.12808460634547592    training loss:  4.228404682281067\n",
      "iteration  1700  :    training acc:  0.12769291030160596    training loss:  4.217516544696149\n",
      "iteration  1720  :    training acc:  0.1300430865648257    training loss:  4.199548212414064\n",
      "iteration  1740  :    training acc:  0.1335683509596553    training loss:  4.176641431745293\n",
      "iteration  1760  :    training acc:  0.13474343909126518    training loss:  4.146446387253651\n",
      "iteration  1780  :    training acc:  0.13748531139835488    training loss:  4.109614623737168\n",
      "iteration  1800  :    training acc:  0.13748531139835488    training loss:  4.079673075624501\n",
      "iteration  1820  :    training acc:  0.14022718370544457    training loss:  4.126747140559279\n",
      "iteration  1840  :    training acc:  0.14610262436349392    training loss:  4.265379625653798\n",
      "iteration  1860  :    training acc:  0.14688601645123384    training loss:  4.3101585836519645\n",
      "iteration  1880  :    training acc:  0.14610262436349392    training loss:  4.3381446276610385\n",
      "iteration  1900  :    training acc:  0.14845280062671368    training loss:  4.365673464598463\n",
      "iteration  1920  :    training acc:  0.1535448491970231    training loss:  4.3805925681695275\n",
      "iteration  1940  :    training acc:  0.15432824128476302    training loss:  4.397258207953957\n",
      "iteration  1960  :    training acc:  0.1519780650215433    training loss:  4.207844012023332\n",
      "iteration  1980  :    training acc:  0.10889150019584802    training loss:  5.283767652632601\n",
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.09326625386996903    training loss:  18.95653948779365\n",
      "iteration  20  :    training acc:  0.10758513931888544    training loss:  4.848458404317448\n",
      "iteration  40  :    training acc:  0.10913312693498452    training loss:  4.688872508591753\n",
      "iteration  60  :    training acc:  0.10681114551083591    training loss:  4.938230233471292\n",
      "iteration  80  :    training acc:  0.10797213622291021    training loss:  4.677437697016242\n",
      "iteration  100  :    training acc:  0.10835913312693499    training loss:  4.573008211775661\n",
      "iteration  120  :    training acc:  0.10797213622291021    training loss:  4.644373272928758\n",
      "iteration  140  :    training acc:  0.10797213622291021    training loss:  4.70139525942653\n",
      "iteration  160  :    training acc:  0.10758513931888544    training loss:  4.722905957068423\n",
      "iteration  180  :    training acc:  0.10797213622291021    training loss:  4.623026857722482\n",
      "iteration  200  :    training acc:  0.11222910216718267    training loss:  4.665896537771535\n",
      "iteration  220  :    training acc:  0.1153250773993808    training loss:  4.5821188704610725\n",
      "iteration  240  :    training acc:  0.11764705882352941    training loss:  4.629603283690633\n",
      "iteration  260  :    training acc:  0.11493808049535603    training loss:  4.561758115020853\n",
      "iteration  280  :    training acc:  0.11493808049535603    training loss:  4.6279105529607385\n",
      "iteration  300  :    training acc:  0.1141640866873065    training loss:  4.538691571325152\n",
      "iteration  320  :    training acc:  0.11687306501547988    training loss:  4.62895109447383\n",
      "iteration  340  :    training acc:  0.11571207430340558    training loss:  4.532448246582908\n",
      "iteration  360  :    training acc:  0.11919504643962849    training loss:  4.615056340419589\n",
      "iteration  380  :    training acc:  0.11919504643962849    training loss:  4.530587023206375\n",
      "iteration  400  :    training acc:  0.12190402476780186    training loss:  4.5895110123556595\n",
      "iteration  420  :    training acc:  0.12654798761609906    training loss:  4.5468962118168825\n",
      "iteration  440  :    training acc:  0.12886996904024767    training loss:  4.628401339700534\n",
      "iteration  460  :    training acc:  0.12693498452012383    training loss:  4.568371216171939\n",
      "iteration  480  :    training acc:  0.12577399380804954    training loss:  4.626738022352207\n",
      "iteration  500  :    training acc:  0.12461300309597523    training loss:  4.6034802262233585\n",
      "iteration  520  :    training acc:  0.1238390092879257    training loss:  4.611044728438421\n",
      "iteration  540  :    training acc:  0.12422600619195047    training loss:  4.604135138451517\n",
      "iteration  560  :    training acc:  0.12461300309597523    training loss:  4.585865833222491\n",
      "iteration  580  :    training acc:  0.12848297213622292    training loss:  4.567450557222983\n",
      "iteration  600  :    training acc:  0.12809597523219815    training loss:  4.566848788863233\n",
      "iteration  620  :    training acc:  0.13815789473684212    training loss:  4.5953144474820675\n",
      "iteration  640  :    training acc:  0.13738390092879257    training loss:  4.590694151794128\n",
      "iteration  660  :    training acc:  0.13815789473684212    training loss:  4.5927622995901745\n",
      "iteration  680  :    training acc:  0.14202786377708979    training loss:  4.492219133014477\n",
      "iteration  700  :    training acc:  0.16292569659442724    training loss:  3.8801336949653003\n",
      "iteration  720  :    training acc:  0.15905572755417957    training loss:  4.357168423155352\n",
      "iteration  740  :    training acc:  0.13196594427244582    training loss:  6.4659727494683255\n",
      "iteration  760  :    training acc:  0.168343653250774    training loss:  3.9913680829928198\n",
      "iteration  780  :    training acc:  0.09945820433436532    training loss:  7.5748771258593655\n",
      "iteration  800  :    training acc:  0.16718266253869968    training loss:  5.597015813898299\n",
      "iteration  820  :    training acc:  0.16292569659442724    training loss:  4.4103917996312125\n",
      "iteration  840  :    training acc:  0.10487616099071208    training loss:  7.3662894903711615\n",
      "iteration  860  :    training acc:  0.16137770897832818    training loss:  4.348431579293551\n",
      "iteration  880  :    training acc:  0.16292569659442724    training loss:  4.1713501052765976\n",
      "iteration  900  :    training acc:  0.14938080495356038    training loss:  4.52787802301242\n",
      "iteration  920  :    training acc:  0.0998452012383901    training loss:  9.285002166028839\n",
      "iteration  940  :    training acc:  0.1021671826625387    training loss:  9.927432789538258\n",
      "iteration  960  :    training acc:  0.16369969040247678    training loss:  6.397935963173008\n",
      "iteration  980  :    training acc:  0.12925696594427244    training loss:  6.390876414187898\n",
      "iteration  1000  :    training acc:  0.14473684210526316    training loss:  4.876593253416203\n",
      "iteration  1020  :    training acc:  0.18343653250773995    training loss:  4.490712216363695\n",
      "iteration  1040  :    training acc:  0.19349845201238391    training loss:  4.26243215475184\n",
      "iteration  1060  :    training acc:  0.15247678018575853    training loss:  6.440839761534196\n",
      "iteration  1080  :    training acc:  0.15441176470588236    training loss:  9.647004118832765\n",
      "iteration  1100  :    training acc:  0.15054179566563466    training loss:  7.916659622866424\n",
      "iteration  1120  :    training acc:  0.12229102167182662    training loss:  8.012204566265567\n",
      "iteration  1140  :    training acc:  0.12113003095975232    training loss:  6.565770169597582\n",
      "iteration  1160  :    training acc:  0.13970588235294118    training loss:  6.223277084160083\n",
      "iteration  1180  :    training acc:  0.14512383900928794    training loss:  6.9488244069481375\n",
      "iteration  1200  :    training acc:  0.14628482972136223    training loss:  8.12234000203082\n",
      "iteration  1220  :    training acc:  0.16060371517027863    training loss:  6.452869551407508\n",
      "iteration  1240  :    training acc:  0.15170278637770898    training loss:  8.084442670758738\n",
      "iteration  1260  :    training acc:  0.15286377708978327    training loss:  6.526919004566144\n",
      "iteration  1280  :    training acc:  0.17453560371517027    training loss:  inf\n",
      "iteration  1300  :    training acc:  0.1296439628482972    training loss:  inf\n",
      "iteration  1320  :    training acc:  0.13080495356037153    training loss:  10.0580007621731\n",
      "iteration  1340  :    training acc:  0.11958204334365326    training loss:  15.093689578851244\n",
      "iteration  1360  :    training acc:  0.15479876160990713    training loss:  inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1380  :    training acc:  0.16253869969040247    training loss:  8.211529125582258\n",
      "iteration  1400  :    training acc:  0.1443498452012384    training loss:  inf\n",
      "iteration  1420  :    training acc:  0.16060371517027863    training loss:  nan\n",
      "iteration  1440  :    training acc:  0.1513157894736842    training loss:  nan\n",
      "iteration  1460  :    training acc:  0.12113003095975232    training loss:  13.902689732406383\n",
      "iteration  1480  :    training acc:  0.1660216718266254    training loss:  10.230826106535812\n",
      "iteration  1500  :    training acc:  0.10565015479876161    training loss:  nan\n",
      "iteration  1520  :    training acc:  0.13544891640866874    training loss:  inf\n",
      "iteration  1540  :    training acc:  0.15441176470588236    training loss:  nan\n",
      "iteration  1560  :    training acc:  0.16215170278637772    training loss:  nan\n",
      "iteration  1580  :    training acc:  0.11996904024767802    training loss:  17.556043315522142\n",
      "iteration  1600  :    training acc:  0.11571207430340558    training loss:  nan\n",
      "iteration  1620  :    training acc:  0.12886996904024767    training loss:  26.667648080004547\n",
      "iteration  1640  :    training acc:  0.17879256965944273    training loss:  nan\n",
      "iteration  1660  :    training acc:  0.14047987616099072    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.15944272445820434    training loss:  nan\n",
      "iteration  1700  :    training acc:  0.1513157894736842    training loss:  nan\n",
      "iteration  1720  :    training acc:  0.13583591331269348    training loss:  nan\n",
      "iteration  1740  :    training acc:  0.146671826625387    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.13893188854489164    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.15441176470588236    training loss:  nan\n",
      "iteration  1800  :    training acc:  0.18150154798761609    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.15673374613003097    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.12461300309597523    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.12229102167182662    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.14551083591331268    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.17956656346749225    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.1141640866873065    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.14357585139318885    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.10526315789473684    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.14357585139318885    training loss:  nan\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.10256410256410256    training loss:  18.725280924421448\n",
      "iteration  20  :    training acc:  0.10178710178710179    training loss:  4.090533387821845\n",
      "iteration  40  :    training acc:  0.11188811188811189    training loss:  4.287370530616517\n",
      "iteration  60  :    training acc:  0.11383061383061382    training loss:  4.367820759476065\n",
      "iteration  80  :    training acc:  0.11266511266511267    training loss:  4.268475423538198\n",
      "iteration  100  :    training acc:  0.11072261072261072    training loss:  4.138804813640188\n",
      "iteration  120  :    training acc:  0.10955710955710955    training loss:  4.085880429637269\n",
      "iteration  140  :    training acc:  0.1114996114996115    training loss:  4.09373544243817\n",
      "iteration  160  :    training acc:  0.11033411033411034    training loss:  4.057738135027702\n",
      "iteration  180  :    training acc:  0.11227661227661227    training loss:  4.067590703035762\n",
      "iteration  200  :    training acc:  0.10994560994560995    training loss:  4.027948528507607\n",
      "iteration  220  :    training acc:  0.11538461538461539    training loss:  4.054500305640919\n",
      "iteration  240  :    training acc:  0.11383061383061382    training loss:  4.0211608950021525\n",
      "iteration  260  :    training acc:  0.11732711732711733    training loss:  4.044406109765784\n",
      "iteration  280  :    training acc:  0.11655011655011654    training loss:  4.026697769027524\n",
      "iteration  300  :    training acc:  0.11693861693861694    training loss:  4.041549724877838\n",
      "iteration  320  :    training acc:  0.11616161616161616    training loss:  4.037136894820324\n",
      "iteration  340  :    training acc:  0.11693861693861694    training loss:  4.0449700426856054\n",
      "iteration  360  :    training acc:  0.11616161616161616    training loss:  4.047844573692557\n",
      "iteration  380  :    training acc:  0.11888111888111888    training loss:  4.053876137248849\n",
      "iteration  400  :    training acc:  0.12082362082362082    training loss:  4.059719152858727\n",
      "iteration  420  :    training acc:  0.12315462315462315    training loss:  4.066881101694708\n",
      "iteration  440  :    training acc:  0.1216006216006216    training loss:  4.075269137425375\n",
      "iteration  460  :    training acc:  0.12082362082362082    training loss:  4.085079444437745\n",
      "iteration  480  :    training acc:  0.1216006216006216    training loss:  4.09613946915259\n",
      "iteration  500  :    training acc:  0.12237762237762238    training loss:  4.1082220958334075\n",
      "iteration  520  :    training acc:  0.12276612276612277    training loss:  4.120762164513807\n",
      "iteration  540  :    training acc:  0.12432012432012432    training loss:  4.133800988571343\n",
      "iteration  560  :    training acc:  0.12626262626262627    training loss:  4.147321585759585\n",
      "iteration  580  :    training acc:  0.12626262626262627    training loss:  4.162445312825881\n",
      "iteration  600  :    training acc:  0.128982128982129    training loss:  4.182504025201842\n",
      "iteration  620  :    training acc:  0.1285936285936286    training loss:  4.210865249040179\n",
      "iteration  640  :    training acc:  0.1317016317016317    training loss:  4.25117335968637\n",
      "iteration  660  :    training acc:  0.12703962703962704    training loss:  4.289653586000596\n",
      "iteration  680  :    training acc:  0.12703962703962704    training loss:  4.320001712590393\n",
      "iteration  700  :    training acc:  0.12975912975912976    training loss:  4.349670921371283\n",
      "iteration  720  :    training acc:  0.12937062937062938    training loss:  4.380210417635084\n",
      "iteration  740  :    training acc:  0.12432012432012432    training loss:  4.410199061407499\n",
      "iteration  760  :    training acc:  0.12548562548562547    training loss:  4.4464887727370055\n",
      "iteration  780  :    training acc:  0.14646464646464646    training loss:  3.8420516702773453\n",
      "iteration  800  :    training acc:  0.12393162393162394    training loss:  3.8610425527779766\n",
      "iteration  820  :    training acc:  0.11732711732711733    training loss:  4.355910993725013\n",
      "iteration  840  :    training acc:  0.1282051282051282    training loss:  4.226525231769068\n",
      "iteration  860  :    training acc:  0.11810411810411811    training loss:  4.66358890430486\n",
      "iteration  880  :    training acc:  0.12276612276612277    training loss:  4.03644576563776\n",
      "iteration  900  :    training acc:  0.13247863247863248    training loss:  4.472003054837104\n",
      "iteration  920  :    training acc:  0.13442113442113443    training loss:  4.430197176880712\n",
      "iteration  940  :    training acc:  0.13131313131313133    training loss:  4.544734867572733\n",
      "iteration  960  :    training acc:  0.13325563325563325    training loss:  4.498420199560166\n",
      "iteration  980  :    training acc:  0.13442113442113443    training loss:  4.334020457153982\n",
      "iteration  1000  :    training acc:  0.1351981351981352    training loss:  4.364850094108682\n",
      "iteration  1020  :    training acc:  0.1351981351981352    training loss:  4.420770581515336\n",
      "iteration  1040  :    training acc:  0.13675213675213677    training loss:  4.332556363067031\n",
      "iteration  1060  :    training acc:  0.13286713286713286    training loss:  4.654190175679926\n",
      "iteration  1080  :    training acc:  0.16744366744366745    training loss:  3.7656360220691325\n",
      "iteration  1100  :    training acc:  0.17560217560217561    training loss:  3.66540908266429\n",
      "iteration  1120  :    training acc:  0.16083916083916083    training loss:  3.6122270354487362\n",
      "iteration  1140  :    training acc:  0.1717171717171717    training loss:  3.593574788888139\n",
      "iteration  1160  :    training acc:  0.14763014763014762    training loss:  4.077122045210089\n",
      "iteration  1180  :    training acc:  0.1414141414141414    training loss:  4.367494223453072\n",
      "iteration  1200  :    training acc:  0.19036519036519037    training loss:  3.5858003951839987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1220  :    training acc:  0.1522921522921523    training loss:  4.12528417789299\n",
      "iteration  1240  :    training acc:  0.15345765345765347    training loss:  4.031063687577988\n",
      "iteration  1260  :    training acc:  0.17094017094017094    training loss:  3.5701479184422973\n",
      "iteration  1280  :    training acc:  0.16355866355866355    training loss:  4.101574807021459\n",
      "iteration  1300  :    training acc:  0.19347319347319347    training loss:  3.827607486729017\n",
      "iteration  1320  :    training acc:  0.15734265734265734    training loss:  4.061948671613817\n",
      "iteration  1340  :    training acc:  0.12703962703962704    training loss:  4.641184112124097\n",
      "iteration  1360  :    training acc:  0.15345765345765347    training loss:  3.9380295806204315\n",
      "iteration  1380  :    training acc:  0.17987567987567987    training loss:  3.857657889146934\n",
      "iteration  1400  :    training acc:  0.12354312354312354    training loss:  4.404655744687232\n",
      "iteration  1420  :    training acc:  0.20085470085470086    training loss:  3.5638002164827434\n",
      "iteration  1440  :    training acc:  0.17637917637917638    training loss:  3.854418494540542\n",
      "iteration  1460  :    training acc:  0.13442113442113443    training loss:  3.8619456337150644\n",
      "iteration  1480  :    training acc:  0.14763014763014762    training loss:  3.8881718925837427\n",
      "iteration  1500  :    training acc:  0.15656565656565657    training loss:  3.8097289622131982\n",
      "iteration  1520  :    training acc:  0.18337218337218336    training loss:  3.7122465919596186\n",
      "iteration  1540  :    training acc:  0.17560217560217561    training loss:  3.709158553439105\n",
      "iteration  1560  :    training acc:  0.19463869463869463    training loss:  3.69374415920263\n",
      "iteration  1580  :    training acc:  0.20046620046620048    training loss:  3.66080079510079\n",
      "iteration  1600  :    training acc:  0.19153069153069152    training loss:  3.564507094492345\n",
      "iteration  1620  :    training acc:  0.19774669774669776    training loss:  3.5967798661711106\n",
      "iteration  1640  :    training acc:  0.19347319347319347    training loss:  3.5891720438710757\n",
      "iteration  1660  :    training acc:  0.1958041958041958    training loss:  3.6157757004251243\n",
      "iteration  1680  :    training acc:  0.19425019425019424    training loss:  3.5510973577345415\n",
      "iteration  1700  :    training acc:  0.1961926961926962    training loss:  3.6049962759801413\n",
      "iteration  1720  :    training acc:  0.18919968919968919    training loss:  3.6401088086943667\n",
      "iteration  1740  :    training acc:  0.20202020202020202    training loss:  3.5717864344358508\n",
      "iteration  1760  :    training acc:  0.19696969696969696    training loss:  3.5871799983756154\n",
      "iteration  1780  :    training acc:  0.20473970473970474    training loss:  3.532510541371382\n",
      "iteration  1800  :    training acc:  0.19347319347319347    training loss:  3.6083896375961433\n",
      "iteration  1820  :    training acc:  0.19230769230769232    training loss:  3.5862053912405276\n",
      "iteration  1840  :    training acc:  0.19036519036519037    training loss:  3.574658702457488\n",
      "iteration  1860  :    training acc:  0.19114219114219114    training loss:  3.5607327173868186\n",
      "iteration  1880  :    training acc:  0.1930846930846931    training loss:  3.540946462054475\n",
      "iteration  1900  :    training acc:  0.1919191919191919    training loss:  3.53771559558106\n",
      "iteration  1920  :    training acc:  0.19463869463869463    training loss:  3.517368072811952\n",
      "iteration  1940  :    training acc:  0.1926961926961927    training loss:  3.512188073249534\n",
      "iteration  1960  :    training acc:  0.19153069153069152    training loss:  3.5017629621850777\n",
      "iteration  1980  :    training acc:  0.19075369075369075    training loss:  3.4965133671976383\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.0989010989010989    training loss:  16.770370378067167\n",
      "iteration  20  :    training acc:  0.09076109076109076    training loss:  4.529870398303113\n",
      "iteration  40  :    training acc:  0.09116809116809117    training loss:  4.887199239782808\n",
      "iteration  60  :    training acc:  0.0927960927960928    training loss:  4.893927564804499\n",
      "iteration  80  :    training acc:  0.09320309320309321    training loss:  4.933355543220776\n",
      "iteration  100  :    training acc:  0.09238909238909239    training loss:  4.953075747794671\n",
      "iteration  120  :    training acc:  0.09442409442409443    training loss:  4.965903826345161\n",
      "iteration  140  :    training acc:  0.0936100936100936    training loss:  4.967922111679941\n",
      "iteration  160  :    training acc:  0.0927960927960928    training loss:  4.965297593390852\n",
      "iteration  180  :    training acc:  0.09686609686609686    training loss:  4.960774361054255\n",
      "iteration  200  :    training acc:  0.09605209605209605    training loss:  4.959101936442471\n",
      "iteration  220  :    training acc:  0.10052910052910052    training loss:  4.960013248154158\n",
      "iteration  240  :    training acc:  0.0993080993080993    training loss:  4.961141419228041\n",
      "iteration  260  :    training acc:  0.09686609686609686    training loss:  4.960630195279984\n",
      "iteration  280  :    training acc:  0.09686609686609686    training loss:  4.956891862931857\n",
      "iteration  300  :    training acc:  0.09686609686609686    training loss:  4.949875203958481\n",
      "iteration  320  :    training acc:  0.09686609686609686    training loss:  4.94122696533177\n",
      "iteration  340  :    training acc:  0.09686609686609686    training loss:  4.933071687663695\n",
      "iteration  360  :    training acc:  0.09645909645909646    training loss:  4.926640278224316\n",
      "iteration  380  :    training acc:  0.09727309727309727    training loss:  4.921584825998096\n",
      "iteration  400  :    training acc:  0.09645909645909646    training loss:  4.917151442493127\n",
      "iteration  420  :    training acc:  0.09605209605209605    training loss:  4.913069588886358\n",
      "iteration  440  :    training acc:  0.09808709808709809    training loss:  4.909389074061191\n",
      "iteration  460  :    training acc:  0.0989010989010989    training loss:  4.906264785209229\n",
      "iteration  480  :    training acc:  0.0989010989010989    training loss:  4.903878955644786\n",
      "iteration  500  :    training acc:  0.09686609686609686    training loss:  4.90220241806056\n",
      "iteration  520  :    training acc:  0.0993080993080993    training loss:  4.900986103029278\n",
      "iteration  540  :    training acc:  0.09645909645909646    training loss:  4.900185169808951\n",
      "iteration  560  :    training acc:  0.09564509564509564    training loss:  4.899792993636426\n",
      "iteration  580  :    training acc:  0.09401709401709402    training loss:  4.899484010708981\n",
      "iteration  600  :    training acc:  0.09198209198209198    training loss:  4.898701417291703\n",
      "iteration  620  :    training acc:  0.08954008954008955    training loss:  4.897498267852008\n",
      "iteration  640  :    training acc:  0.08831908831908832    training loss:  4.897702433968457\n",
      "iteration  660  :    training acc:  0.09035409035409035    training loss:  4.905964785147082\n",
      "iteration  680  :    training acc:  0.09238909238909239    training loss:  4.923812989823361\n",
      "iteration  700  :    training acc:  0.09238909238909239    training loss:  4.921017764680759\n",
      "iteration  720  :    training acc:  0.0927960927960928    training loss:  4.916830766518057\n",
      "iteration  740  :    training acc:  0.09320309320309321    training loss:  4.910694500240672\n",
      "iteration  760  :    training acc:  0.09116809116809117    training loss:  4.902273041933696\n",
      "iteration  780  :    training acc:  0.09238909238909239    training loss:  4.891312344526674\n",
      "iteration  800  :    training acc:  0.09157509157509157    training loss:  4.879032939179838\n",
      "iteration  820  :    training acc:  0.08954008954008955    training loss:  4.865715249652835\n",
      "iteration  840  :    training acc:  0.08954008954008955    training loss:  4.850376454217013\n",
      "iteration  860  :    training acc:  0.08954008954008955    training loss:  4.831768772044191\n",
      "iteration  880  :    training acc:  0.08872608872608873    training loss:  4.808752147859802\n",
      "iteration  900  :    training acc:  0.09076109076109076    training loss:  4.780499316876983\n",
      "iteration  920  :    training acc:  0.08750508750508751    training loss:  4.747125693877715\n",
      "iteration  940  :    training acc:  0.09116809116809117    training loss:  4.706863654825106\n",
      "iteration  960  :    training acc:  0.08872608872608873    training loss:  4.653543990663225\n",
      "iteration  980  :    training acc:  0.09116809116809117    training loss:  4.561692963627584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1000  :    training acc:  0.0989010989010989    training loss:  4.440565007298851\n",
      "iteration  1020  :    training acc:  0.10093610093610093    training loss:  4.426189254842871\n",
      "iteration  1040  :    training acc:  0.10297110297110297    training loss:  4.576681224785856\n",
      "iteration  1060  :    training acc:  0.10175010175010175    training loss:  4.692408737643998\n",
      "iteration  1080  :    training acc:  0.11355311355311355    training loss:  4.538558741727472\n",
      "iteration  1100  :    training acc:  0.11314611314611314    training loss:  4.012028305977699\n",
      "iteration  1120  :    training acc:  0.11233211233211234    training loss:  5.270644331040246\n",
      "iteration  1140  :    training acc:  0.11518111518111518    training loss:  4.4180598397029875\n",
      "iteration  1160  :    training acc:  0.0984940984940985    training loss:  4.05718073256186\n",
      "iteration  1180  :    training acc:  0.115995115995116    training loss:  3.9090771814032355\n",
      "iteration  1200  :    training acc:  0.11192511192511193    training loss:  5.224086173112279\n",
      "iteration  1220  :    training acc:  0.1045991045991046    training loss:  3.9475906962838057\n",
      "iteration  1240  :    training acc:  0.12901912901912901    training loss:  3.890168882775449\n",
      "iteration  1260  :    training acc:  0.12535612535612536    training loss:  3.7558756338514137\n",
      "iteration  1280  :    training acc:  0.12861212861212862    training loss:  4.381286829131263\n",
      "iteration  1300  :    training acc:  0.12617012617012616    training loss:  4.852313057312615\n",
      "iteration  1320  :    training acc:  0.13024013024013023    training loss:  5.037688012269615\n",
      "iteration  1340  :    training acc:  0.12983312983312983    training loss:  4.509778146644497\n",
      "iteration  1360  :    training acc:  0.13675213675213677    training loss:  4.6769746454658385\n",
      "iteration  1380  :    training acc:  0.13878713878713878    training loss:  4.703029570576068\n",
      "iteration  1400  :    training acc:  0.14122914122914124    training loss:  4.611969358851364\n",
      "iteration  1420  :    training acc:  0.14285714285714285    training loss:  4.677976991380798\n",
      "iteration  1440  :    training acc:  0.13064713064713065    training loss:  4.970875040358237\n",
      "iteration  1460  :    training acc:  0.1391941391941392    training loss:  4.833328043471718\n",
      "iteration  1480  :    training acc:  0.14163614163614163    training loss:  4.807300492374639\n",
      "iteration  1500  :    training acc:  0.14692714692714692    training loss:  4.667746076244053\n",
      "iteration  1520  :    training acc:  0.14896214896214896    training loss:  4.652078004796619\n",
      "iteration  1540  :    training acc:  0.14814814814814814    training loss:  4.661454968749637\n",
      "iteration  1560  :    training acc:  0.14977614977614978    training loss:  4.475816778438301\n",
      "iteration  1580  :    training acc:  0.14774114774114774    training loss:  4.558432920006382\n",
      "iteration  1600  :    training acc:  0.15466015466015465    training loss:  4.296451225258006\n",
      "iteration  1620  :    training acc:  0.14692714692714692    training loss:  4.472935521291546\n",
      "iteration  1640  :    training acc:  0.1623931623931624    training loss:  4.253079414935493\n",
      "iteration  1660  :    training acc:  0.1571021571021571    training loss:  4.465946717914313\n",
      "iteration  1680  :    training acc:  0.17175417175417176    training loss:  4.236128015792789\n",
      "iteration  1700  :    training acc:  0.1676841676841677    training loss:  4.412549297921595\n",
      "iteration  1720  :    training acc:  0.16687016687016687    training loss:  4.272155199941232\n",
      "iteration  1740  :    training acc:  0.15954415954415954    training loss:  4.354225636233924\n",
      "iteration  1760  :    training acc:  0.1737891737891738    training loss:  4.277792900638435\n",
      "iteration  1780  :    training acc:  0.17175417175417176    training loss:  4.314105902069314\n",
      "iteration  1800  :    training acc:  0.17094017094017094    training loss:  4.287375189135186\n",
      "iteration  1820  :    training acc:  0.16809116809116809    training loss:  4.2761577453469855\n",
      "iteration  1840  :    training acc:  0.17094017094017094    training loss:  4.270824775375911\n",
      "iteration  1860  :    training acc:  0.1684981684981685    training loss:  4.26142473313667\n",
      "iteration  1880  :    training acc:  0.16687016687016687    training loss:  4.237366404870528\n",
      "iteration  1900  :    training acc:  0.16971916971916973    training loss:  4.210607296162349\n",
      "iteration  1920  :    training acc:  0.1689051689051689    training loss:  4.180864053556507\n",
      "iteration  1940  :    training acc:  0.16971916971916973    training loss:  4.150657431675395\n",
      "iteration  1960  :    training acc:  0.16809116809116809    training loss:  4.121511461382813\n",
      "iteration  1980  :    training acc:  0.1693121693121693    training loss:  4.094146100590575\n",
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.09992101105845182    training loss:  19.96614848965437\n",
      "iteration  20  :    training acc:  0.09913112164296999    training loss:  3.879717545372273\n",
      "iteration  40  :    training acc:  0.09399684044233807    training loss:  3.9432732035074007\n",
      "iteration  60  :    training acc:  0.09360189573459715    training loss:  3.907443079302122\n",
      "iteration  80  :    training acc:  0.10229067930489731    training loss:  3.8885697760444535\n",
      "iteration  100  :    training acc:  0.09992101105845182    training loss:  3.8642182441598787\n",
      "iteration  120  :    training acc:  0.09636650868878358    training loss:  3.841793140986289\n",
      "iteration  140  :    training acc:  0.09636650868878358    training loss:  3.818419631540311\n",
      "iteration  160  :    training acc:  0.09913112164296999    training loss:  3.7916352126055006\n",
      "iteration  180  :    training acc:  0.10031595576619273    training loss:  3.7641481738840525\n",
      "iteration  200  :    training acc:  0.10031595576619273    training loss:  3.735982541260597\n",
      "iteration  220  :    training acc:  0.10229067930489731    training loss:  3.7063313011210894\n",
      "iteration  240  :    training acc:  0.10268562401263823    training loss:  3.6755972440571343\n",
      "iteration  260  :    training acc:  0.1046603475513428    training loss:  3.649415131008631\n",
      "iteration  280  :    training acc:  0.1066350710900474    training loss:  3.6301714848441953\n",
      "iteration  300  :    training acc:  0.10703001579778831    training loss:  3.6162127263555073\n",
      "iteration  320  :    training acc:  0.10979462875197472    training loss:  3.6037044465664367\n",
      "iteration  340  :    training acc:  0.10979462875197472    training loss:  3.588391768452673\n",
      "iteration  360  :    training acc:  0.11453396524486571    training loss:  3.572367649503372\n",
      "iteration  380  :    training acc:  0.11453396524486571    training loss:  3.558004493760365\n",
      "iteration  400  :    training acc:  0.1141390205371248    training loss:  3.5437623726719085\n",
      "iteration  420  :    training acc:  0.11097946287519747    training loss:  3.5297108554422305\n",
      "iteration  440  :    training acc:  0.11255924170616113    training loss:  3.5165738240603854\n",
      "iteration  460  :    training acc:  0.11571879936808847    training loss:  3.5042614908725516\n",
      "iteration  480  :    training acc:  0.1165086887835703    training loss:  3.4923626097908786\n",
      "iteration  500  :    training acc:  0.12006319115323855    training loss:  3.481253967347077\n",
      "iteration  520  :    training acc:  0.12045813586097946    training loss:  3.4715079260832282\n",
      "iteration  540  :    training acc:  0.12322274881516587    training loss:  3.4635907810586075\n",
      "iteration  560  :    training acc:  0.12717219589257503    training loss:  3.456970271026459\n",
      "iteration  580  :    training acc:  0.12717219589257503    training loss:  3.451018516744053\n",
      "iteration  600  :    training acc:  0.12717219589257503    training loss:  3.4452920189651257\n",
      "iteration  620  :    training acc:  0.13151658767772512    training loss:  3.4393404543231076\n",
      "iteration  640  :    training acc:  0.13151658767772512    training loss:  3.43297730547654\n",
      "iteration  660  :    training acc:  0.13191153238546605    training loss:  3.4262360509636887\n",
      "iteration  680  :    training acc:  0.13112164296998421    training loss:  3.4196252528994258\n",
      "iteration  700  :    training acc:  0.1334913112164297    training loss:  3.413570789212167\n",
      "iteration  720  :    training acc:  0.13428120063191154    training loss:  3.4081265628641995\n",
      "iteration  740  :    training acc:  0.13033175355450238    training loss:  3.403329672600295\n",
      "iteration  760  :    training acc:  0.12796208530805686    training loss:  3.3991967798748366\n",
      "iteration  780  :    training acc:  0.1259873617693523    training loss:  3.3956348251587762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  800  :    training acc:  0.13270142180094788    training loss:  3.3926085648653617\n",
      "iteration  820  :    training acc:  0.1334913112164297    training loss:  3.3900585704436246\n",
      "iteration  840  :    training acc:  0.13270142180094788    training loss:  3.3878440684428\n",
      "iteration  860  :    training acc:  0.13704581358609794    training loss:  3.3857724906748965\n",
      "iteration  880  :    training acc:  0.13270142180094788    training loss:  3.3836872996786713\n",
      "iteration  900  :    training acc:  0.13191153238546605    training loss:  3.381525505962165\n",
      "iteration  920  :    training acc:  0.13309636650868878    training loss:  3.379253657719572\n",
      "iteration  940  :    training acc:  0.13467614533965244    training loss:  3.37684975112073\n",
      "iteration  960  :    training acc:  0.1382306477093207    training loss:  3.3743423012664095\n",
      "iteration  980  :    training acc:  0.13902053712480253    training loss:  3.37177283114597\n",
      "iteration  1000  :    training acc:  0.14218009478672985    training loss:  3.369096151464762\n",
      "iteration  1020  :    training acc:  0.14336492890995262    training loss:  3.3662368526467357\n",
      "iteration  1040  :    training acc:  0.14099526066350712    training loss:  3.3631566934064745\n",
      "iteration  1060  :    training acc:  0.14336492890995262    training loss:  3.3598040460252925\n",
      "iteration  1080  :    training acc:  0.14099526066350712    training loss:  3.3561043796432846\n",
      "iteration  1100  :    training acc:  0.14336492890995262    training loss:  3.3520698597817793\n",
      "iteration  1120  :    training acc:  0.14494470774091628    training loss:  3.347761480518798\n",
      "iteration  1140  :    training acc:  0.14770932069510267    training loss:  3.343251712906548\n",
      "iteration  1160  :    training acc:  0.14652448657187994    training loss:  3.3387032423013974\n",
      "iteration  1180  :    training acc:  0.14889415481832544    training loss:  3.3341696077218237\n",
      "iteration  1200  :    training acc:  0.1481042654028436    training loss:  3.3295300083967634\n",
      "iteration  1220  :    training acc:  0.15363349131121642    training loss:  3.3246903579545446\n",
      "iteration  1240  :    training acc:  0.15639810426540285    training loss:  3.319629395459959\n",
      "iteration  1260  :    training acc:  0.15876777251184834    training loss:  3.3143967315112\n",
      "iteration  1280  :    training acc:  0.16113744075829384    training loss:  3.3090862404918617\n",
      "iteration  1300  :    training acc:  0.16113744075829384    training loss:  3.3036185966746023\n",
      "iteration  1320  :    training acc:  0.16666666666666666    training loss:  3.2979490578616124\n",
      "iteration  1340  :    training acc:  0.1646919431279621    training loss:  3.292522478118786\n",
      "iteration  1360  :    training acc:  0.16390205371248026    training loss:  3.287825475744535\n",
      "iteration  1380  :    training acc:  0.16074249605055294    training loss:  3.2843563710007544\n",
      "iteration  1400  :    training acc:  0.160347551342812    training loss:  3.2824617576534862\n",
      "iteration  1420  :    training acc:  0.15995260663507108    training loss:  3.282105632887497\n",
      "iteration  1440  :    training acc:  0.15876777251184834    training loss:  3.282647092273374\n",
      "iteration  1460  :    training acc:  0.1583728278041074    training loss:  3.282602638113874\n",
      "iteration  1480  :    training acc:  0.15679304897314375    training loss:  3.281030055210015\n",
      "iteration  1500  :    training acc:  0.15718799368088468    training loss:  3.277268854108646\n",
      "iteration  1520  :    training acc:  0.15758293838862558    training loss:  3.272190636943302\n",
      "iteration  1540  :    training acc:  0.16113744075829384    training loss:  3.2684238917499417\n",
      "iteration  1560  :    training acc:  0.160347551342812    training loss:  3.266081623106893\n",
      "iteration  1580  :    training acc:  0.16153238546603477    training loss:  3.26391601164224\n",
      "iteration  1600  :    training acc:  0.15758293838862558    training loss:  3.261354851667817\n",
      "iteration  1620  :    training acc:  0.15876777251184834    training loss:  3.256116718933862\n",
      "iteration  1640  :    training acc:  0.15758293838862558    training loss:  3.2495979438975833\n",
      "iteration  1660  :    training acc:  0.15758293838862558    training loss:  3.24287731886631\n",
      "iteration  1680  :    training acc:  0.15995260663507108    training loss:  3.236979527552086\n",
      "iteration  1700  :    training acc:  0.1552132701421801    training loss:  3.231015896878472\n",
      "iteration  1720  :    training acc:  0.160347551342812    training loss:  3.226347850531025\n",
      "iteration  1740  :    training acc:  0.15955766192733017    training loss:  3.222393521228687\n",
      "iteration  1760  :    training acc:  0.16192733017377567    training loss:  3.216835678781247\n",
      "iteration  1780  :    training acc:  0.1623222748815166    training loss:  3.211283977056348\n",
      "iteration  1800  :    training acc:  0.1646919431279621    training loss:  3.2059007714761933\n",
      "iteration  1820  :    training acc:  0.1670616113744076    training loss:  3.197186704626582\n",
      "iteration  1840  :    training acc:  0.16824644549763032    training loss:  3.1892493557521293\n",
      "iteration  1860  :    training acc:  0.1674565560821485    training loss:  3.1847697306234726\n",
      "iteration  1880  :    training acc:  0.1702211690363349    training loss:  3.184911175172965\n",
      "iteration  1900  :    training acc:  0.16982622432859398    training loss:  3.1846979831797206\n",
      "iteration  1920  :    training acc:  0.16864139020537125    training loss:  3.186551698351267\n",
      "iteration  1940  :    training acc:  0.17101105845181674    training loss:  3.186796181157539\n",
      "iteration  1960  :    training acc:  0.16982622432859398    training loss:  3.184311723043263\n",
      "iteration  1980  :    training acc:  0.16982622432859398    training loss:  3.181991512554353\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.10482263850139498    training loss:  17.008626403733384\n",
      "iteration  20  :    training acc:  0.11438820247110403    training loss:  3.853209387180975\n",
      "iteration  40  :    training acc:  0.11797528895974492    training loss:  3.7328175434047184\n",
      "iteration  60  :    training acc:  0.12953367875647667    training loss:  3.6825444122308073\n",
      "iteration  80  :    training acc:  0.12634515743324035    training loss:  3.7285248107218982\n",
      "iteration  100  :    training acc:  0.12754085292945397    training loss:  3.6880266818294687\n",
      "iteration  120  :    training acc:  0.13670785173375846    training loss:  3.6492783345622044\n",
      "iteration  140  :    training acc:  0.1315265045834994    training loss:  3.627569704728708\n",
      "iteration  160  :    training acc:  0.1287365484256676    training loss:  3.6224060180282143\n",
      "iteration  180  :    training acc:  0.12833798326026305    training loss:  3.6248696371040374\n",
      "iteration  200  :    training acc:  0.12714228776404943    training loss:  3.6293354557675475\n",
      "iteration  220  :    training acc:  0.12754085292945397    training loss:  3.631394084114828\n",
      "iteration  240  :    training acc:  0.13112793941809486    training loss:  3.6303431122379637\n",
      "iteration  260  :    training acc:  0.1335193304105221    training loss:  3.6256087499345075\n",
      "iteration  280  :    training acc:  0.13670785173375846    training loss:  3.623774006501808\n",
      "iteration  300  :    training acc:  0.137106416899163    training loss:  3.623818821104297\n",
      "iteration  320  :    training acc:  0.13551215623754484    training loss:  3.6225200551368992\n",
      "iteration  340  :    training acc:  0.13670785173375846    training loss:  3.61883543441754\n",
      "iteration  360  :    training acc:  0.137106416899163    training loss:  3.6141311091704664\n",
      "iteration  380  :    training acc:  0.13630928656835392    training loss:  3.6058307284457882\n",
      "iteration  400  :    training acc:  0.13551215623754484    training loss:  3.5975209409119753\n",
      "iteration  420  :    training acc:  0.13312076524511757    training loss:  3.588191893415638\n",
      "iteration  440  :    training acc:  0.13232363491430849    training loss:  3.5774326500250586\n",
      "iteration  460  :    training acc:  0.1315265045834994    training loss:  3.5658013506517974\n",
      "iteration  480  :    training acc:  0.13232363491430849    training loss:  3.5537566445360866\n",
      "iteration  500  :    training acc:  0.1335193304105221    training loss:  3.541718624128123\n",
      "iteration  520  :    training acc:  0.1335193304105221    training loss:  3.5297612681945982\n",
      "iteration  540  :    training acc:  0.13471502590673576    training loss:  3.5176998225716307\n",
      "iteration  560  :    training acc:  0.13670785173375846    training loss:  3.5051076031470307\n",
      "iteration  580  :    training acc:  0.1387006775607812    training loss:  3.4915377236615557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  600  :    training acc:  0.137106416899163    training loss:  3.477381435293918\n",
      "iteration  620  :    training acc:  0.14109206855320844    training loss:  3.4631916713206525\n",
      "iteration  640  :    training acc:  0.1406935033878039    training loss:  3.449154155853933\n",
      "iteration  660  :    training acc:  0.1434834595456357    training loss:  3.4353561703763678\n",
      "iteration  680  :    training acc:  0.14866480669589477    training loss:  3.422092404152909\n",
      "iteration  700  :    training acc:  0.14826624153049023    training loss:  3.4096702366855913\n",
      "iteration  720  :    training acc:  0.15145476285372658    training loss:  3.398807531574431\n",
      "iteration  740  :    training acc:  0.1562375448385811    training loss:  3.3894221602584156\n",
      "iteration  760  :    training acc:  0.1578318055001993    training loss:  3.3804843190198333\n",
      "iteration  780  :    training acc:  0.15942606616181745    training loss:  3.3715757683752376\n",
      "iteration  800  :    training acc:  0.16699880430450378    training loss:  3.362543567583834\n",
      "iteration  820  :    training acc:  0.17218015145476284    training loss:  3.353179185582216\n",
      "iteration  840  :    training acc:  0.17058589079314468    training loss:  3.344143900980841\n",
      "iteration  860  :    training acc:  0.1697887604623356    training loss:  3.335838843141442\n",
      "iteration  880  :    training acc:  0.1733758469509765    training loss:  3.328202704518972\n",
      "iteration  900  :    training acc:  0.17218015145476284    training loss:  3.3211567772436235\n",
      "iteration  920  :    training acc:  0.1725787166201674    training loss:  3.3147802296127344\n",
      "iteration  940  :    training acc:  0.17098445595854922    training loss:  3.3091534571038745\n",
      "iteration  960  :    training acc:  0.17616580310880828    training loss:  3.304274283411905\n",
      "iteration  980  :    training acc:  0.17377441211638103    training loss:  3.300123560722576\n",
      "iteration  1000  :    training acc:  0.17377441211638103    training loss:  3.296691404364779\n",
      "iteration  1020  :    training acc:  0.17457154244719011    training loss:  3.293883423228655\n",
      "iteration  1040  :    training acc:  0.17457154244719011    training loss:  3.2916543820097144\n",
      "iteration  1060  :    training acc:  0.17576723794340374    training loss:  3.2902361484693756\n",
      "iteration  1080  :    training acc:  0.17497010761259466    training loss:  3.2896540874120137\n",
      "iteration  1100  :    training acc:  0.17457154244719011    training loss:  3.2894713829325264\n",
      "iteration  1120  :    training acc:  0.178158628935831    training loss:  3.2895385674260185\n",
      "iteration  1140  :    training acc:  0.17776006377042647    training loss:  3.290072401444605\n",
      "iteration  1160  :    training acc:  0.1817457154244719    training loss:  3.291645773221207\n",
      "iteration  1180  :    training acc:  0.18134715025906736    training loss:  3.2936215140295886\n",
      "iteration  1200  :    training acc:  0.18333997608609007    training loss:  3.296150115126379\n",
      "iteration  1220  :    training acc:  0.18573136707851734    training loss:  3.2995770217062903\n",
      "iteration  1240  :    training acc:  0.1853328019131128    training loss:  3.303509493783831\n",
      "iteration  1260  :    training acc:  0.1853328019131128    training loss:  3.3087580070450153\n",
      "iteration  1280  :    training acc:  0.18692706257473096    training loss:  3.3144950590080926\n",
      "iteration  1300  :    training acc:  0.18852132323634915    training loss:  3.31961510283789\n",
      "iteration  1320  :    training acc:  0.19011558389796732    training loss:  3.321540009301545\n",
      "iteration  1340  :    training acc:  0.19569549621363092    training loss:  3.3260658930769704\n",
      "iteration  1360  :    training acc:  0.1980868872060582    training loss:  3.340651328333624\n",
      "iteration  1380  :    training acc:  0.20007971303308092    training loss:  3.3607466370668537\n",
      "iteration  1400  :    training acc:  0.19888401753686727    training loss:  3.386147691681013\n",
      "iteration  1420  :    training acc:  0.20087684336389    training loss:  3.4432270317417597\n",
      "iteration  1440  :    training acc:  0.21243523316062177    training loss:  3.6642374176961976\n",
      "iteration  1460  :    training acc:  0.21203666799521723    training loss:  3.6624107596053914\n",
      "iteration  1480  :    training acc:  0.21323236349143085    training loss:  3.931904954797601\n",
      "iteration  1500  :    training acc:  0.18852132323634915    training loss:  3.811937721970077\n",
      "iteration  1520  :    training acc:  0.17736149860502193    training loss:  4.233252213850524\n",
      "iteration  1540  :    training acc:  0.19529693104822637    training loss:  3.767396017229447\n",
      "iteration  1560  :    training acc:  0.1590275009964129    training loss:  5.960319532039853\n",
      "iteration  1580  :    training acc:  0.1944998007174173    training loss:  6.698197786364646\n",
      "iteration  1600  :    training acc:  0.1287365484256676    training loss:  10.156096252640085\n",
      "iteration  1620  :    training acc:  0.12435233160621761    training loss:  8.720209729563107\n",
      "iteration  1640  :    training acc:  0.20605819051414906    training loss:  10.301607436765682\n",
      "iteration  1660  :    training acc:  0.1506576325229175    training loss:  10.301242000568157\n",
      "iteration  1680  :    training acc:  0.11837385412514946    training loss:  nan\n",
      "iteration  1700  :    training acc:  0.11239537664408131    training loss:  11.050388661126728\n",
      "iteration  1720  :    training acc:  0.12315663611000399    training loss:  15.43175135039914\n",
      "iteration  1740  :    training acc:  0.11119968114786767    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.15583897967317656    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.10362694300518134    training loss:  nan\n",
      "iteration  1800  :    training acc:  0.11319250697489039    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.17218015145476284    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.1151853328019131    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.1287365484256676    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.11359107214029494    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.12196094061379036    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.13192506974890394    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.11359107214029494    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.13989637305699482    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.11279394180948585    training loss:  nan\n",
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.09575710393149085    training loss:  19.929671007416292\n",
      "iteration  20  :    training acc:  0.09575710393149085    training loss:  3.8348740573766302\n",
      "iteration  40  :    training acc:  0.09809264305177112    training loss:  3.9286613917460786\n",
      "iteration  60  :    training acc:  0.09964966913195795    training loss:  3.9230503402821073\n",
      "iteration  80  :    training acc:  0.1012066952121448    training loss:  3.8960315870959654\n",
      "iteration  100  :    training acc:  0.10198520825223822    training loss:  3.8696593187075106\n",
      "iteration  120  :    training acc:  0.10548851693265862    training loss:  3.8477973802150096\n",
      "iteration  140  :    training acc:  0.10548851693265862    training loss:  3.827670803265427\n",
      "iteration  160  :    training acc:  0.1043207473725185    training loss:  3.8078105754905236\n",
      "iteration  180  :    training acc:  0.10626702997275204    training loss:  3.7888520193385684\n",
      "iteration  200  :    training acc:  0.10743479953289217    training loss:  3.7707288464740736\n",
      "iteration  220  :    training acc:  0.11093810821331257    training loss:  3.750893609227314\n",
      "iteration  240  :    training acc:  0.11171662125340599    training loss:  3.726945207767259\n",
      "iteration  260  :    training acc:  0.11599844297391981    training loss:  3.6974911477859895\n",
      "iteration  280  :    training acc:  0.11560918645387311    training loss:  3.668062730846652\n",
      "iteration  300  :    training acc:  0.11638769949396652    training loss:  3.636887744482691\n",
      "iteration  320  :    training acc:  0.11794472557415336    training loss:  3.6064373807340244\n",
      "iteration  340  :    training acc:  0.12066952121448034    training loss:  3.583685825768388\n",
      "iteration  360  :    training acc:  0.12105877773452706    training loss:  3.5654573493702904\n",
      "iteration  380  :    training acc:  0.1226158038147139    training loss:  3.5490592827702376\n",
      "iteration  400  :    training acc:  0.1226158038147139    training loss:  3.53356380044646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  420  :    training acc:  0.12378357337485403    training loss:  3.5188627717071266\n",
      "iteration  440  :    training acc:  0.12728688205527441    training loss:  3.5045606906638875\n",
      "iteration  460  :    training acc:  0.13195796029583495    training loss:  3.485930162926725\n",
      "iteration  480  :    training acc:  0.13507201245620865    training loss:  3.466034841287687\n",
      "iteration  500  :    training acc:  0.1331257298559751    training loss:  3.4488060751150136\n",
      "iteration  520  :    training acc:  0.1366290385363955    training loss:  3.4355212218663347\n",
      "iteration  540  :    training acc:  0.1366290385363955    training loss:  3.4267523983719648\n",
      "iteration  560  :    training acc:  0.141300116776956    training loss:  3.420661940602191\n",
      "iteration  580  :    training acc:  0.1409108602569093    training loss:  3.4162024272839213\n",
      "iteration  600  :    training acc:  0.14324639937718958    training loss:  3.4129942648638605\n",
      "iteration  620  :    training acc:  0.14674970805760998    training loss:  3.4109759525061087\n",
      "iteration  640  :    training acc:  0.1475282210977034    training loss:  3.4099466033480854\n",
      "iteration  660  :    training acc:  0.144024912417283    training loss:  3.4101070973954903\n",
      "iteration  680  :    training acc:  0.14869599065784353    training loss:  3.412752694978805\n",
      "iteration  700  :    training acc:  0.15103152977812379    training loss:  3.418114599653071\n",
      "iteration  720  :    training acc:  0.14713896457765668    training loss:  3.426266324042345\n",
      "iteration  740  :    training acc:  0.14869599065784353    training loss:  3.436911824005248\n",
      "iteration  760  :    training acc:  0.15414558193849748    training loss:  3.4498581664588697\n",
      "iteration  780  :    training acc:  0.15453483845854418    training loss:  3.460038526948656\n",
      "iteration  800  :    training acc:  0.15375632541845075    training loss:  3.4680435199127984\n",
      "iteration  820  :    training acc:  0.158816660179058    training loss:  3.479171616563658\n",
      "iteration  840  :    training acc:  0.15725963409887114    training loss:  3.487471682144637\n",
      "iteration  860  :    training acc:  0.15648112105877773    training loss:  3.4928551806282044\n",
      "iteration  880  :    training acc:  0.15920591669910472    training loss:  3.493427685191643\n",
      "iteration  900  :    training acc:  0.15959517321915143    training loss:  3.4885347661372164\n",
      "iteration  920  :    training acc:  0.16115219929933827    training loss:  3.4858546438980182\n",
      "iteration  940  :    training acc:  0.15998442973919813    training loss:  3.474097011529159\n",
      "iteration  960  :    training acc:  0.16270922537952512    training loss:  3.455888144024008\n",
      "iteration  980  :    training acc:  0.16815881666017907    training loss:  3.4366842497394825\n",
      "iteration  1000  :    training acc:  0.17166212534059946    training loss:  3.4200223207585085\n",
      "iteration  1020  :    training acc:  0.1697158427403659    training loss:  3.404715312508851\n",
      "iteration  1040  :    training acc:  0.17360840794083301    training loss:  3.3877326647278854\n",
      "iteration  1060  :    training acc:  0.17166212534059946    training loss:  3.3725810273543586\n",
      "iteration  1080  :    training acc:  0.17477617750097313    training loss:  3.3577908204192854\n",
      "iteration  1100  :    training acc:  0.17633320358115998    training loss:  3.3431085126577478\n",
      "iteration  1120  :    training acc:  0.17827948618139353    training loss:  3.34179525599696\n",
      "iteration  1140  :    training acc:  0.1806150253016738    training loss:  3.357665579111715\n",
      "iteration  1160  :    training acc:  0.1837290774620475    training loss:  3.3843253782057694\n",
      "iteration  1180  :    training acc:  0.17166212534059946    training loss:  3.5160980447693917\n",
      "iteration  1200  :    training acc:  0.17983651226158037    training loss:  3.496400451348376\n",
      "iteration  1220  :    training acc:  0.18333982094200077    training loss:  3.515609478127189\n",
      "iteration  1240  :    training acc:  0.18217205138186066    training loss:  3.545606855728076\n",
      "iteration  1260  :    training acc:  0.1880108991825613    training loss:  3.5262555685315053\n",
      "iteration  1280  :    training acc:  0.16543402101985208    training loss:  3.64601264373277\n",
      "iteration  1300  :    training acc:  0.17789022966134682    training loss:  3.4940458453261884\n",
      "iteration  1320  :    training acc:  0.15064227325807708    training loss:  4.4891063019541875\n",
      "iteration  1340  :    training acc:  0.14713896457765668    training loss:  4.128763461172895\n",
      "iteration  1360  :    training acc:  0.1300116776956014    training loss:  6.459972149900951\n",
      "iteration  1380  :    training acc:  0.188400155702608    training loss:  4.678475850646337\n",
      "iteration  1400  :    training acc:  0.17127286882055273    training loss:  4.874162744957418\n",
      "iteration  1420  :    training acc:  0.16270922537952512    training loss:  7.01610080547888\n",
      "iteration  1440  :    training acc:  0.14207862981704944    training loss:  8.112859734478107\n",
      "iteration  1460  :    training acc:  0.1872323861424679    training loss:  7.564495208775083\n",
      "iteration  1480  :    training acc:  0.17244063838069287    training loss:  10.79296050084744\n",
      "iteration  1500  :    training acc:  0.1872323861424679    training loss:  10.475050947659968\n",
      "iteration  1520  :    training acc:  0.13117944725574154    training loss:  12.130976106145418\n",
      "iteration  1540  :    training acc:  0.11872323861424679    training loss:  inf\n",
      "iteration  1560  :    training acc:  0.14246788633709614    training loss:  nan\n",
      "iteration  1580  :    training acc:  0.1047100038925652    training loss:  nan\n",
      "iteration  1600  :    training acc:  0.11327364733359284    training loss:  nan\n",
      "iteration  1620  :    training acc:  0.10003892565200467    training loss:  nan\n",
      "iteration  1640  :    training acc:  0.16309848189957182    training loss:  nan\n",
      "iteration  1660  :    training acc:  0.1191124951342935    training loss:  nan\n",
      "iteration  1680  :    training acc:  0.14246788633709614    training loss:  nan\n",
      "iteration  1700  :    training acc:  0.16854807318022577    training loss:  nan\n",
      "iteration  1720  :    training acc:  0.09926041261191125    training loss:  nan\n",
      "iteration  1740  :    training acc:  0.13156870377578825    training loss:  nan\n",
      "iteration  1760  :    training acc:  0.1362397820163488    training loss:  nan\n",
      "iteration  1780  :    training acc:  0.10860256909303231    training loss:  nan\n",
      "iteration  1800  :    training acc:  0.14986376021798364    training loss:  nan\n",
      "iteration  1820  :    training acc:  0.09887115609186455    training loss:  nan\n",
      "iteration  1840  :    training acc:  0.1339042428960685    training loss:  nan\n",
      "iteration  1860  :    training acc:  0.11327364733359284    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.13935383417672245    training loss:  nan\n",
      "iteration  1900  :    training acc:  0.10198520825223822    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.13935383417672245    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.12144803425457376    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.14674970805760998    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.1377968080965356    training loss:  nan\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.09328968903436989    training loss:  16.871212640770725\n",
      "iteration  20  :    training acc:  0.1039279869067103    training loss:  3.999639946520449\n",
      "iteration  40  :    training acc:  0.10351882160392799    training loss:  3.921246313553183\n",
      "iteration  60  :    training acc:  0.10515548281505728    training loss:  3.9509209317860163\n",
      "iteration  80  :    training acc:  0.10474631751227496    training loss:  3.9378034006647202\n",
      "iteration  100  :    training acc:  0.1039279869067103    training loss:  3.925506303388609\n",
      "iteration  120  :    training acc:  0.1072013093289689    training loss:  3.9145311427331535\n",
      "iteration  140  :    training acc:  0.10761047463175123    training loss:  3.9036122385798286\n",
      "iteration  160  :    training acc:  0.1072013093289689    training loss:  3.8924003367956965\n",
      "iteration  180  :    training acc:  0.10474631751227496    training loss:  3.8812378853118688\n",
      "iteration  200  :    training acc:  0.10761047463175123    training loss:  3.8716049379885615\n",
      "iteration  220  :    training acc:  0.10474631751227496    training loss:  3.864505101039698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  240  :    training acc:  0.10924713584288052    training loss:  3.8593051451992126\n",
      "iteration  260  :    training acc:  0.11170212765957446    training loss:  3.8555032892417853\n",
      "iteration  280  :    training acc:  0.11292962356792144    training loss:  3.852999505803307\n",
      "iteration  300  :    training acc:  0.11252045826513912    training loss:  3.851679208895307\n",
      "iteration  320  :    training acc:  0.10965630114566285    training loss:  3.8513559916544997\n",
      "iteration  340  :    training acc:  0.1104746317512275    training loss:  3.851728557770397\n",
      "iteration  360  :    training acc:  0.10761047463175123    training loss:  3.8524823921243425\n",
      "iteration  380  :    training acc:  0.10679214402618659    training loss:  3.8534000298505204\n",
      "iteration  400  :    training acc:  0.1088379705400982    training loss:  3.8544587865568207\n",
      "iteration  420  :    training acc:  0.11006546644844517    training loss:  3.8557858277483645\n",
      "iteration  440  :    training acc:  0.11088379705400982    training loss:  3.857358976259806\n",
      "iteration  460  :    training acc:  0.11129296235679215    training loss:  3.859012966535714\n",
      "iteration  480  :    training acc:  0.11170212765957446    training loss:  3.860540456204161\n",
      "iteration  500  :    training acc:  0.11497545008183306    training loss:  3.8618982795602896\n",
      "iteration  520  :    training acc:  0.11783960720130933    training loss:  3.86336431201896\n",
      "iteration  540  :    training acc:  0.11824877250409166    training loss:  3.8651939668565576\n",
      "iteration  560  :    training acc:  0.11824877250409166    training loss:  3.86734796373159\n",
      "iteration  580  :    training acc:  0.11865793780687398    training loss:  3.8697465636058714\n",
      "iteration  600  :    training acc:  0.12193126022913257    training loss:  3.8721577248052084\n",
      "iteration  620  :    training acc:  0.12356792144026187    training loss:  3.873816035263712\n",
      "iteration  640  :    training acc:  0.12274959083469722    training loss:  3.874162115941292\n",
      "iteration  660  :    training acc:  0.12234042553191489    training loss:  3.8732626270965635\n",
      "iteration  680  :    training acc:  0.12152209492635024    training loss:  3.8724518756024375\n",
      "iteration  700  :    training acc:  0.12152209492635024    training loss:  3.871863139857376\n",
      "iteration  720  :    training acc:  0.12356792144026187    training loss:  3.8715134542431597\n",
      "iteration  740  :    training acc:  0.12725040916530278    training loss:  3.8727193847039247\n",
      "iteration  760  :    training acc:  0.1239770867430442    training loss:  3.8772733773017625\n",
      "iteration  780  :    training acc:  0.12315875613747954    training loss:  3.8863420816568914\n",
      "iteration  800  :    training acc:  0.12929623567921442    training loss:  3.901455956744337\n",
      "iteration  820  :    training acc:  0.13379705400981998    training loss:  3.9208959031305026\n",
      "iteration  840  :    training acc:  0.13379705400981998    training loss:  3.9393609068138\n",
      "iteration  860  :    training acc:  0.13829787234042554    training loss:  3.9595624153003226\n",
      "iteration  880  :    training acc:  0.13829787234042554    training loss:  3.9750167296243126\n",
      "iteration  900  :    training acc:  0.14034369885433715    training loss:  3.9906512055500234\n",
      "iteration  920  :    training acc:  0.14157119476268412    training loss:  4.007364880124274\n",
      "iteration  940  :    training acc:  0.1448445171849427    training loss:  4.026272819470966\n",
      "iteration  960  :    training acc:  0.14238952536824878    training loss:  4.041492514050552\n",
      "iteration  980  :    training acc:  0.14361702127659576    training loss:  4.050782644304246\n",
      "iteration  1000  :    training acc:  0.14607201309328968    training loss:  4.062754966314643\n",
      "iteration  1020  :    training acc:  0.14729950900163666    training loss:  4.0932317977879835\n",
      "iteration  1040  :    training acc:  0.15630114566284778    training loss:  3.6801771184554144\n",
      "iteration  1060  :    training acc:  0.10515548281505728    training loss:  7.290532482734049\n",
      "iteration  1080  :    training acc:  0.12970540098199673    training loss:  4.377100202779424\n",
      "iteration  1100  :    training acc:  0.15589198036006546    training loss:  3.598477615221111\n",
      "iteration  1120  :    training acc:  0.16366612111292964    training loss:  4.0726180053796055\n",
      "iteration  1140  :    training acc:  0.12970540098199673    training loss:  5.955773125762699\n",
      "iteration  1160  :    training acc:  0.14198036006546644    training loss:  4.269843788991692\n",
      "iteration  1180  :    training acc:  0.13788870703764322    training loss:  5.835055307049048\n",
      "iteration  1200  :    training acc:  0.1497545008183306    training loss:  4.590286327485451\n",
      "iteration  1220  :    training acc:  0.11088379705400982    training loss:  7.121928771867735\n",
      "iteration  1240  :    training acc:  0.11129296235679215    training loss:  7.719436730804912\n",
      "iteration  1260  :    training acc:  0.11374795417348608    training loss:  5.775452393302478\n",
      "iteration  1280  :    training acc:  0.1309328968903437    training loss:  6.110635209549473\n",
      "iteration  1300  :    training acc:  0.1342062193126023    training loss:  5.603737451182399\n",
      "iteration  1320  :    training acc:  0.1256137479541735    training loss:  4.816083635682745\n",
      "iteration  1340  :    training acc:  0.13911620294599017    training loss:  4.032903794312157\n",
      "iteration  1360  :    training acc:  0.1497545008183306    training loss:  4.230411711376758\n",
      "iteration  1380  :    training acc:  0.14852700490998363    training loss:  4.465324820133331\n",
      "iteration  1400  :    training acc:  0.11988543371522095    training loss:  5.950747602313474\n",
      "iteration  1420  :    training acc:  0.16734860883797054    training loss:  3.8013040534053153\n",
      "iteration  1440  :    training acc:  0.14893617021276595    training loss:  4.822876229066714\n",
      "iteration  1460  :    training acc:  0.19230769230769232    training loss:  3.3502108605533047\n",
      "iteration  1480  :    training acc:  0.12438625204582651    training loss:  5.169997104496036\n",
      "iteration  1500  :    training acc:  0.15834697217675942    training loss:  4.3518540327618025\n",
      "iteration  1520  :    training acc:  0.11824877250409166    training loss:  5.856304758465491\n",
      "iteration  1540  :    training acc:  0.17594108019639934    training loss:  3.942416747867934\n",
      "iteration  1560  :    training acc:  0.1804418985270049    training loss:  3.4003726453084835\n",
      "iteration  1580  :    training acc:  0.17757774140752863    training loss:  3.957029035349935\n",
      "iteration  1600  :    training acc:  0.1992635024549918    training loss:  3.4663770382286128\n",
      "iteration  1620  :    training acc:  0.1939443535188216    training loss:  3.4746736780747502\n",
      "iteration  1640  :    training acc:  0.20294599018003273    training loss:  3.3852898310821207\n",
      "iteration  1660  :    training acc:  0.20008183306055646    training loss:  3.289629430828922\n",
      "iteration  1680  :    training acc:  0.11333878887070377    training loss:  5.409177687534946\n",
      "iteration  1700  :    training acc:  0.18617021276595744    training loss:  3.277299354437393\n",
      "iteration  1720  :    training acc:  0.20867430441898527    training loss:  3.2368454119468275\n",
      "iteration  1740  :    training acc:  0.17553191489361702    training loss:  3.633326640299182\n",
      "iteration  1760  :    training acc:  0.16489361702127658    training loss:  3.787479226292281\n",
      "iteration  1780  :    training acc:  0.16898527004909983    training loss:  3.7275485496367793\n",
      "iteration  1800  :    training acc:  0.14770867430441897    training loss:  3.7219569058712736\n",
      "iteration  1820  :    training acc:  0.13338788870703763    training loss:  4.202822920777565\n",
      "iteration  1840  :    training acc:  0.11865793780687398    training loss:  4.172704327410997\n",
      "iteration  1860  :    training acc:  0.1039279869067103    training loss:  6.609318279390922\n",
      "iteration  1880  :    training acc:  0.12847790507364976    training loss:  4.571369323425104\n",
      "iteration  1900  :    training acc:  0.15425531914893617    training loss:  4.04262128610394\n",
      "iteration  1920  :    training acc:  0.21112929623567922    training loss:  3.523404598535284\n",
      "iteration  1940  :    training acc:  0.1804418985270049    training loss:  4.40339898421927\n",
      "iteration  1960  :    training acc:  0.20130932896890344    training loss:  3.331012031207792\n",
      "iteration  1980  :    training acc:  0.17635024549918166    training loss:  4.440716527677224\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.10522022838499184    training loss:  18.161339042194058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  20  :    training acc:  0.10522022838499184    training loss:  4.1108447637569485\n",
      "iteration  40  :    training acc:  0.09787928221859707    training loss:  4.515165223530336\n",
      "iteration  60  :    training acc:  0.09787928221859707    training loss:  4.628844683798003\n",
      "iteration  80  :    training acc:  0.100326264274062    training loss:  4.612793485752957\n",
      "iteration  100  :    training acc:  0.1064437194127243    training loss:  4.522179967584939\n",
      "iteration  120  :    training acc:  0.09951060358890701    training loss:  4.516255347401045\n",
      "iteration  140  :    training acc:  0.10399673735725938    training loss:  4.468800760378042\n",
      "iteration  160  :    training acc:  0.10154975530179446    training loss:  4.6816477439259785\n",
      "iteration  180  :    training acc:  0.1064437194127243    training loss:  4.409469440478559\n",
      "iteration  200  :    training acc:  0.10603588907014681    training loss:  4.48798383616046\n",
      "iteration  220  :    training acc:  0.10929853181076672    training loss:  4.388520943090172\n",
      "iteration  240  :    training acc:  0.10358890701468189    training loss:  4.717723255110581\n",
      "iteration  260  :    training acc:  0.10318107667210441    training loss:  4.462355245502929\n",
      "iteration  280  :    training acc:  0.10358890701468189    training loss:  4.37606133796875\n",
      "iteration  300  :    training acc:  0.10318107667210441    training loss:  4.443906061019472\n",
      "iteration  320  :    training acc:  0.10195758564437195    training loss:  4.611602010287311\n",
      "iteration  340  :    training acc:  0.10481239804241435    training loss:  4.351491167642588\n",
      "iteration  360  :    training acc:  0.1068515497553018    training loss:  4.647915183356016\n",
      "iteration  380  :    training acc:  0.10603588907014681    training loss:  4.388049911992083\n",
      "iteration  400  :    training acc:  0.11215334420880914    training loss:  4.334396207119374\n",
      "iteration  420  :    training acc:  0.11745513866231648    training loss:  4.3927812782372735\n",
      "iteration  440  :    training acc:  0.1166394779771615    training loss:  4.2879090959161354\n",
      "iteration  460  :    training acc:  0.11500815660685156    training loss:  4.232376255116255\n",
      "iteration  480  :    training acc:  0.11256117455138662    training loss:  4.134815624261991\n",
      "iteration  500  :    training acc:  0.11541598694942903    training loss:  4.035258142276708\n",
      "iteration  520  :    training acc:  0.11460032626427406    training loss:  3.994899652323941\n",
      "iteration  540  :    training acc:  0.11460032626427406    training loss:  3.9967612487950097\n",
      "iteration  560  :    training acc:  0.11541598694942903    training loss:  4.012117112158343\n",
      "iteration  580  :    training acc:  0.11582381729200653    training loss:  4.023629774648335\n",
      "iteration  600  :    training acc:  0.11623164763458402    training loss:  4.032761371506318\n",
      "iteration  620  :    training acc:  0.11745513866231648    training loss:  4.041932359422866\n",
      "iteration  640  :    training acc:  0.11704730831973899    training loss:  4.054819109632767\n",
      "iteration  660  :    training acc:  0.11500815660685156    training loss:  4.067145931030368\n",
      "iteration  680  :    training acc:  0.11745513866231648    training loss:  4.075402444834892\n",
      "iteration  700  :    training acc:  0.11623164763458402    training loss:  4.089033792321135\n",
      "iteration  720  :    training acc:  0.11541598694942903    training loss:  4.100159070225927\n",
      "iteration  740  :    training acc:  0.11949429037520391    training loss:  4.097391666130633\n",
      "iteration  760  :    training acc:  0.12438825448613376    training loss:  4.084617063864206\n",
      "iteration  780  :    training acc:  0.12887438825448613    training loss:  4.068743292603314\n",
      "iteration  800  :    training acc:  0.13132137030995106    training loss:  4.047957508823307\n",
      "iteration  820  :    training acc:  0.132952691680261    training loss:  4.021714914245213\n",
      "iteration  840  :    training acc:  0.13254486133768353    training loss:  3.992414839951447\n",
      "iteration  860  :    training acc:  0.13580750407830341    training loss:  3.9628936690117627\n",
      "iteration  880  :    training acc:  0.13866231647634583    training loss:  3.934809150644629\n",
      "iteration  900  :    training acc:  0.13825448613376834    training loss:  3.906860041778179\n",
      "iteration  920  :    training acc:  0.14274061990212072    training loss:  3.8776467153632628\n",
      "iteration  940  :    training acc:  0.14437194127243066    training loss:  3.846307480966008\n",
      "iteration  960  :    training acc:  0.1407014681892333    training loss:  3.8114618684158104\n",
      "iteration  980  :    training acc:  0.14110929853181076    training loss:  3.779032352634726\n",
      "iteration  1000  :    training acc:  0.14192495921696574    training loss:  3.7546731630236954\n",
      "iteration  1020  :    training acc:  0.14355628058727568    training loss:  3.73298845942335\n",
      "iteration  1040  :    training acc:  0.14477977161500816    training loss:  3.7115048198022302\n",
      "iteration  1060  :    training acc:  0.14518760195758565    training loss:  3.6899120765691715\n",
      "iteration  1080  :    training acc:  0.1464110929853181    training loss:  3.6679084752816693\n",
      "iteration  1100  :    training acc:  0.14845024469820556    training loss:  3.6455539152756535\n",
      "iteration  1120  :    training acc:  0.14722675367047308    training loss:  3.623355420539558\n",
      "iteration  1140  :    training acc:  0.1464110929853181    training loss:  3.6019502132537538\n",
      "iteration  1160  :    training acc:  0.14518760195758565    training loss:  3.5820753964282206\n",
      "iteration  1180  :    training acc:  0.14314845024469822    training loss:  3.5655248348440147\n",
      "iteration  1200  :    training acc:  0.14355628058727568    training loss:  3.553127369378057\n",
      "iteration  1220  :    training acc:  0.14274061990212072    training loss:  3.544878080371538\n",
      "iteration  1240  :    training acc:  0.14233278955954323    training loss:  3.538816739379728\n",
      "iteration  1260  :    training acc:  0.14192495921696574    training loss:  3.533731575880706\n",
      "iteration  1280  :    training acc:  0.14396411092985317    training loss:  3.5287536175128214\n",
      "iteration  1300  :    training acc:  0.14110929853181076    training loss:  3.523396263536746\n",
      "iteration  1320  :    training acc:  0.14274061990212072    training loss:  3.5174739336837724\n",
      "iteration  1340  :    training acc:  0.14355628058727568    training loss:  3.51102118282273\n",
      "iteration  1360  :    training acc:  0.14437194127243066    training loss:  3.504152840921746\n",
      "iteration  1380  :    training acc:  0.14559543230016314    training loss:  3.496940313739505\n",
      "iteration  1400  :    training acc:  0.14885807504078302    training loss:  3.489149962574246\n",
      "iteration  1420  :    training acc:  0.14804241435562807    training loss:  3.4803516453950647\n",
      "iteration  1440  :    training acc:  0.14559543230016314    training loss:  3.4707252404431563\n",
      "iteration  1460  :    training acc:  0.1468189233278956    training loss:  3.462621613625252\n",
      "iteration  1480  :    training acc:  0.14885807504078302    training loss:  3.4572751761937406\n",
      "iteration  1500  :    training acc:  0.14926590538336051    training loss:  3.4540234527955067\n",
      "iteration  1520  :    training acc:  0.14722675367047308    training loss:  3.452279083008552\n",
      "iteration  1540  :    training acc:  0.15089722675367048    training loss:  3.4513455124511094\n",
      "iteration  1560  :    training acc:  0.15252854812398042    training loss:  3.451261906570276\n",
      "iteration  1580  :    training acc:  0.1537520391517129    training loss:  3.4518313103461935\n",
      "iteration  1600  :    training acc:  0.15497553017944535    training loss:  3.4528515742552615\n",
      "iteration  1620  :    training acc:  0.15456769983686786    training loss:  3.4524271951359213\n",
      "iteration  1640  :    training acc:  0.15783034257748776    training loss:  3.4493130263523257\n",
      "iteration  1660  :    training acc:  0.15864600326264275    training loss:  3.4440479937091095\n",
      "iteration  1680  :    training acc:  0.16109298531810767    training loss:  3.437803448747089\n",
      "iteration  1700  :    training acc:  0.16190864600326263    training loss:  3.4313190223064374\n",
      "iteration  1720  :    training acc:  0.1635399673735726    training loss:  3.42454560110548\n",
      "iteration  1740  :    training acc:  0.1639477977161501    training loss:  3.418328337259262\n",
      "iteration  1760  :    training acc:  0.16639477977161501    training loss:  3.412586973101301\n",
      "iteration  1780  :    training acc:  0.16843393148450245    training loss:  3.4065159994172025\n",
      "iteration  1800  :    training acc:  0.16843393148450245    training loss:  3.3992175706214094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1820  :    training acc:  0.1737357259380098    training loss:  3.392303699253329\n",
      "iteration  1840  :    training acc:  0.17903752039151713    training loss:  3.390903539141608\n",
      "iteration  1860  :    training acc:  0.18270799347471453    training loss:  3.3914023105181688\n",
      "iteration  1880  :    training acc:  0.18189233278955955    training loss:  3.3869674931943465\n",
      "iteration  1900  :    training acc:  0.18148450244698205    training loss:  3.3869135490786952\n",
      "iteration  1920  :    training acc:  0.17903752039151713    training loss:  3.3891240584235147\n",
      "iteration  1940  :    training acc:  0.17985318107667211    training loss:  3.391388999605311\n",
      "iteration  1960  :    training acc:  0.18066884176182707    training loss:  3.393314025697889\n",
      "iteration  1980  :    training acc:  0.183115823817292    training loss:  3.3952470589830623\n",
      "\n",
      "Cross Validation fold  0\n",
      "iteration  0  :    training acc:  0.10575364667747164    training loss:  21.094053377472655\n",
      "iteration  20  :    training acc:  0.11102106969205834    training loss:  4.83116585709402\n",
      "iteration  40  :    training acc:  0.09967585089141005    training loss:  6.202838331154285\n",
      "iteration  60  :    training acc:  0.09805510534846029    training loss:  5.16057310381527\n",
      "iteration  80  :    training acc:  0.10413290113452188    training loss:  5.422626816353955\n",
      "iteration  100  :    training acc:  0.10210696920583469    training loss:  6.8375027598375455\n",
      "iteration  120  :    training acc:  0.10453808752025932    training loss:  4.912741782134782\n",
      "iteration  140  :    training acc:  0.10210696920583469    training loss:  5.800142720085509\n",
      "iteration  160  :    training acc:  0.10210696920583469    training loss:  4.884538961202158\n",
      "iteration  180  :    training acc:  0.10656401944894651    training loss:  4.8848426456649765\n",
      "iteration  200  :    training acc:  0.1085899513776337    training loss:  4.976830436391616\n",
      "iteration  220  :    training acc:  0.10372771474878444    training loss:  5.050817215541585\n",
      "iteration  240  :    training acc:  0.10980551053484602    training loss:  5.038242284520945\n",
      "iteration  260  :    training acc:  0.10372771474878444    training loss:  5.927925946540014\n",
      "iteration  280  :    training acc:  0.11102106969205834    training loss:  5.104757498732313\n",
      "iteration  300  :    training acc:  0.11183144246353323    training loss:  4.8168908541886015\n",
      "iteration  320  :    training acc:  0.11385737439222042    training loss:  4.799760205841931\n",
      "iteration  340  :    training acc:  0.11142625607779579    training loss:  4.825194840514907\n",
      "iteration  360  :    training acc:  0.09278768233387358    training loss:  4.893551225240197\n",
      "iteration  380  :    training acc:  0.1126418152350081    training loss:  4.976446856627287\n",
      "iteration  400  :    training acc:  0.11183144246353323    training loss:  5.355042927483794\n",
      "iteration  420  :    training acc:  0.10372771474878444    training loss:  5.457642655600685\n",
      "iteration  440  :    training acc:  0.10818476499189628    training loss:  6.066803181878368\n",
      "iteration  460  :    training acc:  0.11304700162074555    training loss:  5.346774760974518\n",
      "iteration  480  :    training acc:  0.10818476499189628    training loss:  4.659763203011001\n",
      "iteration  500  :    training acc:  0.1094003241491086    training loss:  5.09775079057525\n",
      "iteration  520  :    training acc:  0.10818476499189628    training loss:  4.87614295865467\n",
      "iteration  540  :    training acc:  0.11142625607779579    training loss:  4.686032984139869\n",
      "iteration  560  :    training acc:  0.11223662884927066    training loss:  5.3125939891227985\n",
      "iteration  580  :    training acc:  0.1085899513776337    training loss:  4.85204600466163\n",
      "iteration  600  :    training acc:  0.11628849270664506    training loss:  4.809492739750233\n",
      "iteration  620  :    training acc:  0.11021069692058347    training loss:  5.281881069705707\n",
      "iteration  640  :    training acc:  0.10737439222042139    training loss:  4.897564642343025\n",
      "iteration  660  :    training acc:  0.1126418152350081    training loss:  5.024858962710847\n",
      "iteration  680  :    training acc:  0.10413290113452188    training loss:  5.325699362092154\n",
      "iteration  700  :    training acc:  0.10089141004862237    training loss:  5.689588276119967\n",
      "iteration  720  :    training acc:  0.09967585089141005    training loss:  5.168215023924647\n",
      "iteration  740  :    training acc:  0.10170178282009724    training loss:  4.753395488070675\n",
      "iteration  760  :    training acc:  0.11912479740680713    training loss:  4.767015014139045\n",
      "iteration  780  :    training acc:  0.10372771474878444    training loss:  4.914687462473476\n",
      "iteration  800  :    training acc:  0.10413290113452188    training loss:  4.813083506258728\n",
      "iteration  820  :    training acc:  0.10372771474878444    training loss:  4.446900994880196\n",
      "iteration  840  :    training acc:  0.09805510534846029    training loss:  4.879596252491796\n",
      "iteration  860  :    training acc:  0.1094003241491086    training loss:  4.592300035054695\n",
      "iteration  880  :    training acc:  0.11547811993517018    training loss:  4.8861251537232\n",
      "iteration  900  :    training acc:  0.10575364667747164    training loss:  4.810948692852161\n",
      "iteration  920  :    training acc:  0.10291734197730956    training loss:  5.331848377190091\n",
      "iteration  940  :    training acc:  0.10696920583468396    training loss:  5.27785699738725\n",
      "iteration  960  :    training acc:  0.10251215559157212    training loss:  6.226335322309889\n",
      "iteration  980  :    training acc:  0.10453808752025932    training loss:  4.796385713030039\n",
      "iteration  1000  :    training acc:  0.1146677471636953    training loss:  4.9119376058557425\n",
      "iteration  1020  :    training acc:  0.11223662884927066    training loss:  5.447493296894607\n",
      "iteration  1040  :    training acc:  0.11102106969205834    training loss:  4.8437052014390245\n",
      "iteration  1060  :    training acc:  0.1146677471636953    training loss:  4.847462141931015\n",
      "iteration  1080  :    training acc:  0.11912479740680713    training loss:  4.783500490276398\n",
      "iteration  1100  :    training acc:  0.10575364667747164    training loss:  4.867087013742963\n",
      "iteration  1120  :    training acc:  0.10575364667747164    training loss:  4.8153115863550395\n",
      "iteration  1140  :    training acc:  0.11304700162074555    training loss:  4.895278191265762\n",
      "iteration  1160  :    training acc:  0.103322528363047    training loss:  4.808320882643986\n",
      "iteration  1180  :    training acc:  0.11426256077795786    training loss:  5.440179451728493\n",
      "iteration  1200  :    training acc:  0.11426256077795786    training loss:  4.448469368613891\n",
      "iteration  1220  :    training acc:  0.1094003241491086    training loss:  4.712974045135467\n",
      "iteration  1240  :    training acc:  0.11345218800648298    training loss:  4.766573460174776\n",
      "iteration  1260  :    training acc:  0.11588330632090761    training loss:  4.453516500366167\n",
      "iteration  1280  :    training acc:  0.12236628849270664    training loss:  4.707339464243976\n",
      "iteration  1300  :    training acc:  0.11871961102106969    training loss:  4.423226058766805\n",
      "iteration  1320  :    training acc:  0.11993517017828201    training loss:  4.5892975514233685\n",
      "iteration  1340  :    training acc:  0.11385737439222042    training loss:  4.4398249110305\n",
      "iteration  1360  :    training acc:  0.11223662884927066    training loss:  4.469460605293139\n",
      "iteration  1380  :    training acc:  0.10777957860615883    training loss:  4.374284191259995\n",
      "iteration  1400  :    training acc:  0.11426256077795786    training loss:  4.4799893409474825\n",
      "iteration  1420  :    training acc:  0.11183144246353323    training loss:  4.499898630856381\n",
      "iteration  1440  :    training acc:  0.11345218800648298    training loss:  4.515911884946999\n",
      "iteration  1460  :    training acc:  0.11304700162074555    training loss:  4.396918743694959\n",
      "iteration  1480  :    training acc:  0.11790923824959482    training loss:  4.40232417124685\n",
      "iteration  1500  :    training acc:  0.1146677471636953    training loss:  4.802809392549115\n",
      "iteration  1520  :    training acc:  0.10818476499189628    training loss:  4.454824148520075\n",
      "iteration  1540  :    training acc:  0.12074554294975688    training loss:  4.4306202381622075\n",
      "iteration  1560  :    training acc:  0.11021069692058347    training loss:  4.3785796854707995\n",
      "iteration  1580  :    training acc:  0.11628849270664506    training loss:  4.462335288686142\n",
      "iteration  1600  :    training acc:  0.11142625607779579    training loss:  4.455006687818887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1620  :    training acc:  0.11831442463533225    training loss:  4.374450622499402\n",
      "iteration  1640  :    training acc:  0.11628849270664506    training loss:  4.64688250328438\n",
      "iteration  1660  :    training acc:  0.11912479740680713    training loss:  4.1688558644293385\n",
      "iteration  1680  :    training acc:  0.10291734197730956    training loss:  4.54353792059614\n",
      "iteration  1700  :    training acc:  0.10615883306320907    training loss:  4.723892693946251\n",
      "iteration  1720  :    training acc:  0.11304700162074555    training loss:  4.737964230937248\n",
      "iteration  1740  :    training acc:  0.11102106969205834    training loss:  4.293585458075595\n",
      "iteration  1760  :    training acc:  0.1094003241491086    training loss:  4.130685091086029\n",
      "iteration  1780  :    training acc:  0.11061588330632091    training loss:  4.270062562889542\n",
      "iteration  1800  :    training acc:  0.12844408427876824    training loss:  4.0509584211804714\n",
      "iteration  1820  :    training acc:  0.11831442463533225    training loss:  4.578546214511657\n",
      "iteration  1840  :    training acc:  0.12115072933549433    training loss:  4.726512601526879\n",
      "iteration  1860  :    training acc:  0.11588330632090761    training loss:  4.728516696920841\n",
      "iteration  1880  :    training acc:  0.1272285251215559    training loss:  4.214476084140315\n",
      "iteration  1900  :    training acc:  0.11952998379254456    training loss:  4.106345302481037\n",
      "iteration  1920  :    training acc:  0.12844408427876824    training loss:  4.201589723892066\n",
      "iteration  1940  :    training acc:  0.11750405186385737    training loss:  4.350796751277399\n",
      "iteration  1960  :    training acc:  0.1272285251215559    training loss:  4.1911859720970535\n",
      "iteration  1980  :    training acc:  0.13978930307941653    training loss:  3.9162649895418147\n",
      "\n",
      "Cross Validation fold  1\n",
      "iteration  0  :    training acc:  0.09539078156312625    training loss:  21.007175559608857\n",
      "iteration  20  :    training acc:  0.10981963927855712    training loss:  4.590489312278145\n",
      "iteration  40  :    training acc:  0.11222444889779559    training loss:  4.528075058493823\n",
      "iteration  60  :    training acc:  0.11022044088176353    training loss:  4.404819279079095\n",
      "iteration  80  :    training acc:  0.11382765531062124    training loss:  4.377143635838699\n",
      "iteration  100  :    training acc:  0.11743486973947896    training loss:  4.510824007996136\n",
      "iteration  120  :    training acc:  0.11743486973947896    training loss:  4.374265949942065\n",
      "iteration  140  :    training acc:  0.1190380761523046    training loss:  4.41144421218447\n",
      "iteration  160  :    training acc:  0.11943887775551103    training loss:  4.417154094956585\n",
      "iteration  180  :    training acc:  0.12024048096192384    training loss:  4.355180273280541\n",
      "iteration  200  :    training acc:  0.1218436873747495    training loss:  4.375014115942198\n",
      "iteration  220  :    training acc:  0.11943887775551103    training loss:  4.361903379320789\n",
      "iteration  240  :    training acc:  0.1218436873747495    training loss:  4.319216315627686\n",
      "iteration  260  :    training acc:  0.12104208416833667    training loss:  4.347139960522406\n",
      "iteration  280  :    training acc:  0.12144288577154308    training loss:  4.319390732592946\n",
      "iteration  300  :    training acc:  0.12745490981963928    training loss:  4.280727121774132\n",
      "iteration  320  :    training acc:  0.12985971943887775    training loss:  4.266340838591393\n",
      "iteration  340  :    training acc:  0.13026052104208416    training loss:  4.2312392741127605\n",
      "iteration  360  :    training acc:  0.1342685370741483    training loss:  4.203209312054662\n",
      "iteration  380  :    training acc:  0.13386773547094188    training loss:  4.173845844671398\n",
      "iteration  400  :    training acc:  0.13547094188376754    training loss:  4.13437906839117\n",
      "iteration  420  :    training acc:  0.13627254509018036    training loss:  4.444592806224253\n",
      "iteration  440  :    training acc:  0.13707414829659317    training loss:  4.384235440939892\n",
      "iteration  460  :    training acc:  0.131062124248497    training loss:  4.426638766193267\n",
      "iteration  480  :    training acc:  0.13226452905811623    training loss:  4.449051099966468\n",
      "iteration  500  :    training acc:  0.13507014028056114    training loss:  4.4620511013314\n",
      "iteration  520  :    training acc:  0.13907815631262524    training loss:  4.428252563682527\n",
      "iteration  540  :    training acc:  0.1374749498997996    training loss:  4.392198439362847\n",
      "iteration  560  :    training acc:  0.13547094188376754    training loss:  4.351983881970606\n",
      "iteration  580  :    training acc:  0.13667334669338677    training loss:  4.315659565191989\n",
      "iteration  600  :    training acc:  0.13226452905811623    training loss:  4.290563823758648\n",
      "iteration  620  :    training acc:  0.13507014028056114    training loss:  4.27698491426102\n",
      "iteration  640  :    training acc:  0.13707414829659317    training loss:  4.267490265900337\n",
      "iteration  660  :    training acc:  0.13827655310621242    training loss:  4.213611310180317\n",
      "iteration  680  :    training acc:  0.14268537074148296    training loss:  4.170081268479739\n",
      "iteration  700  :    training acc:  0.1410821643286573    training loss:  4.152670543065715\n",
      "iteration  720  :    training acc:  0.14388777555110221    training loss:  4.10952063325813\n",
      "iteration  740  :    training acc:  0.14589178356713428    training loss:  4.072589886237994\n",
      "iteration  760  :    training acc:  0.14308617234468937    training loss:  4.037833323968114\n",
      "iteration  780  :    training acc:  0.1466933867735471    training loss:  3.958965972710902\n",
      "iteration  800  :    training acc:  0.13627254509018036    training loss:  3.8501285312401556\n",
      "iteration  820  :    training acc:  0.13947895791583168    training loss:  4.525036405999182\n",
      "iteration  840  :    training acc:  0.14148296593186374    training loss:  3.7349993975514075\n",
      "iteration  860  :    training acc:  0.13947895791583168    training loss:  3.6752157822148117\n",
      "iteration  880  :    training acc:  0.1402805611222445    training loss:  4.537816611173696\n",
      "iteration  900  :    training acc:  0.1623246492985972    training loss:  3.4769477718652517\n",
      "iteration  920  :    training acc:  0.1406813627254509    training loss:  4.113609049591559\n",
      "iteration  940  :    training acc:  0.1406813627254509    training loss:  4.383558132396866\n",
      "iteration  960  :    training acc:  0.16713426853707414    training loss:  3.477426119214823\n",
      "iteration  980  :    training acc:  0.13226452905811623    training loss:  4.144608754898528\n",
      "iteration  1000  :    training acc:  0.17595190380761522    training loss:  3.3847251574025607\n",
      "iteration  1020  :    training acc:  0.14308617234468937    training loss:  4.668646603458128\n",
      "iteration  1040  :    training acc:  0.17234468937875752    training loss:  3.4811877308497174\n",
      "iteration  1060  :    training acc:  0.16112224448897797    training loss:  3.499401486523841\n",
      "iteration  1080  :    training acc:  0.1875751503006012    training loss:  3.3324541615710723\n",
      "iteration  1100  :    training acc:  0.16352705410821644    training loss:  3.9294917637718885\n",
      "iteration  1120  :    training acc:  0.16633266533066132    training loss:  3.77591906242678\n",
      "iteration  1140  :    training acc:  0.19158316633266534    training loss:  3.340964088706406\n",
      "iteration  1160  :    training acc:  0.17675350701402806    training loss:  3.4093063535553374\n",
      "iteration  1180  :    training acc:  0.16392785571142285    training loss:  4.506281588234064\n",
      "iteration  1200  :    training acc:  0.1683366733466934    training loss:  3.8784702497567056\n",
      "iteration  1220  :    training acc:  0.18476953907815633    training loss:  3.4271529214372523\n",
      "iteration  1240  :    training acc:  0.1963927855711423    training loss:  3.251216807783626\n",
      "iteration  1260  :    training acc:  0.20120240480961923    training loss:  3.2747622526074296\n",
      "iteration  1280  :    training acc:  0.21482965931863726    training loss:  3.1913734022802367\n",
      "iteration  1300  :    training acc:  0.2160320641282565    training loss:  3.1676585937792923\n",
      "iteration  1320  :    training acc:  0.17675350701402806    training loss:  3.3930792584266842\n",
      "iteration  1340  :    training acc:  0.21002004008016031    training loss:  3.198868886467534\n",
      "iteration  1360  :    training acc:  0.22444889779559118    training loss:  3.1457300253714644\n",
      "iteration  1380  :    training acc:  0.1843687374749499    training loss:  3.422594995506367\n",
      "iteration  1400  :    training acc:  0.16032064128256512    training loss:  4.067499153468851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1420  :    training acc:  0.16593186372745491    training loss:  3.7084383072832057\n",
      "iteration  1440  :    training acc:  0.21923847695390783    training loss:  3.1610012490242307\n",
      "iteration  1460  :    training acc:  0.20400801603206412    training loss:  3.3055482463451376\n",
      "iteration  1480  :    training acc:  0.20761523046092184    training loss:  3.186696998393667\n",
      "iteration  1500  :    training acc:  0.10981963927855712    training loss:  5.800281812640833\n",
      "iteration  1520  :    training acc:  0.1935871743486974    training loss:  3.311962477963527\n",
      "iteration  1540  :    training acc:  0.22444889779559118    training loss:  3.1162586542003488\n",
      "iteration  1560  :    training acc:  0.17595190380761522    training loss:  3.7787290659226556\n",
      "iteration  1580  :    training acc:  0.18877755511022043    training loss:  3.340780837137885\n",
      "iteration  1600  :    training acc:  0.22244488977955912    training loss:  3.1288944031128505\n",
      "iteration  1620  :    training acc:  0.20601202404809618    training loss:  3.1873571055903747\n",
      "iteration  1640  :    training acc:  0.21122244488977956    training loss:  3.2416285368875135\n",
      "iteration  1660  :    training acc:  0.20561122244488977    training loss:  3.2747223772269587\n",
      "iteration  1680  :    training acc:  0.19919839679358717    training loss:  3.2994009078358713\n",
      "iteration  1700  :    training acc:  0.20200400801603208    training loss:  3.281756152474161\n",
      "iteration  1720  :    training acc:  0.2032064128256513    training loss:  3.2964050314682356\n",
      "iteration  1740  :    training acc:  0.18677354709418836    training loss:  3.4338641226133175\n",
      "iteration  1760  :    training acc:  0.13827655310621242    training loss:  5.02389311731135\n",
      "iteration  1780  :    training acc:  0.14188376753507015    training loss:  4.4879665742742105\n",
      "iteration  1800  :    training acc:  0.156312625250501    training loss:  4.027592103207749\n",
      "iteration  1820  :    training acc:  0.12985971943887775    training loss:  4.880344485304476\n",
      "iteration  1840  :    training acc:  0.15831663326653306    training loss:  4.686080728073895\n",
      "iteration  1860  :    training acc:  0.156312625250501    training loss:  3.757527469591301\n",
      "iteration  1880  :    training acc:  0.20761523046092184    training loss:  3.592195244600889\n",
      "iteration  1900  :    training acc:  0.18557114228456914    training loss:  3.8050993786106573\n",
      "iteration  1920  :    training acc:  0.21042084168336672    training loss:  3.290401493251583\n",
      "iteration  1940  :    training acc:  0.14468937875751503    training loss:  3.946646623611272\n",
      "iteration  1960  :    training acc:  0.19438877755511022    training loss:  4.180900223737158\n",
      "iteration  1980  :    training acc:  0.2028056112224449    training loss:  3.6840413159540013\n",
      "\n",
      "Cross Validation fold  2\n",
      "iteration  0  :    training acc:  0.1008610086100861    training loss:  18.822238874180346\n",
      "iteration  20  :    training acc:  0.0979909799097991    training loss:  9.61822515015998\n",
      "iteration  40  :    training acc:  0.0963509635096351    training loss:  8.308723619020805\n",
      "iteration  60  :    training acc:  0.09881098810988109    training loss:  8.16159860305934\n",
      "iteration  80  :    training acc:  0.1037310373103731    training loss:  7.119877273422505\n",
      "iteration  100  :    training acc:  0.1029110291102911    training loss:  6.85772711736818\n",
      "iteration  120  :    training acc:  0.09881098810988109    training loss:  7.277228343668158\n",
      "iteration  140  :    training acc:  0.1016810168101681    training loss:  7.9266745333070245\n",
      "iteration  160  :    training acc:  0.10209102091020911    training loss:  6.949527411911992\n",
      "iteration  180  :    training acc:  0.1057810578105781    training loss:  8.88974494528633\n",
      "iteration  200  :    training acc:  0.0943009430094301    training loss:  7.0214167328480555\n",
      "iteration  220  :    training acc:  0.10414104141041411    training loss:  8.395210286915441\n",
      "iteration  240  :    training acc:  0.1066010660106601    training loss:  7.881716276491704\n",
      "iteration  260  :    training acc:  0.11029110291102912    training loss:  7.110062569911721\n",
      "iteration  280  :    training acc:  0.10742107421074211    training loss:  8.18454723094028\n",
      "iteration  300  :    training acc:  0.10824108241082411    training loss:  7.0165763049795675\n",
      "iteration  320  :    training acc:  0.1025010250102501    training loss:  6.444915813059205\n",
      "iteration  340  :    training acc:  0.1004510045100451    training loss:  8.808486268067298\n",
      "iteration  360  :    training acc:  0.11193111931119311    training loss:  6.136968359530275\n",
      "iteration  380  :    training acc:  0.11685116851168512    training loss:  7.0446768593233475\n",
      "iteration  400  :    training acc:  0.11316113161131611    training loss:  6.298524653141308\n",
      "iteration  420  :    training acc:  0.10947109471094711    training loss:  6.69133423899578\n",
      "iteration  440  :    training acc:  0.11029110291102912    training loss:  7.264072246254946\n",
      "iteration  460  :    training acc:  0.11521115211152111    training loss:  7.506790054344316\n",
      "iteration  480  :    training acc:  0.11029110291102912    training loss:  6.473410046949774\n",
      "iteration  500  :    training acc:  0.12136121361213612    training loss:  6.6002673201517315\n",
      "iteration  520  :    training acc:  0.10742107421074211    training loss:  6.678961797270187\n",
      "iteration  540  :    training acc:  0.11480114801148011    training loss:  7.176678462149389\n",
      "iteration  560  :    training acc:  0.11521115211152111    training loss:  5.821339307983631\n",
      "iteration  580  :    training acc:  0.11562115621156212    training loss:  6.266710009141241\n",
      "iteration  600  :    training acc:  0.11234112341123412    training loss:  6.353367620343378\n",
      "iteration  620  :    training acc:  0.11767117671176712    training loss:  6.481165541729525\n",
      "iteration  640  :    training acc:  0.1111111111111111    training loss:  6.064462147368923\n",
      "iteration  660  :    training acc:  0.11316113161131611    training loss:  5.817151979522048\n",
      "iteration  680  :    training acc:  0.11685116851168512    training loss:  8.07399588279531\n",
      "iteration  700  :    training acc:  0.1078310783107831    training loss:  7.429815111707335\n",
      "iteration  720  :    training acc:  0.11685116851168512    training loss:  7.201723267587772\n",
      "iteration  740  :    training acc:  0.12792127921279212    training loss:  6.0064878837594895\n",
      "iteration  760  :    training acc:  0.11931119311193111    training loss:  5.812770853398915\n",
      "iteration  780  :    training acc:  0.11890118901189012    training loss:  6.224328664012707\n",
      "iteration  800  :    training acc:  0.10537105371053711    training loss:  6.718088077624617\n",
      "iteration  820  :    training acc:  0.11767117671176712    training loss:  6.494422021013249\n",
      "iteration  840  :    training acc:  0.11644116441164412    training loss:  6.723651578536959\n",
      "iteration  860  :    training acc:  0.11439114391143912    training loss:  6.061388134588516\n",
      "iteration  880  :    training acc:  0.11931119311193111    training loss:  6.357125738709537\n",
      "iteration  900  :    training acc:  0.13038130381303814    training loss:  6.722848971925859\n",
      "iteration  920  :    training acc:  0.13366133661336613    training loss:  8.378306914814312\n",
      "iteration  940  :    training acc:  0.12013120131201312    training loss:  6.459215586347877\n",
      "iteration  960  :    training acc:  0.11152111521115211    training loss:  5.83487337105981\n",
      "iteration  980  :    training acc:  0.11562115621156212    training loss:  6.425331254922453\n",
      "iteration  1000  :    training acc:  0.12300123001230012    training loss:  5.873541581343919\n",
      "iteration  1020  :    training acc:  0.12218122181221812    training loss:  6.110841093763057\n",
      "iteration  1040  :    training acc:  0.11193111931119311    training loss:  7.284881368302054\n",
      "iteration  1060  :    training acc:  0.12382123821238213    training loss:  5.918628656167736\n",
      "iteration  1080  :    training acc:  0.12956129561295612    training loss:  8.011424247472853\n",
      "iteration  1100  :    training acc:  0.12710127101271013    training loss:  6.406207634673402\n",
      "iteration  1120  :    training acc:  0.11767117671176712    training loss:  6.847632461667378\n",
      "iteration  1140  :    training acc:  0.11398113981139811    training loss:  6.803561709948203\n",
      "iteration  1160  :    training acc:  0.12464124641246413    training loss:  5.8924327452302325\n",
      "iteration  1180  :    training acc:  0.12300123001230012    training loss:  5.887628013529148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  1200  :    training acc:  0.12915129151291513    training loss:  6.03304501458407\n",
      "iteration  1220  :    training acc:  0.11931119311193111    training loss:  7.668511008502704\n",
      "iteration  1240  :    training acc:  0.11275112751127511    training loss:  6.431144778741406\n",
      "iteration  1260  :    training acc:  0.12464124641246413    training loss:  6.283823144553017\n",
      "iteration  1280  :    training acc:  0.12464124641246413    training loss:  6.132819800834195\n",
      "iteration  1300  :    training acc:  0.12054120541205413    training loss:  6.345949435550406\n",
      "iteration  1320  :    training acc:  0.12095120951209512    training loss:  5.8386319065775085\n",
      "iteration  1340  :    training acc:  0.13079130791307914    training loss:  5.919461677305439\n",
      "iteration  1360  :    training acc:  0.12464124641246413    training loss:  5.747758704731991\n",
      "iteration  1380  :    training acc:  0.12669126691266913    training loss:  6.132643256303584\n",
      "iteration  1400  :    training acc:  0.12587125871258711    training loss:  5.868223344429619\n",
      "iteration  1420  :    training acc:  0.12751127511275112    training loss:  5.739720852020317\n",
      "iteration  1440  :    training acc:  0.11767117671176712    training loss:  5.547538415419597\n",
      "iteration  1460  :    training acc:  0.11767117671176712    training loss:  5.845278437040561\n",
      "iteration  1480  :    training acc:  0.12177121771217712    training loss:  6.09234528817043\n",
      "iteration  1500  :    training acc:  0.12751127511275112    training loss:  6.100615862698821\n",
      "iteration  1520  :    training acc:  0.12341123411234112    training loss:  6.4672163534093485\n",
      "iteration  1540  :    training acc:  0.12915129151291513    training loss:  5.482482025153645\n",
      "iteration  1560  :    training acc:  0.12751127511275112    training loss:  5.951991732619352\n",
      "iteration  1580  :    training acc:  0.12218122181221812    training loss:  6.080423171438545\n",
      "iteration  1600  :    training acc:  0.12505125051250512    training loss:  5.934132482277398\n",
      "iteration  1620  :    training acc:  0.11193111931119311    training loss:  7.599056086045511\n",
      "iteration  1640  :    training acc:  0.14473144731447316    training loss:  5.606174639841201\n",
      "iteration  1660  :    training acc:  0.11726117261172611    training loss:  7.0018803833403025\n",
      "iteration  1680  :    training acc:  0.11890118901189012    training loss:  9.037106392656154\n",
      "iteration  1700  :    training acc:  0.12341123411234112    training loss:  6.477312563574562\n",
      "iteration  1720  :    training acc:  0.13325133251332513    training loss:  7.293191519483672\n",
      "iteration  1740  :    training acc:  0.10537105371053711    training loss:  9.606081145652643\n",
      "iteration  1760  :    training acc:  0.13817138171381713    training loss:  6.971500157593431\n",
      "iteration  1780  :    training acc:  0.14842148421484214    training loss:  5.757822799479745\n",
      "iteration  1800  :    training acc:  0.13202132021320212    training loss:  8.680739783251793\n",
      "iteration  1820  :    training acc:  0.12628126281262814    training loss:  7.609936951894438\n",
      "iteration  1840  :    training acc:  0.13079130791307914    training loss:  8.267726346650788\n",
      "iteration  1860  :    training acc:  0.10414104141041411    training loss:  10.15153393721604\n",
      "iteration  1880  :    training acc:  0.13571135711357113    training loss:  6.7556940832694545\n",
      "iteration  1900  :    training acc:  0.14145141451414514    training loss:  5.973726681762874\n",
      "iteration  1920  :    training acc:  0.11808118081180811    training loss:  9.560048840343324\n",
      "iteration  1940  :    training acc:  0.12095120951209512    training loss:  8.140150624904201\n",
      "iteration  1960  :    training acc:  0.13817138171381713    training loss:  6.3405558846028836\n",
      "iteration  1980  :    training acc:  0.12587125871258711    training loss:  10.414288851615105\n",
      "\n",
      "Cross Validation fold  3\n",
      "iteration  0  :    training acc:  0.10655408122235625    training loss:  21.342983207021245\n",
      "iteration  20  :    training acc:  0.10776035383996783    training loss:  4.946905572669646\n",
      "iteration  40  :    training acc:  0.10896662645757942    training loss:  4.748348158277543\n",
      "iteration  60  :    training acc:  0.09931644551668677    training loss:  4.8697759671460386\n",
      "iteration  80  :    training acc:  0.1093687173301166    training loss:  4.8027522023961415\n",
      "iteration  100  :    training acc:  0.11057498994772819    training loss:  4.7407862959624945\n",
      "iteration  120  :    training acc:  0.10253317249698432    training loss:  4.444667177554913\n",
      "iteration  140  :    training acc:  0.11499798954563731    training loss:  4.612755672363364\n",
      "iteration  160  :    training acc:  0.11620426216324889    training loss:  4.4960966054834\n",
      "iteration  180  :    training acc:  0.10655408122235625    training loss:  4.82772583234559\n",
      "iteration  200  :    training acc:  0.11057498994772819    training loss:  4.182820489639349\n",
      "iteration  220  :    training acc:  0.10092480900683555    training loss:  4.358799749666858\n",
      "iteration  240  :    training acc:  0.11540008041817451    training loss:  4.17317238817741\n",
      "iteration  260  :    training acc:  0.11499798954563731    training loss:  4.877969013781891\n",
      "iteration  280  :    training acc:  0.10695617209489344    training loss:  4.297929218653142\n",
      "iteration  300  :    training acc:  0.11178126256533977    training loss:  4.479572366761135\n",
      "iteration  320  :    training acc:  0.11620426216324889    training loss:  3.9649577622227303\n",
      "iteration  340  :    training acc:  0.11419380780056293    training loss:  4.160918273472983\n",
      "iteration  360  :    training acc:  0.11379171692802574    training loss:  4.21103737888534\n",
      "iteration  380  :    training acc:  0.11338962605548854    training loss:  4.021358841210775\n",
      "iteration  400  :    training acc:  0.11218335343787696    training loss:  4.098050846342246\n",
      "iteration  420  :    training acc:  0.11901889827100925    training loss:  4.075867818887459\n",
      "iteration  440  :    training acc:  0.11781262565339767    training loss:  4.158287117139115\n",
      "iteration  460  :    training acc:  0.10655408122235625    training loss:  3.9930434791277714\n",
      "iteration  480  :    training acc:  0.11178126256533977    training loss:  4.322902597157277\n",
      "iteration  500  :    training acc:  0.10574989947728186    training loss:  3.8894572303902346\n",
      "iteration  520  :    training acc:  0.1158021712907117    training loss:  3.980253229661672\n",
      "iteration  540  :    training acc:  0.12263771612384398    training loss:  3.9408883895540097\n",
      "iteration  560  :    training acc:  0.11741053478086047    training loss:  4.069599328036375\n",
      "iteration  580  :    training acc:  0.10816244471250502    training loss:  3.8502637011138567\n",
      "iteration  600  :    training acc:  0.11499798954563731    training loss:  4.084361420091229\n",
      "iteration  620  :    training acc:  0.11781262565339767    training loss:  4.033844819633883\n",
      "iteration  640  :    training acc:  0.11982308001608363    training loss:  3.890689016777785\n",
      "iteration  660  :    training acc:  0.1222356252513068    training loss:  3.8989778695461874\n",
      "iteration  680  :    training acc:  0.12022517088862082    training loss:  4.030717986444969\n",
      "iteration  700  :    training acc:  0.11901889827100925    training loss:  4.024135944317359\n",
      "iteration  720  :    training acc:  0.11821471652593486    training loss:  4.034106748393611\n",
      "iteration  740  :    training acc:  0.11942098914354644    training loss:  4.017432758886803\n",
      "iteration  760  :    training acc:  0.11741053478086047    training loss:  4.024388575244261\n",
      "iteration  780  :    training acc:  0.11741053478086047    training loss:  3.9822439591103347\n",
      "iteration  800  :    training acc:  0.11781262565339767    training loss:  3.985421504268031\n",
      "iteration  820  :    training acc:  0.11781262565339767    training loss:  3.9581906852452042\n",
      "iteration  840  :    training acc:  0.11821471652593486    training loss:  3.9477475457184856\n",
      "iteration  860  :    training acc:  0.11861680739847205    training loss:  3.926312967120301\n",
      "iteration  880  :    training acc:  0.11942098914354644    training loss:  3.886187167039823\n",
      "iteration  900  :    training acc:  0.11901889827100925    training loss:  3.877346558253225\n",
      "iteration  920  :    training acc:  0.11620426216324889    training loss:  3.8448136394144394\n",
      "iteration  940  :    training acc:  0.11338962605548854    training loss:  3.820950405564237\n",
      "iteration  960  :    training acc:  0.11861680739847205    training loss:  3.7595420010858565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  980  :    training acc:  0.11700844390832328    training loss:  3.761294515861453\n",
      "iteration  1000  :    training acc:  0.12102935263369521    training loss:  3.734155955631961\n",
      "iteration  1020  :    training acc:  0.12344189786891838    training loss:  3.73681363105651\n",
      "iteration  1040  :    training acc:  0.12545235223160434    training loss:  3.7231426630900053\n",
      "iteration  1060  :    training acc:  0.12464817048652996    training loss:  3.710320208742713\n",
      "iteration  1080  :    training acc:  0.12585444310414154    training loss:  3.7063015241458603\n",
      "iteration  1100  :    training acc:  0.12706071572175312    training loss:  3.6917688560017754\n",
      "iteration  1120  :    training acc:  0.13067953357458786    training loss:  3.6852927507882756\n",
      "iteration  1140  :    training acc:  0.12866907921190188    training loss:  3.6773716235957687\n",
      "iteration  1160  :    training acc:  0.1342983514274226    training loss:  3.670674540177933\n",
      "iteration  1180  :    training acc:  0.13671089666264577    training loss:  3.6617425995700494\n",
      "iteration  1200  :    training acc:  0.1338962605548854    training loss:  3.6746871986864007\n",
      "iteration  1220  :    training acc:  0.13630880579010857    training loss:  3.6638719153492847\n",
      "iteration  1240  :    training acc:  0.1334941696823482    training loss:  3.6535701091014476\n",
      "iteration  1260  :    training acc:  0.13510253317249699    training loss:  3.6498390703605637\n",
      "iteration  1280  :    training acc:  0.13027744270205066    training loss:  3.6847274962214143\n",
      "iteration  1300  :    training acc:  0.13067953357458786    training loss:  3.7031018833894005\n",
      "iteration  1320  :    training acc:  0.1282669883393647    training loss:  3.6799005059961023\n",
      "iteration  1340  :    training acc:  0.12987535182951346    training loss:  3.6774031556632516\n",
      "iteration  1360  :    training acc:  0.12987535182951346    training loss:  3.7279189650680187\n",
      "iteration  1380  :    training acc:  0.12866907921190188    training loss:  3.7594629830788064\n",
      "iteration  1400  :    training acc:  0.13188580619219945    training loss:  3.759020727411971\n",
      "iteration  1420  :    training acc:  0.13831926015279453    training loss:  3.7512396821563194\n",
      "iteration  1440  :    training acc:  0.14354644149577805    training loss:  3.7381988367050782\n",
      "iteration  1460  :    training acc:  0.1451548049859268    training loss:  3.7073408662566503\n",
      "iteration  1480  :    training acc:  0.15681544028950542    training loss:  3.5859436716540087\n",
      "iteration  1500  :    training acc:  0.15400080418174508    training loss:  3.535505809659618\n",
      "iteration  1520  :    training acc:  0.15721753116204262    training loss:  3.7469467216934667\n",
      "iteration  1540  :    training acc:  0.17048652995577002    training loss:  3.418122048519942\n",
      "iteration  1560  :    training acc:  0.16485725774024929    training loss:  3.5661310543434888\n",
      "iteration  1580  :    training acc:  0.15721753116204262    training loss:  3.7911441953086196\n",
      "iteration  1600  :    training acc:  0.15922798552472858    training loss:  3.8252608916082846\n",
      "iteration  1620  :    training acc:  0.15681544028950542    training loss:  3.9338799682559715\n",
      "iteration  1640  :    training acc:  0.18375552874949738    training loss:  3.4020278329242606\n",
      "iteration  1660  :    training acc:  0.1640530759951749    training loss:  3.727982519883562\n",
      "iteration  1680  :    training acc:  0.19099316445516687    training loss:  3.316143513639761\n",
      "iteration  1700  :    training acc:  0.15359871330920788    training loss:  5.049975947767416\n",
      "iteration  1720  :    training acc:  0.1885806192199437    training loss:  3.454710276835346\n",
      "iteration  1740  :    training acc:  0.19300361881785283    training loss:  3.5497337031369103\n",
      "iteration  1760  :    training acc:  0.16606353035786087    training loss:  4.1617803639441755\n",
      "iteration  1780  :    training acc:  0.1636509851226377    training loss:  4.363077391759597\n",
      "iteration  1800  :    training acc:  0.1889827100924809    training loss:  3.606136585374683\n",
      "iteration  1820  :    training acc:  0.15359871330920788    training loss:  4.056683654791526\n",
      "iteration  1840  :    training acc:  0.1881785283474065    training loss:  3.734953632898145\n",
      "iteration  1860  :    training acc:  0.182951347004423    training loss:  3.557591526139579\n",
      "iteration  1880  :    training acc:  0.1708886208283072    training loss:  4.68220762332899\n",
      "iteration  1900  :    training acc:  0.1881785283474065    training loss:  4.037165702346872\n",
      "iteration  1920  :    training acc:  0.17812625653397668    training loss:  4.501043327514981\n",
      "iteration  1940  :    training acc:  0.1218335343787696    training loss:  6.3287298448453155\n",
      "iteration  1960  :    training acc:  0.17209489344591877    training loss:  6.3822518226337435\n",
      "iteration  1980  :    training acc:  0.1753116204262163    training loss:  4.881648996980055\n",
      "\n",
      "Cross Validation fold  4\n",
      "iteration  0  :    training acc:  0.10241433021806853    training loss:  23.331888640716148\n",
      "iteration  20  :    training acc:  0.10397196261682243    training loss:  5.627893960287472\n",
      "iteration  40  :    training acc:  0.09657320872274143    training loss:  5.309480676047856\n",
      "iteration  60  :    training acc:  0.09890965732087227    training loss:  4.997858272154436\n",
      "iteration  80  :    training acc:  0.09813084112149532    training loss:  6.35607975834217\n",
      "iteration  100  :    training acc:  0.10202492211838006    training loss:  5.530929173044042\n",
      "iteration  120  :    training acc:  0.10825545171339564    training loss:  6.411444032039338\n",
      "iteration  140  :    training acc:  0.10669781931464174    training loss:  5.943128215252571\n",
      "iteration  160  :    training acc:  0.1043613707165109    training loss:  5.506793782323459\n",
      "iteration  180  :    training acc:  0.0969626168224299    training loss:  5.090322334953076\n",
      "iteration  200  :    training acc:  0.09890965732087227    training loss:  5.529653717045149\n",
      "iteration  220  :    training acc:  0.10669781931464174    training loss:  4.962167175082163\n",
      "iteration  240  :    training acc:  0.09774143302180685    training loss:  4.814944092967694\n",
      "iteration  260  :    training acc:  0.10397196261682243    training loss:  5.658828707356369\n",
      "iteration  280  :    training acc:  0.10514018691588785    training loss:  4.850872121800874\n",
      "iteration  300  :    training acc:  0.09813084112149532    training loss:  4.509410774989305\n",
      "iteration  320  :    training acc:  0.09929906542056074    training loss:  4.860590306172883\n",
      "iteration  340  :    training acc:  0.110202492211838    training loss:  5.33076955778045\n",
      "iteration  360  :    training acc:  0.1043613707165109    training loss:  5.737026081495973\n",
      "iteration  380  :    training acc:  0.10747663551401869    training loss:  4.786725927822997\n",
      "iteration  400  :    training acc:  0.10475077881619937    training loss:  4.835752105789513\n",
      "iteration  420  :    training acc:  0.10981308411214953    training loss:  4.893859912500488\n",
      "iteration  440  :    training acc:  0.10786604361370716    training loss:  4.760329538863894\n",
      "iteration  460  :    training acc:  0.1059190031152648    training loss:  5.7922988524963275\n",
      "iteration  480  :    training acc:  0.10942367601246106    training loss:  4.608227455726635\n",
      "iteration  500  :    training acc:  0.11137071651090343    training loss:  5.049757520807738\n",
      "iteration  520  :    training acc:  0.110202492211838    training loss:  5.158371307477524\n",
      "iteration  540  :    training acc:  0.110202492211838    training loss:  4.671633218393924\n",
      "iteration  560  :    training acc:  0.10397196261682243    training loss:  5.213509512407557\n",
      "iteration  580  :    training acc:  0.12694704049844235    training loss:  4.864502656952066\n",
      "iteration  600  :    training acc:  0.11993769470404984    training loss:  4.3013242188977365\n",
      "iteration  620  :    training acc:  0.117601246105919    training loss:  4.709628240564312\n",
      "iteration  640  :    training acc:  0.11838006230529595    training loss:  5.604560040454612\n",
      "iteration  660  :    training acc:  0.1265576323987539    training loss:  4.39469950715883\n",
      "iteration  680  :    training acc:  0.13239875389408098    training loss:  4.823122668897046\n",
      "iteration  700  :    training acc:  0.11098130841121495    training loss:  5.264081451060295\n",
      "iteration  720  :    training acc:  0.117601246105919    training loss:  5.278934808242681\n",
      "iteration  740  :    training acc:  0.13123052959501558    training loss:  4.734946178744184\n",
      "iteration  760  :    training acc:  0.10630841121495327    training loss:  6.614996123608588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration  780  :    training acc:  0.11838006230529595    training loss:  6.068295134428093\n",
      "iteration  800  :    training acc:  0.14213395638629284    training loss:  3.9361284010631277\n",
      "iteration  820  :    training acc:  0.11876947040498442    training loss:  5.531762976700438\n",
      "iteration  840  :    training acc:  0.10942367601246106    training loss:  8.224668777298197\n",
      "iteration  860  :    training acc:  0.1440809968847352    training loss:  4.12112422359593\n",
      "iteration  880  :    training acc:  0.13278816199376947    training loss:  6.401786549827515\n",
      "iteration  900  :    training acc:  0.13161993769470404    training loss:  4.429638139917685\n",
      "iteration  920  :    training acc:  0.11526479750778816    training loss:  6.0911643981205525\n",
      "iteration  940  :    training acc:  0.14719626168224298    training loss:  4.104123597774835\n",
      "iteration  960  :    training acc:  0.12149532710280374    training loss:  5.102865224510027\n",
      "iteration  980  :    training acc:  0.13707165109034267    training loss:  4.5272905377908685\n",
      "iteration  1000  :    training acc:  0.15965732087227413    training loss:  3.9801899531738205\n",
      "iteration  1020  :    training acc:  0.1647196261682243    training loss:  3.8011000918109215\n",
      "iteration  1040  :    training acc:  0.10124610591900311    training loss:  5.063073155697878\n",
      "iteration  1060  :    training acc:  0.16004672897196262    training loss:  4.146420814742815\n",
      "iteration  1080  :    training acc:  0.13862928348909656    training loss:  5.268589843170661\n",
      "iteration  1100  :    training acc:  0.1588785046728972    training loss:  4.834872620807328\n",
      "iteration  1120  :    training acc:  0.13590342679127726    training loss:  5.6725923993243805\n",
      "iteration  1140  :    training acc:  0.15965732087227413    training loss:  4.6020064229619955\n",
      "iteration  1160  :    training acc:  0.09735202492211838    training loss:  6.235124709175124\n",
      "iteration  1180  :    training acc:  0.10669781931464174    training loss:  5.449641084783715\n",
      "iteration  1200  :    training acc:  0.13395638629283488    training loss:  5.031231050364038\n",
      "iteration  1220  :    training acc:  0.15342679127725856    training loss:  4.4260484381989755\n",
      "iteration  1240  :    training acc:  0.14563862928348908    training loss:  4.66192931718995\n",
      "iteration  1260  :    training acc:  0.13707165109034267    training loss:  5.741658213083251\n",
      "iteration  1280  :    training acc:  0.15381619937694704    training loss:  5.228180127715466\n",
      "iteration  1300  :    training acc:  0.15615264797507789    training loss:  5.377928321193295\n",
      "iteration  1320  :    training acc:  0.17172897196261683    training loss:  3.977385677901354\n",
      "iteration  1340  :    training acc:  0.1440809968847352    training loss:  4.210724472838845\n",
      "iteration  1360  :    training acc:  0.15031152647975077    training loss:  6.373458398123783\n",
      "iteration  1380  :    training acc:  0.1795171339563863    training loss:  4.826093980408405\n",
      "iteration  1400  :    training acc:  0.19314641744548286    training loss:  5.647191180779699\n",
      "iteration  1420  :    training acc:  0.11565420560747663    training loss:  6.377527681315402\n",
      "iteration  1440  :    training acc:  0.17367601246105918    training loss:  5.562210591523909\n",
      "iteration  1460  :    training acc:  0.18574766355140188    training loss:  4.856847850439052\n",
      "iteration  1480  :    training acc:  0.18613707165109034    training loss:  5.786392224496996\n",
      "iteration  1500  :    training acc:  0.17562305295950156    training loss:  5.534927495572546\n",
      "iteration  1520  :    training acc:  0.1705607476635514    training loss:  7.457270315945843\n",
      "iteration  1540  :    training acc:  0.16277258566978192    training loss:  6.438245575653984\n",
      "iteration  1560  :    training acc:  0.1923676012461059    training loss:  6.3041535938222255\n",
      "iteration  1580  :    training acc:  0.18146417445482865    training loss:  7.499322669158486\n",
      "iteration  1600  :    training acc:  0.10552959501557632    training loss:  11.41847966002353\n",
      "iteration  1620  :    training acc:  0.1806853582554517    training loss:  7.389693540368591\n",
      "iteration  1640  :    training acc:  0.14719626168224298    training loss:  7.0887241295415855\n",
      "iteration  1660  :    training acc:  0.110202492211838    training loss:  8.449130871857442\n",
      "iteration  1680  :    training acc:  0.09657320872274143    training loss:  10.5454279271611\n",
      "iteration  1700  :    training acc:  0.09774143302180685    training loss:  20.43044534095442\n",
      "iteration  1720  :    training acc:  0.11721183800623053    training loss:  8.58920007369723\n",
      "iteration  1740  :    training acc:  0.10163551401869159    training loss:  12.343871748074116\n",
      "iteration  1760  :    training acc:  0.16199376947040497    training loss:  8.699197022371164\n",
      "iteration  1780  :    training acc:  0.10007788161993769    training loss:  17.391065459314795\n",
      "iteration  1800  :    training acc:  0.09890965732087227    training loss:  17.053742731724988\n",
      "iteration  1820  :    training acc:  0.09735202492211838    training loss:  24.224643879088642\n",
      "iteration  1840  :    training acc:  0.11253894080996885    training loss:  14.692504780124093\n",
      "iteration  1860  :    training acc:  0.10475077881619937    training loss:  nan\n",
      "iteration  1880  :    training acc:  0.10085669781931464    training loss:  19.63396301715091\n",
      "iteration  1900  :    training acc:  0.17328660436137072    training loss:  nan\n",
      "iteration  1920  :    training acc:  0.1573208722741433    training loss:  nan\n",
      "iteration  1940  :    training acc:  0.15615264797507789    training loss:  nan\n",
      "iteration  1960  :    training acc:  0.1647196261682243    training loss:  nan\n",
      "iteration  1980  :    training acc:  0.10475077881619937    training loss:  nan\n"
     ]
    }
   ],
   "source": [
    "cross_validation=5\n",
    "valid_split = 0.05\n",
    "\n",
    "# best_pred = [accuracy, nn, training trend]\n",
    "best_pred = (0, 0, [])\n",
    "\n",
    "trends = []\n",
    "layers = np.arange(1, 7)\n",
    "\n",
    "for layer in layers:\n",
    "  for valid in range(cross_validation):\n",
    "      print(\"\\nCross Validation fold \", valid)\n",
    "\n",
    "      # randomly split the dataset into validation and training sets \n",
    "      mask = np.random.rand(train_x.shape[0]) <= valid_split\n",
    "      t_x = train_x[mask]\n",
    "      t_y = train_y[mask]\n",
    "\n",
    "      v_x = train_x[~mask]\n",
    "      v_y = train_y[~mask]\n",
    "\n",
    "      nn = NN(ni=4096, nh=layer, no=10)\n",
    "\n",
    "      res = nn.train(t_x, t_y, 2000)\n",
    "\n",
    "      # validate with validation set after the training\n",
    "      v_o = nn.predict(v_x)\n",
    "      pred = np.argmax(v_o, axis=1)\n",
    "      diff = v_y - pred\n",
    "      acc = (diff == 0).sum() / len(v_y)\n",
    "\n",
    "\n",
    "      if(acc > best_pred[0]): best_pred = (acc, nn, res) \n",
    "   \n",
    "  trends.append(best_pred[2])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 681,
     "output_extras": [
      {
       "item_id": 1
      },
      {
       "item_id": 2
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1341,
     "status": "ok",
     "timestamp": 1521739114984,
     "user": {
      "displayName": "Tiffany Wang",
      "photoUrl": "//lh3.googleusercontent.com/-DxivnaZLcsI/AAAAAAAAAAI/AAAAAAAAH_I/QDov37LUsKk/s50-c-k-no/photo.jpg",
      "userId": "108532619344838745606"
     },
     "user_tz": 240
    },
    "id": "dNcJbdwHRsuC",
    "outputId": "c62d0ca2-d39e-4caa-acc4-d3950f69d6e4"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiKSCUOYiIhIJgxhIiIimTCEiYiIZPL/qbBcEyvC29UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41356a9b38>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFOCAYAAACxAKU1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBgDAEEIYAABD/g+/Ts/3owBBCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f41356a02b0>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = np.array(best_pred[2])\n",
    "epoch = np.arange(len(res))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(epoch[1:], res[:,1][1:], 'r--')\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.plot(epoch, res[:,0], 'b')\n",
    "plt.ylim(0, 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IvNMeC6PsAUE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "4jGZaL_Usf_T"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "neural_net.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
